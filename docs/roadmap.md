# FoloVLLM æŠ€æœ¯è·¯çº¿å›¾

## ğŸ“ˆ æ¸è¿›å¼å¼€å‘æµç¨‹å›¾

```
M0: é¡¹ç›®åˆå§‹åŒ–
    â†“
M1: åŸºç¡€ç¦»çº¿æ¨ç† (å•è¯·æ±‚ã€è¿ç»­KV Cache)
    â†“
M2: è¿ç»­æ‰¹å¤„ç† (åŠ¨æ€è°ƒåº¦ã€å¤šè¯·æ±‚å¹¶è¡Œ)
    â†“
M3: Paged KV Cache (å†…å­˜ä¼˜åŒ–ã€PagedAttention)
    â†“
M4: Flash Attention (è®¡ç®—ä¼˜åŒ–ã€é™ä½å»¶è¿Ÿ)
    â†“
M5: Chunked Prefill (TTFTä¼˜åŒ–ã€æ··åˆè°ƒåº¦)
    â†“
M6: å‰ç¼€å¤ç”¨ (ç¼“å­˜å¤ç”¨ã€åŠ é€Ÿé¢„å¡«å……)
    â†“
M7: GPTQé‡åŒ– (æ˜¾å­˜ä¼˜åŒ–ã€4-bitæ¨ç†)
    â†“
ğŸ‰ å®Œæ•´çš„è½»é‡çº§æ¨ç†æ¡†æ¶
```

## ğŸ¯ å„é˜¶æ®µæ ¸å¿ƒç›®æ ‡

### M0: é¡¹ç›®åˆå§‹åŒ–
**å…³é”®è¯**: åŸºç¡€è®¾æ–½ã€æ¨¡å‹åŠ è½½ã€é…ç½®ç®¡ç†

**æ ¸å¿ƒäº§å‡º**:
- é¡¹ç›®ç›®å½•ç»“æ„
- é…ç½®ç³»ç»Ÿ (ModelConfig, SchedulerConfig, CacheConfig)
- æ¨¡å‹åŠ è½½å™¨ (æ”¯æŒ HuggingFace)
- åŸºç¡€æ•°æ®ç»“æ„ (Request, Sequence, SamplingParams)

**æŠ€æœ¯éš¾ç‚¹**:
- ç»Ÿä¸€çš„é…ç½®ç®¡ç†
- æ¨¡å‹æƒé‡çš„é«˜æ•ˆåŠ è½½

---

### M1: åŸºç¡€ç¦»çº¿æ¨ç†
**å…³é”®è¯**: Forward Passã€Samplingã€è¿ç»­KV Cache

**æ ¸å¿ƒäº§å‡º**:
- LLMEngine åŸºç¡€å®ç°
- Transformer forward pass
- Greedy/Top-k/Top-p Sampling
- ç®€å•çš„è¿ç»­ KV Cache

**æŠ€æœ¯éš¾ç‚¹**:
- KV Cache çš„ç»´æŠ¤å’Œæ›´æ–°
- å¤šç§é‡‡æ ·ç­–ç•¥çš„å®ç°
- Attention mask çš„æ­£ç¡®æ„å»º

**æ€§èƒ½åŸºçº¿**:
- å•è¯·æ±‚æ¨ç†å»¶è¿Ÿ
- Token ç”Ÿæˆé€Ÿåº¦

---

### M2: è¿ç»­æ‰¹å¤„ç†
**å…³é”®è¯**: Dynamic Batchingã€Schedulerã€å¹¶è¡Œå¤„ç†

**æ ¸å¿ƒäº§å‡º**:
- Scheduler (è¯·æ±‚é˜Ÿåˆ—ã€ä¼˜å…ˆçº§)
- åŠ¨æ€ Batch ç»„è£…
- è¯·æ±‚ç”Ÿå‘½å‘¨æœŸç®¡ç†
- æŠ¢å å’Œæ¢å¤æœºåˆ¶

**æŠ€æœ¯éš¾ç‚¹**:
- ä¸åŒé•¿åº¦åºåˆ—çš„å¯¹é½
- åŠ¨æ€æ·»åŠ /ç§»é™¤åºåˆ—
- Batch çŠ¶æ€çš„æ­£ç¡®ç»´æŠ¤

**æ€§èƒ½æå‡**:
- ååé‡: **3-5x** â†‘
- GPU åˆ©ç”¨ç‡æ˜¾è‘—æå‡

---

### M3: Paged KV Cache
**å…³é”®è¯**: PagedAttentionã€Block Managerã€å†…å­˜ä¼˜åŒ–

**æ ¸å¿ƒäº§å‡º**:
- Block Pool Manager
- é€»è¾‘åˆ°ç‰©ç† block æ˜ å°„
- PagedAttention ç®—å­
- Copy-on-Write æœºåˆ¶

**æŠ€æœ¯éš¾ç‚¹**:
- Block åˆ†é…ç®—æ³•
- Attention è®¡ç®—é€‚é…åˆ†é¡µå†…å­˜
- ç¢ç‰‡åŒ–ç®¡ç†

**æ€§èƒ½æå‡**:
- æ˜¾å­˜åˆ©ç”¨ç‡: **æ¥è¿‘ 100%** (vs ~20% ä¼ ç»Ÿæ–¹å¼)
- æ”¯æŒæ›´å¤§ batch size
- æ˜¾å­˜å ç”¨: **2x** â†“

**å…³é”®æ•°æ®ç»“æ„**:
```python
block_table: List[int]  # é€»è¾‘å— -> ç‰©ç†å— ID
block_pool: Tensor      # ç‰©ç†å†…å­˜æ± 
ref_count: Dict[int, int]  # å—å¼•ç”¨è®¡æ•°
```

---

### M4: Flash Attention
**å…³é”®è¯**: ä¼˜åŒ–ç®—æ³•ã€HBMè®¿é—®ã€IOæ„ŸçŸ¥

**æ ¸å¿ƒäº§å‡º**:
- Flash Attention 2 é›†æˆ
- Attention Backend æŠ½è±¡
- Prefill/Decode ç»Ÿä¸€å¤„ç†

**æŠ€æœ¯éš¾ç‚¹**:
- Flash Attention ä¸ Paged KV çš„é€‚é…
- ä¸åŒ backend çš„æ­£ç¡®åˆ‡æ¢

**æ€§èƒ½æå‡**:
- Attention è®¡ç®—: **1.5-2x** â†‘
- å»¶è¿Ÿ: **20-30%** â†“
- æ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡

**ä¼˜åŒ–åŸç†**:
- Tiling: åˆ†å—åŠ è½½åˆ° SRAM
- Recomputation: é¿å…å­˜å‚¨ä¸­é—´ç»“æœ
- Kernel fusion: å‡å°‘ HBM è®¿é—®

---

### M5: Chunked Prefill
**å…³é”®è¯**: åˆ†å—å¤„ç†ã€TTFTä¼˜åŒ–ã€æ··åˆè°ƒåº¦

**æ ¸å¿ƒäº§å‡º**:
- Prefill åˆ†å—é€»è¾‘
- Prefill-Decode æ··åˆè°ƒåº¦
- åŠ¨æ€ chunk size è°ƒæ•´

**æŠ€æœ¯éš¾ç‚¹**:
- Chunk size çš„é€‰æ‹©ç­–ç•¥
- Prefill å’Œ Decode çš„èµ„æºå¹³è¡¡
- çŠ¶æ€è½¬æ¢çš„æ­£ç¡®ç®¡ç†

**æ€§èƒ½æå‡**:
- TTFT (Time to First Token): **æ˜¾è‘—é™ä½**
- å‡å°‘ head-of-line blocking
- ååé‡å’Œå»¶è¿Ÿæ›´å¹³è¡¡

**è°ƒåº¦ç¤ºä¾‹**:
```
ä¼ ç»Ÿ: [Prefill_1000_tokens] -> é˜»å¡å…¶ä»–è¯·æ±‚
Chunked: [Prefill_256] + [Decode * N] -> äº¤æ›¿æ‰§è¡Œ
```

---

### M6: å‰ç¼€å¤ç”¨
**å…³é”®è¯**: Prefix Cachingã€Trieã€å…±äº«å†…å­˜

**æ ¸å¿ƒäº§å‡º**:
- Token åºåˆ—å“ˆå¸Œ
- å‰ç¼€æ ‘ (Trie) åŒ¹é…
- Block å…±äº«æœºåˆ¶ (COW å®Œå–„)
- LRU ç¼“å­˜æ·˜æ±°

**æŠ€æœ¯éš¾ç‚¹**:
- é«˜æ•ˆçš„å‰ç¼€åŒ¹é…ç®—æ³•
- å¼•ç”¨è®¡æ•°çš„æ­£ç¡®ç»´æŠ¤
- ç¼“å­˜æ·˜æ±°ç­–ç•¥

**æ€§èƒ½æå‡**:
- TTFT: **3-10x** â†“ (ç¼“å­˜å‘½ä¸­æ—¶)
- æ˜¾å­˜å¤ç”¨ç‡æå‡

**åº”ç”¨åœºæ™¯**:
- Few-shot prompting (å…±äº«ç¤ºä¾‹)
- å¤šè½®å¯¹è¯ (å…±äº«å†å²)
- æ‰¹é‡è¯·æ±‚ (å…±äº« system prompt)

**æ•°æ®ç»“æ„**:
```python
prefix_trie: TrieNode       # å‰ç¼€æ ‘
token_to_hash: Dict         # Tokenåºåˆ— -> Hash
hash_to_blocks: Dict        # Hash -> Blocks
```

---

### M7: GPTQ é‡åŒ–
**å…³é”®è¯**: 4-bité‡åŒ–ã€æƒé‡å‹ç¼©ã€ç²¾åº¦ä¿æŒ

**æ ¸å¿ƒäº§å‡º**:
- GPTQ æƒé‡åŠ è½½
- é‡åŒ–ç®—å­é›†æˆ (AutoGPTQ)
- åé‡åŒ–æ¨ç†é€»è¾‘

**æŠ€æœ¯éš¾ç‚¹**:
- é‡åŒ–ç²¾åº¦æŸå¤±æ§åˆ¶
- é‡åŒ–ç®—å­çš„æ­£ç¡®ä½¿ç”¨
- æ€§èƒ½å’Œç²¾åº¦å¹³è¡¡

**æ€§èƒ½æå‡**:
- æ˜¾å­˜å ç”¨: **4x** â†“
- æ¨ç†é€Ÿåº¦: **1.2-1.5x** â†‘ (éƒ¨åˆ†åœºæ™¯)
- ç²¾åº¦æŸå¤±: **< 1%** (perplexity)

**é‡åŒ–æµç¨‹**:
```
åŸå§‹æ¨¡å‹ (FP16) -> GPTQé‡åŒ– -> 4-bitæƒé‡ + Scale/Zero-point
                              â†“
                         è¿è¡Œæ—¶åé‡åŒ– -> è®¡ç®—
```

---

## ğŸ”„ æŠ€æœ¯ä¾èµ–å…³ç³»

```mermaid
graph TD
    M0[M0: é¡¹ç›®åˆå§‹åŒ–] --> M1[M1: åŸºç¡€æ¨ç†]
    M1 --> M2[M2: è¿ç»­æ‰¹å¤„ç†]
    M2 --> M3[M3: Paged KV Cache]
    M3 --> M4[M4: Flash Attention]
    M3 --> M6[M6: å‰ç¼€å¤ç”¨]
    M4 --> M5[M5: Chunked Prefill]
    M1 --> M7[M7: GPTQé‡åŒ–]
    
    style M3 fill:#f9f,stroke:#333
    style M4 fill:#bbf,stroke:#333
    style M6 fill:#bfb,stroke:#333
```

**å…³é”®ä¾èµ–**:
- M3 æ˜¯åç»­ä¼˜åŒ–çš„åŸºç¡€ (Paged KV)
- M4 ä¾èµ– M3 çš„åˆ†é¡µå†…å­˜ç»“æ„
- M6 ä¾èµ– M3 çš„ block å…±äº«æœºåˆ¶
- M7 ç›¸å¯¹ç‹¬ç«‹ï¼Œå¯ä»¥å¹¶è¡Œå¼€å‘

---

## ğŸ“Š æ€§èƒ½æ¼”è¿›è·¯å¾„

### å»¶è¿Ÿä¼˜åŒ–è·¯å¾„
```
M1 (åŸºçº¿)
    â†“
M4 (Flash Attention: -20~30%)
    â†“
M5 (Chunked Prefill: TTFT æ˜¾è‘—æ”¹å–„)
    â†“
M6 (Prefix Cache: ç¼“å­˜å‘½ä¸­ -90%)
```

### ååé‡ä¼˜åŒ–è·¯å¾„
```
M1 (åŸºçº¿)
    â†“
M2 (Continuous Batching: +3~5x)
    â†“
M3 (Paged KV: æ›´å¤§ batch)
    â†“
M4 (Flash Attention: +1.5~2x)
```

### æ˜¾å­˜ä¼˜åŒ–è·¯å¾„
```
M1 (åŸºçº¿)
    â†“
M3 (Paged KV: -50%)
    â†“
M7 (GPTQ: -75%)
    â†“
æœ€ç»ˆ: æ˜¾å­˜å ç”¨ ~6% åˆå§‹å€¼
```

---

## ğŸ“ å­¦ä¹ é‡ç‚¹

### å¿…é¡»æ·±å…¥ç†è§£çš„æ¦‚å¿µ

1. **KV Cache æœºåˆ¶** (M1)
   - ä¸ºä»€ä¹ˆéœ€è¦ KV Cacheï¼Ÿ
   - å¦‚ä½•ç»´æŠ¤å’Œæ›´æ–°ï¼Ÿ

2. **åŠ¨æ€æ‰¹å¤„ç†** (M2)
   - Iteration-level scheduling
   - è¯·æ±‚ç”Ÿå‘½å‘¨æœŸç®¡ç†

3. **PagedAttention** (M3)
   - è™šæ‹Ÿå†…å­˜æ€æƒ³åœ¨ LLM çš„åº”ç”¨
   - Block åˆ†é…å’Œå›æ”¶ç®—æ³•

4. **Flash Attention** (M4)
   - IO-aware ç®—æ³•è®¾è®¡
   - Tiling å’Œ recomputation æƒè¡¡

5. **Chunked Prefill** (M5)
   - Prefill-Decode çš„æœ¬è´¨åŒºåˆ«
   - å¦‚ä½•å¹³è¡¡ TTFT å’Œååé‡

6. **Prefix Caching** (M6)
   - å‰ç¼€åŒ¹é…ç®—æ³•
   - Copy-on-Write æœºåˆ¶

7. **GPTQ é‡åŒ–** (M7)
   - æƒé‡é‡åŒ–åŸç†
   - é‡åŒ–è¯¯å·®æ§åˆ¶

---

## ğŸ” é¢è¯•é«˜é¢‘é—®é¢˜

### ç³»ç»Ÿè®¾è®¡ç±»

1. **å¦‚ä½•è®¾è®¡ä¸€ä¸ªé«˜æ€§èƒ½çš„ LLM æ¨ç†ç³»ç»Ÿï¼Ÿ**
   - ç­”æ¡ˆæ¶µç›–: M2 â†’ M3 â†’ M4 â†’ M5

2. **å¦‚ä½•ä¼˜åŒ– LLM æ¨ç†çš„å†…å­˜å ç”¨ï¼Ÿ**
   - ç­”æ¡ˆæ¶µç›–: M3 (Paged KV) + M7 (é‡åŒ–)

3. **å¦‚ä½•é™ä½é¦– token å»¶è¿Ÿï¼Ÿ**
   - ç­”æ¡ˆæ¶µç›–: M5 (Chunked Prefill) + M6 (Prefix Cache)

### æŠ€æœ¯æ·±åº¦ç±»

4. **PagedAttention å’Œä¼ ç»Ÿ Attention çš„åŒºåˆ«ï¼Ÿ**
   - é‡ç‚¹: Block åˆ†é…ã€å†…å­˜æ˜ å°„ã€ç¢ç‰‡åŒ–

5. **Flash Attention ä¸ºä»€ä¹ˆå¿«ï¼Ÿ**
   - é‡ç‚¹: IO å¤æ‚åº¦åˆ†æã€Tilingã€Recomputation

6. **Continuous Batching å¦‚ä½•å®ç°ï¼Ÿ**
   - é‡ç‚¹: åŠ¨æ€è°ƒåº¦ã€Padding å¤„ç†ã€çŠ¶æ€ç®¡ç†

7. **å‰ç¼€å¤ç”¨çš„å®ç°åŸç†ï¼Ÿ**
   - é‡ç‚¹: Trie åŒ¹é…ã€COWã€å¼•ç”¨è®¡æ•°

### æƒè¡¡å–èˆç±»

8. **Chunked Prefill çš„ chunk size å¦‚ä½•é€‰æ‹©ï¼Ÿ**
   - è€ƒè™‘: TTFTã€ååé‡ã€GPU åˆ©ç”¨ç‡

9. **é‡åŒ–å¸¦æ¥çš„ç²¾åº¦æŸå¤±å¦‚ä½•è¯„ä¼°ï¼Ÿ**
   - æŒ‡æ ‡: Perplexityã€ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½

---

## ğŸ“… å¼€å‘æ£€æŸ¥æ¸…å•

### æ¯ä¸ª Milestone å®Œæˆå‰

- [ ] åŠŸèƒ½å®ç°å®Œæ•´
- [ ] å•å…ƒæµ‹è¯•è¦†ç›– > 80%
- [ ] é›†æˆæµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½æµ‹è¯•å®Œæˆ (æœ‰ä¼˜åŒ–æ—¶)
- [ ] å­¦ä¹ ç¬”è®°ç¼–å†™ (æŠ€æœ¯åŸç† + é¢è¯•é¢˜)
- [ ] å¼€å‘æ—¥å¿—æ›´æ–° (åŠŸèƒ½ + é—®é¢˜ + æ¥å£)
- [ ] ä»£ç æ³¨é‡Šæ¸…æ™°
- [ ] README çŠ¶æ€æ›´æ–°

### æ€§èƒ½æµ‹è¯•é¡¹

- [ ] å»¶è¿ŸæŒ‡æ ‡ (TTFT, TPOT, E2E)
- [ ] ååé‡æŒ‡æ ‡ (Tokens/s, Requests/s)
- [ ] èµ„æºæŒ‡æ ‡ (æ˜¾å­˜ã€GPU åˆ©ç”¨ç‡)
- [ ] è´¨é‡æŒ‡æ ‡ (ä¸åŸæ¨¡å‹å¯¹æ¯”)
- [ ] ä¼˜åŒ–å‰åå¯¹æ¯”æ•°æ®

---

## ğŸš€ å¿«é€Ÿå‚è€ƒ

### è¿è¡Œæµ‹è¯•
```bash
# M1: åŸºç¡€æ¨ç†æµ‹è¯•
python examples/basic_inference.py

# M2: æ‰¹å¤„ç†æµ‹è¯•
python examples/batch_inference.py

# M3: Paged KV æµ‹è¯•
pytest tests/unit/test_block_manager.py

# æ€§èƒ½ Benchmark
python tests/benchmark/run_benchmark.py
```

### å…³é”®æ–‡ä»¶é€ŸæŸ¥ï¼ˆå¯¹é½ vLLM v1ï¼‰

| åŠŸèƒ½         | FoloVLLM æ–‡ä»¶                            | vLLM v1 å¯¹åº”æ–‡ä»¶                     |
| ------------ | ---------------------------------------- | ------------------------------------ |
| æ¨ç†å¼•æ“     | `folovllm/engine/llm_engine.py`          | `vllm/v1/engine/llm_engine.py`       |
| è°ƒåº¦å™¨       | `folovllm/core/sched/scheduler.py`       | `vllm/v1/core/sched/scheduler.py`    |
| KV Cache     | `folovllm/core/kv_cache_manager.py`      | `vllm/v1/core/kv_cache_manager.py`   |
| Block Pool   | `folovllm/core/block_pool.py`            | `vllm/v1/core/block_pool.py`         |
| Attention    | `folovllm/attention/backends/`           | `vllm/v1/attention/backends/`        |
| é‡‡æ ·         | `folovllm/sample/sampler.py`             | `vllm/v1/sample/sampler.py`          |
| æ¨¡å‹         | `folovllm/model_executor/models/qwen.py` | `vllm/model_executor/models/qwen.py` |
| Worker       | `folovllm/worker/gpu_worker.py`          | `vllm/v1/worker/gpu_worker.py`       |
| Model Runner | `folovllm/worker/model_runner.py`        | `vllm/v1/worker/gpu_model_runner.py` |

---

## ğŸ“š æ¨èå­¦ä¹ é¡ºåº

1. **ç†è®ºå‡†å¤‡** (1-2å¤©)
   - é˜…è¯» vLLM è®ºæ–‡
   - ç†è§£ Transformer æ¨ç†æµç¨‹
   - äº†è§£ KV Cache æ¦‚å¿µ

2. **å®è·µå¼€å‘** (æŒ‰ M0-M7 é¡ºåº)
   - æ¯å®Œæˆä¸€ä¸ªé˜¶æ®µï¼Œé˜…è¯»å¯¹åº”å­¦ä¹ ç¬”è®°
   - è¿è¡Œæ€§èƒ½æµ‹è¯•ï¼Œç†è§£ä¼˜åŒ–æ•ˆæœ
   - å¯¹æ¯” vLLM æºç ï¼Œç†è§£å¼‚åŒ

3. **æ·±åº¦ä¼˜åŒ–** (å¯é€‰)
   - å°è¯•ä¸åŒè¶…å‚æ•°é…ç½®
   - åˆ†ææ€§èƒ½ç“¶é¢ˆ
   - æ¢ç´¢è¿›ä¸€æ­¥ä¼˜åŒ–ç©ºé—´

4. **é¢è¯•å‡†å¤‡**
   - æ€»ç»“æ¯ä¸ªé˜¶æ®µçš„æ ¸å¿ƒæ¦‚å¿µ
   - å‡†å¤‡æŠ€æœ¯è¿½é—®çš„å›ç­”
   - æ•´ç†é¡¹ç›®äº®ç‚¹

---

**ä¸‹ä¸€æ­¥**: å¼€å§‹ [Milestone 0](../docs/dev/milestone_0.md) - é¡¹ç›®åˆå§‹åŒ–

