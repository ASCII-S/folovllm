# Milestone 1 å£è¿°å±•ç¤ºæ–‡æ¡£

> æœ¬æ–‡æ¡£ä»¥ç±»/å‡½æ•°ä¸ºå•ä½ï¼Œè¯¦ç»†è®²è§£ M1 åŸºç¡€ç¦»çº¿æ¨ç†çš„å®ç°è¿‡ç¨‹ï¼Œé€‚åˆå‘å°ç™½è®²è§£

---

## ğŸ“‹ æ–‡æ¡£ç»“æ„

æœ¬æ–‡æ¡£æŒ‰ç…§å¼€å‘é¡ºåºè®²è§£ï¼š

1. [Attention ç³»ç»Ÿå®ç°](#1-attention-ç³»ç»Ÿå®ç°)
2. [æ¨¡å‹å·¥å…·å®ç°](#2-æ¨¡å‹å·¥å…·å®ç°)
3. [Qwen3 æ¨¡å‹å®ç°](#3-qwen3-æ¨¡å‹å®ç°)
4. [é‡‡æ ·ç³»ç»Ÿå®ç°](#4-é‡‡æ ·ç³»ç»Ÿå®ç°)
5. [Worker å’Œ Executor å®ç°](#5-worker-å’Œ-executor-å®ç°)
6. [Engine å®ç°](#6-engine-å®ç°)
7. [å®Œæ•´æ¨ç†æµç¨‹ä¸²è®²](#7-å®Œæ•´æ¨ç†æµç¨‹ä¸²è®²)

---

## 1. Attention ç³»ç»Ÿå®ç°

### æ¶æ„ç±»å›¾

```mermaid
classDiagram
    %% æŠ½è±¡åŸºç±»
    class AttentionBackend {
        <<abstract>>
        +forward(query, key, value, kv_cache, scale, attn_mask)* Tuple
    }
    
    %% å…·ä½“å®ç°
    class TorchNaiveBackend {
        +forward(query, key, value, kv_cache, scale, attn_mask) Tuple
    }
    
    %% Attention å±‚
    class Attention {
        -int hidden_size
        -int num_heads
        -int num_kv_heads
        -int head_dim
        -float scaling
        -Linear qkv_proj
        -Linear o_proj
        -RotaryEmbedding rotary_emb
        -AttentionBackend backend
        +forward(positions, hidden_states, kv_cache) Tensor
    }
    
    %% ä½ç½®ç¼–ç 
    class RotaryEmbedding {
        -int dim
        -float base
        -int max_position_embeddings
        -Tensor inv_freq
        -Tensor cos_sin_cache
        +forward(positions, query, key) Tuple
    }
    
    %% å·¥å…·å‡½æ•°ï¼ˆç”¨ note è¡¨ç¤ºï¼‰
    note for AttentionOps "æ ¸å¿ƒå‡½æ•°:\nâ€¢ reshape_and_cache_kv()\nâ€¢ naive_attention()\nâ€¢ create_causal_mask()"
    class AttentionOps {
        <<utility>>
    }
    
    %% å…³ç³»
    AttentionBackend <|-- TorchNaiveBackend : å®ç°
    Attention *-- AttentionBackend : ç»„åˆ
    Attention *-- RotaryEmbedding : ç»„åˆ
    TorchNaiveBackend ..> AttentionOps : ä½¿ç”¨
    Attention ..> AttentionOps : ä½¿ç”¨
    
    %% æ³¨é‡Š
    note for AttentionBackend "ç­–ç•¥æ¨¡å¼ï¼šæ–¹ä¾¿åˆ‡æ¢\nFlash Attention/\nPaged Attention ç­‰åç«¯"
    note for Attention "å®Œæ•´çš„ Multi-Head Attention å±‚\nåŒ…å« QKV æŠ•å½±ã€RoPEã€O æŠ•å½±"
```

**ç±»å›¾è¯´æ˜**ï¼š

1. **æ ¸å¿ƒè®¾è®¡æ¨¡å¼**ï¼š
   - ä½¿ç”¨**ç­–ç•¥æ¨¡å¼**ï¼š`AttentionBackend` ä½œä¸ºæŠ½è±¡æ¥å£ï¼Œæ–¹ä¾¿åˆ‡æ¢ä¸åŒå®ç°
   - `TorchNaiveBackend` æ˜¯ M1 çš„æœ´ç´ å®ç°ï¼ŒM3/M4 ä¼šæ·»åŠ ä¼˜åŒ–ç‰ˆæœ¬

2. **ç»„ä»¶èŒè´£**ï¼š
   - `AttentionOps`ï¼šæä¾›åº•å±‚å·¥å…·å‡½æ•°ï¼ˆKV cache ç®¡ç†ã€attention è®¡ç®—ã€mask åˆ›å»ºï¼‰
   - `RotaryEmbedding`ï¼šæ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰
   - `AttentionBackend`ï¼šæ‰§è¡Œå®é™…çš„ attention è®¡ç®—
   - `Attention`ï¼šå®Œæ•´çš„ attention å±‚ï¼Œåè°ƒæ‰€æœ‰ç»„ä»¶

3. **å…³ç³»è¯´æ˜**ï¼š
   - **ç»§æ‰¿**ï¼š`TorchNaiveBackend` å®ç° `AttentionBackend` æ¥å£
   - **ç»„åˆ**ï¼š`Attention` åŒ…å« `RotaryEmbedding` å’Œ `AttentionBackend`
   - **ä¾èµ–**ï¼š`Attention` å’Œ `TorchNaiveBackend` ä½¿ç”¨ `AttentionOps` ä¸­çš„å‡½æ•°

---

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦ Attention ç³»ç»Ÿï¼Ÿ

**åœºæ™¯æ€è€ƒ**ï¼š
```
é—®é¢˜ï¼šTransformer ç”Ÿæˆæ¯ä¸ª token æ—¶ï¼Œéƒ½è¦é‡æ–°è®¡ç®—ä¹‹å‰æ‰€æœ‰ token çš„ K å’Œ V å—ï¼Ÿ
ç­”æ¡ˆï¼šä¸éœ€è¦ï¼æˆ‘ä»¬ç”¨ KV Cache ç¼“å­˜å·²è®¡ç®—çš„ K å’Œ V

é—®é¢˜ï¼šå¦‚ä½•è®¾è®¡ä¸€ä¸ªçµæ´»çš„ Attention ç³»ç»Ÿï¼Œæ–¹ä¾¿åç»­åˆ‡æ¢ä¼˜åŒ–å®ç°ï¼Ÿ
ç­”æ¡ˆï¼šä½¿ç”¨æŠ½è±¡æ¥å£ + ä¸åŒåç«¯å®ç°
```

### 1.2 `reshape_and_cache_kv()` - KV Cache æ ¸å¿ƒå‡½æ•°

**ä½ç½®**ï¼š`folovllm/attention/ops.py`

**ç›®çš„**ï¼šå°†æ–°è®¡ç®—çš„ Kã€V è¿½åŠ åˆ°ç¼“å­˜ä¸­

#### å®ç°æ€è·¯

```python
def reshape_and_cache_kv(
    key: torch.Tensor,      # æ–° token çš„ K
    value: torch.Tensor,    # æ–° token çš„ V
    kv_cache: Tuple[torch.Tensor, torch.Tensor],  # å†å²ç¼“å­˜
    slot_mapping: Optional[torch.Tensor] = None,  # M3 ä¼šç”¨åˆ°
) -> Tuple[torch.Tensor, torch.Tensor]:
```

**å£è¿°è®²è§£**ï¼š

**æ­¥éª¤1ï¼šåˆ¤æ–­æ˜¯å¦é¦–æ¬¡è°ƒç”¨**
```python
key_cache, value_cache = kv_cache

if key_cache.numel() == 0:
    # ç¬¬ä¸€ä¸ª tokenï¼šåˆå§‹åŒ–ç¼“å­˜
    # key shape: [batch_size, num_kv_heads, head_dim]
    # éœ€è¦å¢åŠ  seq_len ç»´åº¦ -> [batch_size, num_kv_heads, 1, head_dim]
    key_cache = key.unsqueeze(2)
    value_cache = value.unsqueeze(2)
```

**ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡**ï¼š
- ç¬¬ä¸€æ¬¡æ²¡æœ‰ç¼“å­˜ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ª 4D tensor
- `unsqueeze(2)` åœ¨ç¬¬ 3 ä¸ªç»´åº¦ï¼ˆseq_lenï¼‰å¢åŠ ç»´åº¦

**æ­¥éª¤2ï¼šåç»­ token è¿½åŠ åˆ°ç¼“å­˜**
```python
else:
    # å·²æœ‰ç¼“å­˜ï¼šè¿½åŠ æ–°çš„ K/V
    key = key.unsqueeze(2)  # [batch, heads, 1, dim]
    value = value.unsqueeze(2)
    
    # æ²¿ç€ seq_len ç»´åº¦æ‹¼æ¥
    key_cache = torch.cat([key_cache, key], dim=2)
    value_cache = torch.cat([value_cache, value], dim=2)
```

**å…³é”®ç‚¹**ï¼š
- `torch.cat` ä¼šåˆ›å»ºæ–° tensorï¼Œå¤åˆ¶æ‰€æœ‰æ•°æ®
- M1 ä½¿ç”¨è¿ç»­å†…å­˜ï¼ŒM3 ä¼šæ”¹ç”¨ Paged Attention ä¼˜åŒ–

**å®Œæ•´æµç¨‹å›¾**ï¼š
```
ç¬¬ä¸€ä¸ª token:
  key: [2, 4, 64] -> unsqueeze(2) -> [2, 4, 1, 64]
  
ç¬¬äºŒä¸ª token:
  key: [2, 4, 64] -> unsqueeze(2) -> [2, 4, 1, 64]
  cache: [2, 4, 1, 64]
  cat -> [2, 4, 2, 64]
  
ç¬¬ N ä¸ª token:
  cache: [2, 4, N-1, 64]
  new: [2, 4, 1, 64]
  cat -> [2, 4, N, 64]
```

### 1.3 `naive_attention()` - æœ´ç´  Attention å®ç°

**ç›®çš„**ï¼šè®¡ç®— Attention(Q, K, V)ï¼Œä¸åšä»»ä½•ä¼˜åŒ–

#### æ•°å­¦åŸç†

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

#### å®ç°è®²è§£

**æ­¥éª¤1ï¼šå¤„ç† Grouped Query Attention (GQA)**

```python
batch_size, num_heads, seq_len_q, head_dim = query.shape
_, num_kv_heads, seq_len_k, _ = key.shape

if num_heads > num_kv_heads:
    # GQA: Q heads å¤šäº KV heads
    # ä¾‹å¦‚ï¼š32 ä¸ª Q heads, 8 ä¸ª KV heads
    # éœ€è¦é‡å¤ KV heads ä»¥åŒ¹é…
    num_repeats = num_heads // num_kv_heads
    key = key.unsqueeze(2).expand(...).reshape(...)
    value = value.unsqueeze(2).expand(...).reshape(...)
```

**ä¸ºä»€ä¹ˆ**ï¼š
- Qwen3 ä½¿ç”¨ GQAï¼ˆ16 Q heads, 2 KV headsï¼‰
- å‡å°‘ KV cache å¤§å°ï¼ŒèŠ‚çœæ˜¾å­˜
- ä½†è®¡ç®—æ—¶éœ€è¦å°† KV é‡å¤ä»¥åŒ¹é… Q

**æ­¥éª¤2ï¼šè®¡ç®— Attention Scores**

```python
# Q @ K^T
attn_weights = torch.matmul(query, key.transpose(-2, -1)) * scale

# scale = 1/sqrt(head_dim)ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
```

**ä¸ºä»€ä¹ˆè¦ scale**ï¼š
- QÂ·K çš„æ–¹å·®ä¼šéš head_dim å¢å¤§
- ä¸ scale çš„è¯ï¼Œsoftmax ä¼šé€€åŒ–æˆ one-hotï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰

**æ­¥éª¤3ï¼šåº”ç”¨ Causal Mask**

```python
if attn_mask is not None:
    # mask: [1, 1, seq_len_q, seq_len_k]
    # mask å€¼ä¸º 0ï¼ˆå…è®¸ï¼‰æˆ– -infï¼ˆç¦æ­¢ï¼‰
    attn_weights = attn_weights + attn_mask
```

**Causal Mask ç¤ºä¾‹**ï¼š
```
seq_len = 4 æ—¶çš„ mask (0=å…è®¸, -inf=ç¦æ­¢):
  0  -âˆ  -âˆ  -âˆ
  0   0  -âˆ  -âˆ
  0   0   0  -âˆ
  0   0   0   0
```

**æ­¥éª¤4ï¼šSoftmax + ä¸ V ç›¸ä¹˜**

```python
# Softmax (åœ¨ float32 ä¸‹è®¡ç®—ï¼Œé¿å…æ•°å€¼ä¸ç¨³å®š)
attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)

# @ V
output = torch.matmul(attn_weights, value)
```

### 1.4 `create_causal_mask()` - åˆ›å»ºå› æœæ©ç 

**ç›®çš„**ï¼šç”Ÿæˆä¸Šä¸‰è§’ maskï¼Œç¡®ä¿æ¯ä¸ª token åªèƒ½çœ‹åˆ°ä¹‹å‰çš„ token

```python
def create_causal_mask(
    seq_len_q: int,    # Query åºåˆ—é•¿åº¦
    seq_len_k: int,    # Key åºåˆ—é•¿åº¦
    device: torch.device,
    dtype: torch.dtype = torch.float32,
) -> torch.Tensor:
```

#### å®ç°ç»†èŠ‚

```python
# åˆ›å»ºå…¨ 1 çŸ©é˜µ
mask = torch.ones(seq_len_q, seq_len_k, device=device, dtype=dtype)

# ä¿ç•™ä¸Šä¸‰è§’ï¼ˆä¸åŒ…æ‹¬å¯¹è§’çº¿ï¼‰
# diagonal=seq_len_k - seq_len_q + 1 å¤„ç† decode é˜¶æ®µ
mask = torch.triu(mask, diagonal=seq_len_k - seq_len_q + 1)

# å°†éœ€è¦ mask çš„ä½ç½®è®¾ä¸º -inf
mask = mask.masked_fill(mask == 1, float('-inf'))

# å¢åŠ  batch å’Œ head ç»´åº¦
mask = mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len_q, seq_len_k]
```

**å…³é”®ç†è§£**ï¼š

**Prefill é˜¶æ®µ**ï¼š`seq_len_q == seq_len_k`
```python
# ä¾‹å¦‚ seq_len = 3
diagonal = 3 - 3 + 1 = 1
mask = triu(ones(3, 3), diagonal=1)

ç»“æœï¼š
[[0, 1, 1],      [[  0, -âˆ, -âˆ],
 [0, 0, 1],  ->   [  0,   0, -âˆ],
 [0, 0, 0]]       [  0,   0,   0]]
```

**Decode é˜¶æ®µ**ï¼š`seq_len_q = 1, seq_len_k = 5`ï¼ˆå·²æœ‰ 4 ä¸ªå†å² tokenï¼‰
```python
diagonal = 5 - 1 + 1 = 5
mask = triu(ones(1, 5), diagonal=5)

ç»“æœï¼š
[[0, 0, 0, 0, 0]]  # å…¨æ˜¯ 0ï¼Œå¯ä»¥çœ‹æ‰€æœ‰å†å² token
```

### 1.5 `AttentionBackend` - æŠ½è±¡åç«¯æ¥å£

**ä½ç½®**ï¼š`folovllm/attention/backends/abstract.py`

**ç›®çš„**ï¼šå®šä¹‰ç»Ÿä¸€æ¥å£ï¼Œæ–¹ä¾¿åˆ‡æ¢ä¸åŒ Attention å®ç°

```python
class AttentionBackend(ABC):
    @abstractmethod
    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        kv_cache: Optional[Tuple],
        scale: float,
        attn_mask: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, Tuple]:
        """
        è¿”å›: (output, updated_kv_cache)
        """
        raise NotImplementedError
```

**è®¾è®¡æ¨¡å¼**ï¼šç­–ç•¥æ¨¡å¼ï¼ˆStrategy Patternï¼‰

**å¥½å¤„**ï¼š
```python
# M1: ä½¿ç”¨ Naive backend
backend = TorchNaiveBackend()

# M3: æ— ç¼åˆ‡æ¢åˆ° Paged Attention
backend = PagedAttentionBackend()

# M4: åˆ‡æ¢åˆ° Flash Attention
backend = FlashAttentionBackend()
```

### 1.6 `TorchNaiveBackend` - æœ´ç´ å®ç°

```python
class TorchNaiveBackend(AttentionBackend):
    def forward(self, query, key, value, kv_cache, scale, attn_mask=None):
        # 1. åˆå§‹åŒ–æˆ–æ›´æ–°ç¼“å­˜
        if kv_cache is None:
            key_cache = torch.empty(0)
            value_cache = torch.empty(0)
        else:
            key_cache, value_cache = kv_cache
        
        # 2. å¤„ç† decode/prefill ä¸¤ç§æƒ…å†µ
        if key.dim() == 3:  # Decode: [batch, heads, dim]
            key_cache, value_cache = reshape_and_cache_kv(...)
            key = key_cache
            value = value_cache
        elif key.dim() == 4:  # Prefill: [batch, heads, seq_len, dim]
            if key_cache.numel() == 0:
                key_cache = key
                value_cache = value
            else:
                key_cache = torch.cat([key_cache, key], dim=2)
                value_cache = torch.cat([value_cache, value], dim=2)
            key = key_cache
            value = value_cache
        
        # 3. è¿è¡Œ attention
        output = naive_attention(query, key, value, scale, attn_mask)
        
        return output, (key_cache, value_cache)
```

**å…³é”®ç‚¹**ï¼š
- é€šè¿‡ `key.dim()` åˆ¤æ–­æ˜¯ decode è¿˜æ˜¯ prefill
- Decode: key æ˜¯ 3Dï¼Œéœ€è¦è¿½åŠ åˆ°ç¼“å­˜
- Prefill: key æ˜¯ 4Dï¼Œç›´æ¥ä½œä¸ºç¼“å­˜

### 1.7 `Attention` Layer - é€šç”¨ Attention å±‚

**ä½ç½®**ï¼š`folovllm/model_executor/layers/attention.py`

**ç›®çš„**ï¼šå°è£…å®Œæ•´çš„ Attention æ¨¡å—ï¼ŒåŒ…æ‹¬ QKV æŠ•å½±ã€RoPEã€Attention è®¡ç®—ã€è¾“å‡ºæŠ•å½±

#### ç±»ç»“æ„

```python
class Attention(nn.Module):
    def __init__(
        self,
        hidden_size: int,        # éšè—å±‚å¤§å°
        num_heads: int,          # Q heads æ•°é‡
        num_kv_heads: int,       # KV heads æ•°é‡ï¼ˆGQAï¼‰
        head_dim: Optional[int], # æ¯ä¸ª head çš„ç»´åº¦
        max_position_embeddings: int,
        rope_theta: float,
        rope_scaling: Optional[float],
        bias: bool,
        backend: Optional[AttentionBackend],
    ):
```

#### åˆå§‹åŒ–ç»„ä»¶

```python
# 1. QKV æŠ•å½±ï¼ˆåˆå¹¶ä¸ºä¸€ä¸ª linearï¼‰
self.qkv_proj = nn.Linear(
    hidden_size,
    self.q_size + 2 * self.kv_size,  # Q + K + V
    bias=bias,
)

# 2. è¾“å‡ºæŠ•å½±
self.o_proj = nn.Linear(self.q_size, hidden_size, bias=bias)

# 3. RoPE
self.rotary_emb = RotaryEmbedding(...)

# 4. Attention backend
self.backend = backend or TorchNaiveBackend()

# 5. KV cacheï¼ˆå¤–éƒ¨è®¾ç½®ï¼‰
self.kv_cache = None
```

**ä¸ºä»€ä¹ˆåˆå¹¶ QKV æŠ•å½±**ï¼š
- å•ä¸ª linear layer æ¯”ä¸‰ä¸ªåˆ†ç¦»çš„æ›´é«˜æ•ˆ
- å‡å°‘ kernel launch overhead
- å†…å­˜è®¿é—®æ›´å‹å¥½

#### Forward å®ç°

```python
def forward(
    self,
    positions: torch.Tensor,     # ä½ç½®ç´¢å¼•
    hidden_states: torch.Tensor, # [batch, seq_len, hidden_size]
    kv_cache: Optional[Tuple],
) -> torch.Tensor:
```

**æ­¥éª¤1ï¼šQKV æŠ•å½±å’Œ Reshape**

```python
# QKV æŠ•å½±
qkv = self.qkv_proj(hidden_states)  # [batch, seq_len, q_size + 2*kv_size]

# åˆ†å‰² Qã€Kã€V
q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)

# Reshape ä¸º multi-head æ ¼å¼
q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)
k = k.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)
v = v.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)

# Transpose åˆ° [batch, heads, seq_len, dim]
q = q.transpose(1, 2)
k = k.transpose(1, 2)
v = v.transpose(1, 2)
```

**æ­¥éª¤2ï¼šåº”ç”¨ RoPE**

```python
q, k = self.rotary_emb(positions, q, k)
```

**æ­¥éª¤3ï¼šå¤„ç† Decode æƒ…å†µ**

```python
if seq_len == 1:
    # Decode: ç§»é™¤ seq_len ç»´åº¦
    k = k.squeeze(2)  # [batch, num_kv_heads, head_dim]
    v = v.squeeze(2)
```

**æ­¥éª¤4ï¼šåˆ›å»º Causal Maskï¼ˆæ”¯æŒå¢é‡ Prefillï¼‰**

è¿™é‡Œçš„è®¾è®¡è€ƒè™‘äº†å¢é‡ prefill çš„åœºæ™¯ï¼š
- **å¢é‡ prefill**ï¼šå½“ KV cache ä¸­å·²æœ‰éƒ¨åˆ† tokensï¼Œæ–°æ¥çš„ tokens éœ€è¦ä¸å†å² tokens è¿›è¡Œ attention
- **cache_len**ï¼šå·²ç¼“å­˜çš„ token æ•°é‡
- **total_len**ï¼šç¼“å­˜é•¿åº¦ + å½“å‰åºåˆ—é•¿åº¦ï¼Œè¡¨ç¤ºå®Œæ•´çš„ä¸Šä¸‹æ–‡é•¿åº¦
- **causal mask**ï¼šç¡®ä¿å½“å‰ tokens åªèƒ½çœ‹åˆ°è‡ªå·±å’Œä¹‹å‰çš„ tokensï¼ˆåŒ…æ‹¬ç¼“å­˜ä¸­çš„ï¼‰

è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹å¯ä»¥ï¼š
1. å¤„ç†é•¿åºåˆ—çš„åˆ†å— prefill
2. åœ¨å·²æœ‰ KV cache åŸºç¡€ä¸Šç»§ç»­ç”Ÿæˆ
3. ä¿æŒå› æœæ€§çº¦æŸçš„æ­£ç¡®æ€§

```python
attn_mask = None
if seq_len > 1:
    cache_len = 0
    if kv_cache is not None and kv_cache[0].numel() > 0:
        cache_len = kv_cache[0].shape[2]
    
    total_len = cache_len + seq_len
    attn_mask = create_causal_mask(seq_len, total_len, device, dtype)
```

**æ­¥éª¤5ï¼šè¿è¡Œ Attention**

```python
output, kv_cache = self.backend.forward(
    q, k, v, kv_cache, self.scaling, attn_mask
)
```

**æ­¥éª¤6ï¼šè¾“å‡ºæŠ•å½±**

```python
# Transpose å›æ¥: [batch, heads, seq_len, dim] -> [batch, seq_len, heads, dim]
output = output.transpose(1, 2).contiguous()

# Reshape: [batch, seq_len, num_heads * head_dim]
output = output.view(batch_size, seq_len, self.q_size)

# è¾“å‡ºæŠ•å½±
output = self.o_proj(output)
```

---

## 2. æ¨¡å‹å·¥å…·å®ç°

### 2.1 `RMSNorm` - Root Mean Square å½’ä¸€åŒ–

**ä½ç½®**ï¼š`folovllm/model_executor/models/utils.py`

**ä¸ºä»€ä¹ˆç”¨ RMSNorm è€Œä¸æ˜¯ LayerNorm**ï¼š
- LayerNorm: $\text{norm}(x) = \frac{x - \mu}{\sigma}$ï¼ˆéœ€è¦è®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼‰
- RMSNorm: $\text{norm}(x) = \frac{x}{\text{RMS}(x)}$ï¼ˆåªéœ€è¦è®¡ç®— RMSï¼‰
- **æ›´å¿«**ï¼šå°‘ä¸€ä¸ªå‡æ³•å’Œå‡å€¼è®¡ç®—
- **æ•ˆæœç›¸å½“**ï¼šå®éªŒè¡¨æ˜ re-centeringï¼ˆå‡å‡å€¼ï¼‰å¯¹æ€§èƒ½æå‡æœ‰é™

#### æ•°å­¦å…¬å¼

$$\text{RMS}(x) = \sqrt{\frac{1}{d}\sum_{i=1}^d x_i^2}$$

$$\text{RMSNorm}(x) = \frac{x}{\text{RMS}(x)} \cdot \gamma$$

#### å®ç°è®²è§£

```python
class RMSNorm(nn.Module):
    def __init__(self, hidden_size: int, eps: float = 1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps
```

**Forward with Fused Residual**ï¼š

```python
def forward(
    self,
    hidden_states: torch.Tensor,
    residual: Optional[torch.Tensor] = None,
) -> Tuple[torch.Tensor, torch.Tensor]:
    input_dtype = hidden_states.dtype
    
    # Fused residual: å…ˆåŠ å†å½’ä¸€åŒ–
    if residual is not None:
        new_residual = hidden_states + residual
        hidden_states = new_residual.to(torch.float32)
    else:
        new_residual = hidden_states
        hidden_states = hidden_states.to(torch.float32)
    
    # è®¡ç®— RMS
    variance = hidden_states.pow(2).mean(-1, keepdim=True)
    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
    
    # åº”ç”¨ weight
    hidden_states = self.weight * hidden_states.to(input_dtype)
    
    return hidden_states, new_residual
```

**Fused Residual çš„å¥½å¤„**ï¼š
```python
# ä¸ fusedï¼ˆä¸¤æ¬¡å†…å­˜è®¿é—®ï¼‰:
x = x + residual
x = norm(x)

# Fusedï¼ˆä¸€æ¬¡å†…å­˜è®¿é—®ï¼‰:
x_norm, residual = norm(x, residual)
```

**ä¸ºä»€ä¹ˆè¿”å›ä¸¤ä¸ªå€¼**ï¼š
- `hidden_states`: å½’ä¸€åŒ–åçš„è¾“å‡ºï¼ˆç»™ä¸‹ä¸€å±‚ï¼‰
- `new_residual`: `x + residual`ï¼ˆç»™ä¸‹ä¸‹å±‚ä½œä¸º residualï¼‰

### 2.2 `RotaryEmbedding` - æ—‹è½¬ä½ç½®ç¼–ç 

**RoPE çš„æ ¸å¿ƒæ€æƒ³**ï¼šä¸åœ¨è¾“å…¥åŠ ä½ç½®ç¼–ç ï¼Œè€Œæ˜¯**åœ¨ attention ä¸­æ—‹è½¬ Q å’Œ K**

#### æ•°å­¦åŸç†

å¯¹äºä½ç½® $m$ å’Œ $n$ çš„ tokenï¼š

$$\text{score}(m, n) = q_m^T k_n = \text{rotate}(q, m\theta)^T \text{rotate}(k, n\theta)$$

**å…³é”®æ€§è´¨**ï¼š
$$\text{rotate}(q, m\theta)^T \text{rotate}(k, n\theta) = \text{rotate}(q^T k, (m-n)\theta)$$

**ç»“è®º**ï¼šAttention score åªä¾èµ–**ç›¸å¯¹ä½ç½®** $m-n$ï¼

#### å®ç°ç»†èŠ‚

**æ­¥éª¤1ï¼šé¢„è®¡ç®—æ—‹è½¬é¢‘ç‡**

```python
def __init__(self, dim: int, max_position_embeddings: int, base: float):
    # è®¡ç®—é¢‘ç‡ï¼ˆå‡ ä½•çº§æ•°ï¼‰
    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
    self.register_buffer("inv_freq", inv_freq)
    
    # Î¸_i = base^(-2i/d)
    # ä¾‹å¦‚ dim=64, base=10000:
    # Î¸_0 = 1.0
    # Î¸_1 = 0.1
    # Î¸_2 = 0.01
    # ...
```

**æ­¥éª¤2ï¼šæ›´æ–° cos/sin ç¼“å­˜**

```python
def _update_cos_sin_cache(self, seq_len, device, dtype):
    # ç”Ÿæˆä½ç½®ç´¢å¼•
    t = torch.arange(seq_len, device=device)
    
    # è®¡ç®—é¢‘ç‡
    freqs = torch.outer(t, self.inv_freq)  # [seq_len, dim//2]
    
    # é‡å¤ä¸€æ¬¡ï¼ˆç”¨äºå®éƒ¨å’Œè™šéƒ¨ï¼‰
    emb = torch.cat([freqs, freqs], dim=-1)  # [seq_len, dim]
    
    # è®¡ç®— cos å’Œ sin
    self._cos_cached = emb.cos()
    self._sin_cached = emb.sin()
```

**æ­¥éª¤3ï¼šåº”ç”¨æ—‹è½¬**

```python
def forward(self, positions, query, key):
    # è·å– cos/sin
    max_pos = positions.max() + 1
    self._update_cos_sin_cache(max_pos, positions.device, query.dtype)
    
    cos = self._cos_cached[positions]
    sin = self._sin_cached[positions]
    
    # åº”ç”¨æ—‹è½¬
    query = self._apply_rotary_emb(query, cos, sin)
    key = self._apply_rotary_emb(key, cos, sin)
    
    return query, key
```

**æ­¥éª¤4ï¼šæ—‹è½¬å®ç°**

```python
@staticmethod
def _apply_rotary_emb(x, cos, sin):
    # åˆ†æˆä¸¤åŠ
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    
    # ç»´åº¦å¯¹é½
    if x.dim() == 4:  # [batch, heads, seq_len, dim]
        if cos.dim() == 3:  # [batch, seq_len, dim]
            cos = cos.unsqueeze(1)  # [batch, 1, seq_len, dim]
            sin = sin.unsqueeze(1)
    elif x.dim() == 3:  # [batch, heads, dim]
        if cos.dim() == 2:  # [batch, dim]
            cos = cos.unsqueeze(1)  # [batch, 1, dim]
            sin = sin.unsqueeze(1)
    
    # åˆ†å‰² cos/sin
    cos1 = cos[..., : cos.shape[-1] // 2]
    cos2 = cos[..., cos.shape[-1] // 2 :]
    sin1 = sin[..., : sin.shape[-1] // 2]
    sin2 = sin[..., sin.shape[-1] // 2 :]
    
    # åº”ç”¨æ—‹è½¬ï¼ˆå¤æ•°ä¹˜æ³•ï¼‰
    rotated = torch.cat([
        x1 * cos1 - x2 * sin1,  # å®éƒ¨
        x1 * sin2 + x2 * cos2,  # è™šéƒ¨
    ], dim=-1)
    
    return rotated
```

**å‡ ä½•ç†è§£**ï¼š
```
åŸå‘é‡: [x1, x2]
æ—‹è½¬å: [x1*cos - x2*sin, x1*sin + x2*cos]

è¿™å°±æ˜¯ 2D å¹³é¢ä¸Šçš„æ—‹è½¬çŸ©é˜µï¼š
[cos  -sin] [x1]
[sin   cos] [x2]
```

### 2.3 `SiLUAndMul` - Fused æ¿€æ´»å‡½æ•°

**SiLU (Swish)**ï¼š$\text{SiLU}(x) = x \cdot \sigma(x)$

**ç”¨äº Gated MLP**ï¼š
```python
# ä¸ fused:
gate = linear_gate(x)
up = linear_up(x)
output = silu(gate) * up

# Fused:
gate_up = linear_gate_up(x)  # ä¸€æ¬¡ linear
output = silu_and_mul(gate_up)  # ä¸€æ¬¡ kernel
```

#### å®ç°

```python
class SiLUAndMul(nn.Module):
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: [..., 2 * hidden_size]
        gate, up = x.chunk(2, dim=-1)
        return F.silu(gate) * up
```

**ä¸ºä»€ä¹ˆ Fused**ï¼š
- å‡å°‘å†…å­˜è®¿é—®
- å‡å°‘ kernel launch
- æå‡æ€§èƒ½

---

## 3. Qwen3 æ¨¡å‹å®ç°

### 3.1 æ¨¡å‹æ¶æ„æ¦‚è§ˆ

```
Qwen3ForCausalLM
â”œâ”€â”€ Qwen3Model
â”‚   â”œâ”€â”€ Embedding
â”‚   â”œâ”€â”€ Qwen3DecoderLayer Ã— N
â”‚   â”‚   â”œâ”€â”€ Qwen3Attention
â”‚   â”‚   â”‚   â”œâ”€â”€ QKV Projection
â”‚   â”‚   â”‚   â”œâ”€â”€ RoPE
â”‚   â”‚   â”‚   â”œâ”€â”€ Attention
â”‚   â”‚   â”‚   â””â”€â”€ O Projection
â”‚   â”‚   â”œâ”€â”€ RMSNorm
â”‚   â”‚   â”œâ”€â”€ Qwen3MLP
â”‚   â”‚   â”‚   â”œâ”€â”€ Gate + Up Projection (merged)
â”‚   â”‚   â”‚   â”œâ”€â”€ SiLU Activation
â”‚   â”‚   â”‚   â””â”€â”€ Down Projection
â”‚   â”‚   â””â”€â”€ RMSNorm
â”‚   â””â”€â”€ Final RMSNorm
â””â”€â”€ LM Head
```

### 3.2 `Qwen3Attention` - Attention å±‚å°è£…

```python
class Qwen3Attention(nn.Module):
    def __init__(self, config: Qwen2Config):
        super().__init__()
        # ä½¿ç”¨æˆ‘ä»¬çš„é€šç”¨ Attention å±‚
        self.attn = Attention(
            hidden_size=config.hidden_size,
            num_heads=config.num_attention_heads,
            num_kv_heads=config.num_key_value_heads,
            head_dim=getattr(config, 'head_dim', None),
            max_position_embeddings=config.max_position_embeddings,
            rope_theta=getattr(config, "rope_theta", 1000000.0),
            bias=getattr(config, 'attention_bias', True),
        )
    
    def forward(self, positions, hidden_states, kv_cache=None):
        return self.attn(positions, hidden_states, kv_cache)
```

**è®¾è®¡æ€è·¯**ï¼š
- è–„å°è£…ï¼Œä¸»è¦é€»è¾‘åœ¨é€šç”¨ `Attention` ç±»
- ä» config æå–å‚æ•°
- æ–¹ä¾¿åç»­æ›¿æ¢ä¸åŒçš„ attention å®ç°

### 3.3 `Qwen3MLP` - Gated FFN

```python
class Qwen3MLP(nn.Module):
    def __init__(self, config: Qwen2Config):
        super().__init__()
        # Gate å’Œ Up æŠ•å½±åˆå¹¶
        self.gate_up_proj = nn.Linear(
            config.hidden_size,
            2 * config.intermediate_size,
            bias=False,
        )
        # Down æŠ•å½±
        self.down_proj = nn.Linear(
            config.intermediate_size,
            config.hidden_size,
            bias=False,
        )
        # SiLU + é€å…ƒç´ ä¹˜æ³•
        self.act_fn = SiLUAndMul()
    
    def forward(self, hidden_states):
        # [h] -> [2*i]
        gate_up = self.gate_up_proj(hidden_states)
        # [2*i] -> [i]
        hidden_states = self.act_fn(gate_up)
        # [i] -> [h]
        hidden_states = self.down_proj(hidden_states)
        return hidden_states
```

**æ•°å­¦è¡¨è¾¾**ï¼š
$$\text{MLP}(x) = W_{\text{down}} \left( \text{SiLU}(W_{\text{gate}}x) \odot W_{\text{up}}x \right)$$

### 3.4 `Qwen3DecoderLayer` - Transformer å±‚

```python
class Qwen3DecoderLayer(nn.Module):
    def __init__(self, config, layer_idx):
        super().__init__()
        self.self_attn = Qwen3Attention(config)
        self.mlp = Qwen3MLP(config)
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
```

**Forward with Pre-Norm + Fused Residual**ï¼š

```python
def forward(self, positions, hidden_states, residual, kv_cache=None):
    # 1. Attention block
    if residual is None:
        residual = hidden_states
        hidden_states, _ = self.input_layernorm(hidden_states, residual=None)
    else:
        hidden_states, residual = self.input_layernorm(hidden_states, residual)
    
    hidden_states = self.self_attn(positions, hidden_states, kv_cache)
    
    # 2. MLP block
    hidden_states, residual = self.post_attention_layernorm(hidden_states, residual)
    hidden_states = self.mlp(hidden_states)
    
    return hidden_states, residual
```

**Residual æµç¨‹å›¾**ï¼š
```
Layer 0:
  input: xâ‚€
  residual = None
  norm(xâ‚€) -> attn -> hâ‚
  residual = xâ‚€ (from norm)
  norm(hâ‚, residual=xâ‚€) -> hâ‚ + xâ‚€ -> mlp -> hâ‚‚
  residual = hâ‚ + xâ‚€
  return hâ‚‚, residual

Layer 1:
  input: hâ‚‚, residual = hâ‚ + xâ‚€
  norm(hâ‚‚, residual=hâ‚+xâ‚€) -> (hâ‚‚+hâ‚+xâ‚€) -> attn -> hâ‚ƒ
  residual = hâ‚‚ + hâ‚ + xâ‚€
  ...
```

### 3.5 `Qwen3Model` - å®Œæ•´æ¨¡å‹

```python
class Qwen3Model(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.layers = nn.ModuleList([
            Qwen3DecoderLayer(config, layer_idx)
            for layer_idx in range(config.num_hidden_layers)
        ])
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
    
    def forward(self, input_ids, positions, kv_caches=None):
        # Embedding
        hidden_states = self.embed_tokens(input_ids)
        
        # Transformer layers
        residual = None
        for layer_idx, layer in enumerate(self.layers):
            kv_cache = kv_caches[layer_idx] if kv_caches else None
            hidden_states, residual = layer(positions, hidden_states, residual, kv_cache)
        
        # Final norm
        hidden_states, _ = self.norm(hidden_states, residual)
        
        return hidden_states
```

### 3.6 ä½¿ç”¨ HuggingFace æ¨¡å‹ï¼ˆM1 å®é™…æ–¹æ¡ˆï¼‰

**ä¸ºä»€ä¹ˆæ”¹ç”¨ HF æ¨¡å‹**ï¼š
- è‡ªå®šä¹‰æ¨¡å‹æ¶æ„ä¸ HF ä¸å®Œå…¨åŒ¹é…ï¼ˆqkv_proj vs q/k/v_projï¼‰
- HF æ¨¡å‹æœ‰ q_norm å’Œ k_norm å±‚
- æƒé‡åŠ è½½å¤æ‚ï¼Œä¸ºäº† M1 ç¨³å®šæ€§ï¼Œç›´æ¥ç”¨ HF

**å°è£…æ–¹æ³•**ï¼š

```python
# folovllm/model_loader.py
def _wrap_model_for_folovllm(self, model):
    # æ·»åŠ  compute_logits æ–¹æ³•
    if not hasattr(model, 'compute_logits'):
        def compute_logits(hidden_states):
            return model.lm_head(hidden_states)
        
        import types
        model.compute_logits = types.MethodType(compute_logits, model)
    
    return model
```

**å…¼å®¹ HF çš„ KV Cache**ï¼š

```python
# folovllm/worker/model_runner.py
# HF æ¨¡å‹ä½¿ç”¨ past_key_values
outputs = self.model(
    input_ids=input_ids,
    position_ids=positions,
    past_key_values=self.past_key_values,  # HF çš„ cache
    use_cache=True,
    return_dict=True,
)
logits = outputs.logits
self.past_key_values = outputs.past_key_values  # æ›´æ–° cache
```

---

## 4. é‡‡æ ·ç³»ç»Ÿå®ç°

### 4.1 é‡‡æ ·ç­–ç•¥æ¦‚è¿°

```
Logits [vocab_size]
  â†“ apply temperature
Logits / T
  â†“ apply min_p
Filtered Logits
  â†“ apply top_k
Filtered Logits
  â†“ apply top_p
Final Filtered Logits
  â†“ softmax
Probabilities
  â†“ greedy or sample
Next Token
```

### 4.2 `apply_top_k_filtering()` - Top-k è¿‡æ»¤

**ç›®çš„**ï¼šåªä¿ç•™æ¦‚ç‡æœ€é«˜çš„ k ä¸ª token

```python
def apply_top_k_filtering(logits: torch.Tensor, top_k: int) -> torch.Tensor:
    if top_k <= 0:
        return logits  # ä¸è¿‡æ»¤
    
    # è·å– top-k å€¼å’Œç´¢å¼•
    top_k = min(top_k, logits.size(-1))
    top_k_values, top_k_indices = torch.topk(logits, top_k, dim=-1)
    
    # åˆ›å»ºå…¨ -inf tensor
    filtered_logits = torch.full_like(logits, float('-inf'))
    
    # åªå¡«å…¥ top-k çš„å€¼
    filtered_logits.scatter_(-1, top_k_indices, top_k_values)
    
    return filtered_logits
```

**ç¤ºä¾‹**ï¼š
```python
logits = [5.0, 3.0, 1.0, 4.0, 2.0]
top_k = 3

top_k_values = [5.0, 4.0, 3.0]
top_k_indices = [0, 3, 1]

filtered = [-inf, 3.0, -inf, 4.0, -inf]
                 â†‘              â†‘
              ä¿ç•™         ä¿ç•™
```

### 4.3 `apply_top_p_filtering()` - Nucleus é‡‡æ ·

**ç›®çš„**ï¼šåŠ¨æ€é€‰æ‹©ï¼Œä¿ç•™ç´¯ç§¯æ¦‚ç‡ â‰¥ p çš„æœ€å° token é›†åˆ

```python
def apply_top_p_filtering(logits: torch.Tensor, top_p: float) -> torch.Tensor:
    if top_p >= 1.0:
        return logits
    
    # 1. æŒ‰æ¦‚ç‡æ’åº
    sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
    
    # 2. è®¡ç®—ç´¯ç§¯æ¦‚ç‡
    sorted_probs = F.softmax(sorted_logits, dim=-1)
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
    
    # 3. æ‰¾å‡ºéœ€è¦ç§»é™¤çš„ token
    sorted_indices_to_remove = cumulative_probs > top_p
    
    # 4. å³ç§»ä¸€ä½ï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªè¶…è¿‡ p çš„ tokenï¼‰
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = False  # è‡³å°‘ä¿ç•™ä¸€ä¸ª
    
    # 5. æ˜ å°„å›åŸå§‹é¡ºåº
    indices_to_remove = sorted_indices_to_remove.scatter(
        -1, sorted_indices, sorted_indices_to_remove
    )
    
    # 6. è®¾ä¸º -inf
    filtered_logits = logits.masked_fill(indices_to_remove, float('-inf'))
    
    return filtered_logits
```

**ç¤ºä¾‹**ï¼š
```python
probs = [0.5, 0.3, 0.1, 0.05, 0.05]
top_p = 0.8

sorted_probs = [0.5, 0.3, 0.1, 0.05, 0.05]
cumsum       = [0.5, 0.8, 0.9, 0.95, 1.0]
                     â†‘
                 åˆ°è¿™é‡Œç´¯ç§¯æ¦‚ç‡ >= 0.8

ä¿ç•™: [0.5, 0.3, 0.1]ï¼ˆ3 ä¸ªtokenï¼‰
```

### 4.4 `apply_min_p_filtering()` - æœ€å°æ¦‚ç‡è¿‡æ»¤

**ç›®çš„**ï¼šè¿‡æ»¤æ‰æ¦‚ç‡ < min_p * max_prob çš„ token

```python
def apply_min_p_filtering(logits: torch.Tensor, min_p: float) -> torch.Tensor:
    if min_p <= 0.0:
        return logits
    
    # è®¡ç®—æ¦‚ç‡
    probs = F.softmax(logits, dim=-1)
    
    # æ‰¾æœ€å¤§æ¦‚ç‡
    max_probs, _ = torch.max(probs, dim=-1, keepdim=True)
    
    # é˜ˆå€¼
    threshold = min_p * max_probs
    
    # è¿‡æ»¤
    mask = probs < threshold
    filtered_logits = logits.masked_fill(mask, float('-inf'))
    
    return filtered_logits
```

**ç¤ºä¾‹**ï¼š
```python
probs = [0.6, 0.2, 0.1, 0.05, 0.05]
min_p = 0.1

max_prob = 0.6
threshold = 0.1 * 0.6 = 0.06

ä¿ç•™: [0.6, 0.2, 0.1]
è¿‡æ»¤: [0.05, 0.05]ï¼ˆ< 0.06ï¼‰
```

### 4.5 `Sampler` - å®Œæ•´é‡‡æ ·å™¨

```python
class Sampler:
    def __init__(self):
        self._generator = None  # éšæœºæ•°ç”Ÿæˆå™¨
    
    def sample(self, logits, sampling_params):
        # 1. è®¾ç½®éšæœºç§å­
        if sampling_params.seed is not None:
            if self._generator is None:
                self._generator = torch.Generator(device=logits.device)
            self._generator.manual_seed(sampling_params.seed)
        
        # 2. Temperature scaling
        if sampling_params.temperature > 0:
            logits = logits / sampling_params.temperature
        
        # 3. ä¾æ¬¡åº”ç”¨è¿‡æ»¤
        if sampling_params.min_p > 0:
            logits = apply_min_p_filtering(logits, sampling_params.min_p)
        
        if sampling_params.top_k > 0:
            logits = apply_top_k_filtering(logits, sampling_params.top_k)
        
        if sampling_params.top_p < 1.0:
            logits = apply_top_p_filtering(logits, sampling_params.top_p)
        
        # 4. é‡‡æ ·
        if sampling_params.sampling_type == SamplingType.GREEDY:
            sampled_tokens = torch.argmax(logits, dim=-1)
        else:
            probs = F.softmax(logits, dim=-1)
            sampled_tokens = torch.multinomial(
                probs, num_samples=1, generator=self._generator
            ).squeeze(-1)
        
        # 5. å¯é€‰ï¼šè®¡ç®— log_probs
        log_probs = None
        if sampling_params.logprobs is not None:
            log_probs = F.log_softmax(logits, dim=-1)
            log_probs = log_probs.gather(-1, sampled_tokens.unsqueeze(-1)).squeeze(-1)
        
        return sampled_tokens, log_probs
```

### 4.6 `check_stop_conditions()` - åœæ­¢æ¡ä»¶æ£€æŸ¥

```python
def check_stop_conditions(
    self,
    token_ids: List[int],      # å·²ç”Ÿæˆçš„ token
    token_text: str,            # è§£ç åçš„æ–‡æœ¬
    sampling_params: SamplingParams,
    eos_token_id: Optional[int],
) -> Tuple[bool, Optional[str]]:
    # 1. æ£€æŸ¥ max_tokens
    if sampling_params.max_tokens is not None:
        if len(token_ids) >= sampling_params.max_tokens:
            return True, "length"
    
    # 2. æ£€æŸ¥ EOS token
    if not sampling_params.ignore_eos and eos_token_id is not None:
        if len(token_ids) > 0 and token_ids[-1] == eos_token_id:
            return True, "stop"
    
    # 3. æ£€æŸ¥ stop_token_ids
    if sampling_params.stop_token_ids:
        if len(token_ids) > 0 and token_ids[-1] in sampling_params.stop_token_ids:
            return True, "stop"
    
    # 4. æ£€æŸ¥ stop strings
    if sampling_params.stop:
        for stop_str in sampling_params.stop:
            if stop_str in token_text:
                return True, "stop"
    
    return False, None
```

---

## 5. Worker å’Œ Executor å®ç°

### 5.1 ç³»ç»Ÿæ¶æ„

```
LLMEngine
  â†“ è°ƒç”¨
GPUExecutor (æ¥å£å±‚)
  â†“ ç®¡ç†
GPUWorker (è®¾å¤‡ç®¡ç†)
  â†“ æ‰§è¡Œ
ModelRunner (æ¨¡å‹è¿è¡Œ)
  â†“ å‰å‘ä¼ æ’­
Model
```

### 5.2 `ModelRunner` - æ¨¡å‹è¿è¡Œå™¨

**èŒè´£**ï¼š
1. å‡†å¤‡æ¨¡å‹è¾“å…¥ï¼ˆinput_ids, positionsï¼‰
2. ç®¡ç† KV cache
3. æ‰§è¡Œæ¨¡å‹å‰å‘ä¼ æ’­
4. è¿”å› logits

#### åˆå§‹åŒ–

```python
class ModelRunner:
    def __init__(self, model, model_config, device):
        self.model = model
        self.model_config = model_config
        self.device = device
        
        # KV cachesï¼ˆè‡ªå®šä¹‰æ¨¡å‹ï¼‰
        self.kv_caches = None
        # past_key_valuesï¼ˆHF æ¨¡å‹ï¼‰
        self.past_key_values = None
        
        # å±‚æ•°
        if hasattr(model, 'model') and hasattr(model.model, 'layers'):
            self.num_layers = len(model.model.layers)
        else:
            self.num_layers = 0
```

#### å‡†å¤‡è¾“å…¥

```python
def prepare_inputs(self, token_ids, start_pos=0):
    batch_size, seq_len = token_ids.shape
    
    # åˆ›å»º position indices
    positions = torch.arange(
        start_pos,
        start_pos + seq_len,
        device=self.device,
        dtype=torch.long,
    )
    # Expand for batch
    positions = positions.unsqueeze(0).expand(batch_size, -1)
    
    return token_ids, positions
```

**ä¸ºä»€ä¹ˆéœ€è¦ start_pos**ï¼š
```
Prefill: start_pos=0
  tokens: [1, 2, 3, 4]
  positions: [0, 1, 2, 3]

Decode step 1: start_pos=4
  tokens: [5]
  positions: [4]

Decode step 2: start_pos=5
  tokens: [6]
  positions: [5]
```

#### æ‰§è¡Œæ¨¡å‹

```python
@torch.no_grad()
def execute_model(self, token_ids, start_pos=0):
    # å‡†å¤‡è¾“å…¥
    input_ids, positions = self.prepare_inputs(token_ids, start_pos)
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ HF æ¨¡å‹
    if hasattr(self.model, 'forward') and 'position_ids' in str(self.model.forward.__code__.co_varnames):
        # HuggingFace æ¨¡å‹
        outputs = self.model(
            input_ids=input_ids,
            position_ids=positions,
            past_key_values=self.past_key_values,  # HF cache
            use_cache=True,
            return_dict=True,
        )
        logits = outputs.logits
        self.past_key_values = outputs.past_key_values  # æ›´æ–° cache
    else:
        # è‡ªå®šä¹‰æ¨¡å‹ï¼ˆM1 å®é™…ä¸ç”¨ï¼Œé¢„ç•™ï¼‰
        if self.kv_caches is None:
            self.initialize_kv_caches(batch_size=input_ids.shape[0])
        
        hidden_states = self.model(
            input_ids=input_ids,
            positions=positions,
            kv_caches=self.kv_caches,
        )
        
        # æ›´æ–° caches
        for layer_idx, layer in enumerate(self.model.model.layers):
            if hasattr(layer.self_attn, 'attn') and hasattr(layer.self_attn.attn, 'kv_cache'):
                self.kv_caches[layer_idx] = layer.self_attn.attn.kv_cache
        
        logits = self.model.compute_logits(hidden_states)
    
    return logits
```

#### è·å–ä¸‹ä¸€ä¸ª token çš„ logits

```python
def get_next_token_logits(self, token_ids, start_pos=0):
    # æ‰§è¡Œæ¨¡å‹
    logits = self.execute_model(token_ids, start_pos)
    
    # è¿”å›æœ€åä¸€ä¸ªä½ç½®çš„ logits
    return logits[:, -1, :]  # [batch, vocab_size]
```

### 5.3 `GPUWorker` - GPU å·¥ä½œå™¨

**èŒè´£**ï¼šç®¡ç† GPU è®¾å¤‡å’Œæ¨¡å‹ï¼Œæä¾›æ‰§è¡Œæ¥å£

```python
class GPUWorker:
    def __init__(self, model_config, device=None):
        self.model_config = model_config
        
        # ç¡®å®šè®¾å¤‡
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = torch.device(device)
        
        # åŠ è½½æ¨¡å‹
        loader = ModelLoader(model_config)
        self.model = loader.load_model(device=str(self.device))
        
        # åˆ›å»º model runner
        self.model_runner = ModelRunner(self.model, model_config, self.device)
    
    def get_next_token_logits(self, token_ids, start_pos=0):
        # ç§»åŠ¨åˆ°è®¾å¤‡
        if token_ids.device != self.device:
            token_ids = token_ids.to(self.device)
        
        return self.model_runner.get_next_token_logits(token_ids, start_pos)
    
    def clear_kv_caches(self):
        self.model_runner.clear_kv_caches()
```

### 5.4 `GPUExecutor` - æ‰§è¡Œå™¨

**èŒè´£**ï¼šç»Ÿä¸€çš„æ‰§è¡Œæ¥å£ï¼Œä¸ºåˆ†å¸ƒå¼é¢„ç•™ç©ºé—´

```python
class GPUExecutor:
    def __init__(self, model_config, device=None):
        self.model_config = model_config
        self.device = device
        
        # M1: å• GPU worker
        self.worker = GPUWorker(model_config, device)
    
    def get_next_token_logits(self, token_ids, start_pos=0):
        return self.worker.get_next_token_logits(token_ids, start_pos)
    
    def clear_kv_caches(self):
        self.worker.clear_kv_caches()
```

**æœªæ¥æ‰©å±•ï¼ˆM6ï¼‰**ï¼š
```python
# å¤š GPU tensor parallelism
class GPUExecutor:
    def __init__(self, ...):
        self.workers = [GPUWorker(..., device=f"cuda:{i}") for i in range(num_gpus)]
    
    def get_next_token_logits(self, ...):
        # åˆ†å‘åˆ°å¤šä¸ª worker
        # All-reduce ç»“æœ
        ...
```

---

## 6. Engine å®ç°

### 6.1 `InputProcessor` - è¾“å…¥å¤„ç†å™¨

**èŒè´£**ï¼š
1. Tokenize prompt
2. åˆ›å»º Request å¯¹è±¡
3. è§£ç  token IDs ä¸ºæ–‡æœ¬

```python
class InputProcessor:
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
    
    def process_request(self, prompt, sampling_params, request_id=None):
        # ç”Ÿæˆ request ID
        if request_id is None:
            request_id = str(uuid.uuid4())
        
        # Tokenize
        prompt_token_ids = self.tokenizer.encode(prompt, add_special_tokens=True)
        
        # åˆ›å»º Request
        request = Request(
            request_id=request_id,
            prompt=prompt,
            prompt_token_ids=prompt_token_ids,
            sampling_params=sampling_params,
        )
        
        return request
    
    def decode_tokens(self, token_ids, skip_special_tokens=True):
        return self.tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)
```

### 6.2 `LLMEngine` - ä¸»å¼•æ“

**èŒè´£**ï¼š
1. åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
2. æä¾› `generate()` æ¥å£
3. æ‰§è¡Œå®Œæ•´çš„ç”Ÿæˆå¾ªç¯
4. æ„é€ è¾“å‡º

#### åˆå§‹åŒ–

```python
class LLMEngine:
    def __init__(self, model_config, device=None):
        self.model_config = model_config
        
        # åŠ è½½ tokenizer
        loader = ModelLoader(model_config)
        self.tokenizer = loader.load_tokenizer()
        
        # åˆ›å»º executor
        self.executor = GPUExecutor(model_config, device)
        
        # åˆ›å»º processor å’Œ sampler
        self.processor = InputProcessor(self.tokenizer)
        self.sampler = Sampler()
```

#### ç”Ÿæˆæ¥å£

```python
def generate(self, prompt, sampling_params=None, return_outputs=True):
    # å¤„ç†è¾“å…¥
    if isinstance(prompt, str):
        if sampling_params is None:
            sampling_params = SamplingParams()
        request = self.processor.process_request(prompt, sampling_params)
    else:
        request = prompt
        sampling_params = request.sampling_params
    
    # æ¸…ç©ºç¼“å­˜
    self.executor.clear_kv_caches()
    
    # ç”Ÿæˆ
    output = self._generate_single(request)
    
    return output
```

#### å•è¯·æ±‚ç”Ÿæˆ

```python
def _generate_single(self, request):
    sampling_params = request.sampling_params
    
    # è·å–ç¬¬ä¸€ä¸ªåºåˆ—ï¼ˆM1 åªæ”¯æŒ n=1ï¼‰
    sequences = request.get_seqs()
    seq = sequences[0]
    seq.status = SequenceStatus.RUNNING
    
    # Prefill é˜¶æ®µ
    prompt_token_ids = seq.data.prompt_token_ids
    prompt_len = len(prompt_token_ids)
    
    start_time = time.time()
    input_tokens = torch.tensor([prompt_token_ids], dtype=torch.long)
    logits = self.executor.get_next_token_logits(input_tokens, start_pos=0)
    
    # é‡‡æ ·ç¬¬ä¸€ä¸ª token
    next_tokens, _ = self.sampler.sample(logits, sampling_params)
    next_token_id = next_tokens[0].item()
    seq.add_token_id(next_token_id)
    
    first_token_time = time.time()
    ttft = first_token_time - start_time
    
    # Decode é˜¶æ®µ
    decode_times = []
    for step in range(1, sampling_params.max_tokens or 100):
        decode_start = time.time()
        
        # æ£€æŸ¥åœæ­¢æ¡ä»¶
        output_text = self.processor.decode_tokens(
            seq.data.output_token_ids,
            skip_special_tokens=sampling_params.skip_special_tokens,
        )
        should_stop, finish_reason = self.sampler.check_stop_conditions(
            seq.data.output_token_ids,
            output_text,
            sampling_params,
            eos_token_id=self.tokenizer.eos_token_id,
        )
        
        if should_stop:
            seq.status = SequenceStatus.FINISHED_STOPPED if finish_reason == "stop" \
                else SequenceStatus.FINISHED_LENGTH_CAPPED
            break
        
        # ç”Ÿæˆä¸‹ä¸€ä¸ª token
        input_tokens = torch.tensor([[next_token_id]], dtype=torch.long)
        logits = self.executor.get_next_token_logits(input_tokens, start_pos=prompt_len + step)
        
        # é‡‡æ ·
        next_tokens, _ = self.sampler.sample(logits, sampling_params)
        next_token_id = next_tokens[0].item()
        seq.add_token_id(next_token_id)
        
        decode_times.append(time.time() - decode_start)
    
    # æ„é€ è¾“å‡º
    output = self._build_output(request)
    
    # æ·»åŠ æ€§èƒ½æŒ‡æ ‡
    total_time = time.time() - start_time
    num_tokens = len(seq.data.output_token_ids)
    tpot = sum(decode_times) / len(decode_times) if decode_times else 0
    
    output.metrics = {
        "ttft": ttft,
        "tpot": tpot,
        "total_time": total_time,
        "throughput": num_tokens / total_time if total_time > 0 else 0,
    }
    
    return output
```

#### æ„é€ è¾“å‡º

```python
def _build_output(self, request):
    sequences = request.get_seqs()
    
    # æ„é€  completion outputs
    completion_outputs = []
    for idx, seq in enumerate(sequences):
        # è§£ç è¾“å‡º
        output_text = self.processor.decode_tokens(
            seq.data.output_token_ids,
            skip_special_tokens=request.sampling_params.skip_special_tokens,
        )
        
        # ç¡®å®š finish reason
        finish_reason = None
        if seq.status == SequenceStatus.FINISHED_STOPPED:
            finish_reason = "stop"
        elif seq.status == SequenceStatus.FINISHED_LENGTH_CAPPED:
            finish_reason = "length"
        
        completion_output = CompletionOutput(
            index=idx,
            text=output_text,
            token_ids=seq.data.output_token_ids.copy(),
            cumulative_logprob=None,
            logprobs=None,
            finish_reason=finish_reason,
        )
        completion_outputs.append(completion_output)
    
    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
    finished = all(seq.is_finished() for seq in sequences)
    
    output = RequestOutput(
        request_id=request.request_id,
        prompt=request.prompt,
        prompt_token_ids=request.prompt_token_ids,
        outputs=completion_outputs,
        finished=finished,
    )
    
    return output
```

---

## 7. å®Œæ•´æ¨ç†æµç¨‹ä¸²è®²

### 7.1 ç”¨æˆ·è°ƒç”¨

```python
from folovllm import LLMEngine, ModelConfig, SamplingParams

# 1. åˆ›å»ºé…ç½®
config = ModelConfig(
    model="Qwen/Qwen3-0.6B",
    dtype="float16",
    trust_remote_code=True,
)

# 2. åˆå§‹åŒ–å¼•æ“
engine = LLMEngine(config, device="cuda")

# 3. è®¾ç½®é‡‡æ ·å‚æ•°
params = SamplingParams(
    temperature=0.7,
    top_k=50,
    top_p=0.95,
    max_tokens=100,
)

# 4. ç”Ÿæˆ
output = engine.generate("Hello, how are you?", params)

# 5. è·å–ç»“æœ
print(output.outputs[0].text)
print(f"TTFT: {output.metrics['ttft']*1000:.2f} ms")
```

### 7.2 å†…éƒ¨æµç¨‹è¯¦è§£

#### Step 1: Engine åˆå§‹åŒ–

```
LLMEngine.__init__()
  â”œâ”€ ModelLoader.load_tokenizer()
  â”‚    â””â”€ AutoTokenizer.from_pretrained()
  â”‚
  â”œâ”€ GPUExecutor(model_config, device)
  â”‚    â””â”€ GPUWorker(model_config, device)
  â”‚         â”œâ”€ ModelLoader.load_model()
  â”‚         â”‚    â”œâ”€ AutoModelForCausalLM.from_pretrained()
  â”‚         â”‚    â””â”€ _wrap_model_for_folovllm()  # æ·»åŠ  compute_logits
  â”‚         â”‚
  â”‚         â””â”€ ModelRunner(model, config, device)
  â”‚
  â”œâ”€ InputProcessor(tokenizer)
  â””â”€ Sampler()
```

#### Step 2: å¤„ç†è¯·æ±‚

```
engine.generate("Hello, how are you?", params)
  â””â”€ processor.process_request()
       â”œâ”€ tokenizer.encode("Hello, how are you?")
       â”‚    â†’ [9906, 11, 1268, 527, 499, 30]
       â”‚
       â””â”€ Request(
            request_id="uuid...",
            prompt="Hello, how are you?",
            prompt_token_ids=[9906, 11, 1268, 527, 499, 30],
            sampling_params=params,
          )
             â””â”€ è‡ªåŠ¨åˆ›å»º Sequence
```

#### Step 3: Prefill é˜¶æ®µ

```
executor.get_next_token_logits(
    token_ids=[[9906, 11, 1268, 527, 499, 30]],
    start_pos=0
)
  â†“
GPUWorker.get_next_token_logits()
  â†“
ModelRunner.get_next_token_logits()
  â”œâ”€ prepare_inputs()
  â”‚    input_ids: [[9906, 11, 1268, 527, 499, 30]]
  â”‚    positions: [[0, 1, 2, 3, 4, 5]]
  â”‚
  â”œâ”€ execute_model()
  â”‚    â”œâ”€ model(
  â”‚    â”‚    input_ids=[[9906, 11, 1268, 527, 499, 30]],
  â”‚    â”‚    position_ids=[[0, 1, 2, 3, 4, 5]],
  â”‚    â”‚    past_key_values=None,
  â”‚    â”‚    use_cache=True,
  â”‚    â”‚  )
  â”‚    â”‚
  â”‚    â”‚  å†…éƒ¨æµç¨‹ï¼š
  â”‚    â”‚  â”œâ”€ Embedding: [1, 6] â†’ [1, 6, 896]
  â”‚    â”‚  â”‚
  â”‚    â”‚  â”œâ”€ Layer 0:
  â”‚    â”‚  â”‚  â”œâ”€ RMSNorm
  â”‚    â”‚  â”‚  â”œâ”€ Attention:
  â”‚    â”‚  â”‚  â”‚  â”œâ”€ QKV Projection: [1, 6, 896] â†’ [1, 6, 1024+128+128]
  â”‚    â”‚  â”‚  â”‚  â”œâ”€ Split Q,K,V: [1,16,6,64], [1,2,6,64], [1,2,6,64]
  â”‚    â”‚  â”‚  â”‚  â”œâ”€ RoPE: rotate Q, K
  â”‚    â”‚  â”‚  â”‚  â”œâ”€ Attention:
  â”‚    â”‚  â”‚  â”‚  â”‚  â”œâ”€ Q @ K^T: [1,16,6,64] @ [1,16,64,6] â†’ [1,16,6,6]
  â”‚    â”‚  â”‚  â”‚  â”‚  â”œâ”€ Causal Mask (ä¸Šä¸‰è§’)
  â”‚    â”‚  â”‚  â”‚  â”‚  â”œâ”€ Softmax
  â”‚    â”‚  â”‚  â”‚  â”‚  â””â”€ @ V: [1,16,6,6] @ [1,16,6,64] â†’ [1,16,6,64]
  â”‚    â”‚  â”‚  â”‚  â””â”€ O Projection: [1, 6, 1024] â†’ [1, 6, 896]
  â”‚    â”‚  â”‚  â”œâ”€ RMSNorm
  â”‚    â”‚  â”‚  â””â”€ MLP:
  â”‚    â”‚  â”‚     â”œâ”€ Gate+Up: [1, 6, 896] â†’ [1, 6, 9216]
  â”‚    â”‚  â”‚     â”œâ”€ SiLU + Mul
  â”‚    â”‚  â”‚     â””â”€ Down: [1, 6, 4608] â†’ [1, 6, 896]
  â”‚    â”‚  â”‚
  â”‚    â”‚  â”œâ”€ Layer 1-27: ...
  â”‚    â”‚  â”‚
  â”‚    â”‚  â”œâ”€ Final RMSNorm
  â”‚    â”‚  â””â”€ LM Head: [1, 6, 896] â†’ [1, 6, 151936]
  â”‚    â”‚
  â”‚    â”œâ”€ ä¿å­˜ past_key_values (28 layers Ã— 2 (K,V))
  â”‚    â””â”€ è¿”å› logits: [1, 6, 151936]
  â”‚
  â””â”€ è¿”å›æœ€åä¸€ä¸ªä½ç½®: logits[:, -1, :] â†’ [1, 151936]
```

#### Step 4: é‡‡æ ·ç¬¬ä¸€ä¸ª token

```
sampler.sample(logits=[1, 151936], params)
  â”œâ”€ Temperature scaling: logits / 0.7
  â”œâ”€ Top-k filtering: ä¿ç•™ top 50
  â”œâ”€ Top-p filtering: ä¿ç•™ç´¯ç§¯æ¦‚ç‡ 0.95
  â”œâ”€ Softmax: â†’ probs [1, 151936]
  â””â”€ Multinomial: é‡‡æ · â†’ token_id = 358

seq.add_token_id(358)
```

#### Step 5: Decode å¾ªç¯

```
For step in [1, 2, ..., max_tokens]:
    # æ£€æŸ¥åœæ­¢æ¡ä»¶
    decode_tokens([358, ...])
    check_stop_conditions()
    
    # ç”Ÿæˆä¸‹ä¸€ä¸ª token
    executor.get_next_token_logits(
        token_ids=[[358]],  # åªæœ‰ä¸€ä¸ª token
        start_pos=6 + step  # ä» prompt_len å¼€å§‹
    )
      â†“
    ModelRunner.execute_model()
      â”œâ”€ prepare_inputs()
      â”‚    input_ids: [[358]]
      â”‚    positions: [[6]]  # ç¬¬ 7 ä¸ªä½ç½®
      â”‚
      â”œâ”€ model(
      â”‚    input_ids=[[358]],
      â”‚    position_ids=[[6]],
      â”‚    past_key_values=<28 layers KV>,  # ä½¿ç”¨ç¼“å­˜ï¼
      â”‚    use_cache=True,
      â”‚  )
      â”‚
      â”‚  å†…éƒ¨æµç¨‹ï¼ˆå¿«å¾ˆå¤šï¼‰ï¼š
      â”‚  â”œâ”€ Embedding: [1, 1] â†’ [1, 1, 896]
      â”‚  â”‚
      â”‚  â”œâ”€ Layer 0:
      â”‚  â”‚  â”œâ”€ Attention:
      â”‚  â”‚  â”‚  â”œâ”€ åªè®¡ç®—æ–° token çš„ Q,K,V
      â”‚  â”‚  â”‚  â”œâ”€ ä» past_key_values è·å–å†å² K,V: [1,2,6,64]
      â”‚  â”‚  â”‚  â”œâ”€ è¿½åŠ æ–° K,V: [1,2,6,64] + [1,2,1,64] â†’ [1,2,7,64]
      â”‚  â”‚  â”‚  â”œâ”€ Q @ K^T: [1,16,1,64] @ [1,16,64,7] â†’ [1,16,1,7]
      â”‚  â”‚  â”‚  â”œâ”€ ä¸éœ€è¦ maskï¼ˆdecode å¯ä»¥çœ‹æ‰€æœ‰å†å²ï¼‰
      â”‚  â”‚  â”‚  â”œâ”€ Softmax
      â”‚  â”‚  â”‚  â””â”€ @ V: [1,16,1,7] @ [1,16,7,64] â†’ [1,16,1,64]
      â”‚  â”‚  â””â”€ MLP
      â”‚  â”‚
      â”‚  â”œâ”€ Layer 1-27: ...
      â”‚  â””â”€ LM Head: [1, 1, 896] â†’ [1, 1, 151936]
      â”‚
      â””â”€ è¿”å› logits: [1, 151936]
    
    # é‡‡æ ·
    sampler.sample(logits, params) â†’ next_token
    seq.add_token_id(next_token)
```

#### Step 6: æ„é€ è¾“å‡º

```
_build_output(request)
  â”œâ”€ è§£ç  output_token_ids
  â”‚    decoder.decode([358, 286, 1436, ...])
  â”‚    â†’ " I'm doing well, thank you!"
  â”‚
  â”œâ”€ ç¡®å®š finish_reason
  â”‚    seq.status = FINISHED_STOPPED â†’ "stop"
  â”‚
  â”œâ”€ åˆ›å»º CompletionOutput
  â”‚    CompletionOutput(
  â”‚      index=0,
  â”‚      text=" I'm doing well, thank you!",
  â”‚      token_ids=[358, 286, 1436, ...],
  â”‚      finish_reason="stop",
  â”‚    )
  â”‚
  â””â”€ åˆ›å»º RequestOutput
       RequestOutput(
         request_id="uuid...",
         prompt="Hello, how are you?",
         outputs=[CompletionOutput(...)],
         finished=True,
         metrics={
           "ttft": 0.262,
           "tpot": 0.064,
           "total_time": 1.48,
           "throughput": 13.5,
         }
       )
```

### 7.3 æ•°æ®æµå›¾

```
User Input: "Hello, how are you?"
  â†“
Tokenizer: [9906, 11, 1268, 527, 499, 30]
  â†“
Embedding: [1, 6, 896]
  â†“
Transformer Layers (28 layers):
  Layer 0: [1, 6, 896] â†’ (Attn + MLP) â†’ [1, 6, 896]
  Layer 1: [1, 6, 896] â†’ (Attn + MLP) â†’ [1, 6, 896]
  ...
  Layer 27: [1, 6, 896] â†’ (Attn + MLP) â†’ [1, 6, 896]
  â†“
Final Norm: [1, 6, 896]
  â†“
LM Head: [1, 6, 151936]
  â†“
Last Token Logits: [1, 151936]
  â†“
Sampling (Temperature â†’ Top-k â†’ Top-p â†’ Multinomial)
  â†“
Next Token: 358
  â†“
Decode Loop (with KV Cache):
  Token 358 â†’ embedding â†’ Transformer â†’ logits â†’ sample â†’ Token 286
  Token 286 â†’ embedding â†’ Transformer â†’ logits â†’ sample â†’ Token 1436
  ...
  â†“
Output Tokens: [358, 286, 1436, ...]
  â†“
Detokenizer: " I'm doing well, thank you!"
  â†“
User Output
```

---

## 8. å…³é”®è®¾è®¡æ¨¡å¼æ€»ç»“

### 8.1 ç­–ç•¥æ¨¡å¼ (Strategy Pattern)

**Attention Backend**ï¼š
```python
# æŠ½è±¡ç­–ç•¥
class AttentionBackend(ABC):
    @abstractmethod
    def forward(self, ...): pass

# å…·ä½“ç­–ç•¥
class TorchNaiveBackend(AttentionBackend):
    def forward(self, ...): ...

class PagedAttentionBackend(AttentionBackend):
    def forward(self, ...): ...

# ä½¿ç”¨
attention = Attention(..., backend=TorchNaiveBackend())
```

### 8.2 æ¨¡æ¿æ–¹æ³•æ¨¡å¼ (Template Method)

**LLMEngine.generate**ï¼š
```python
def generate(self, prompt, params):
    request = self._process_input(prompt, params)
    self._clear_cache()
    output = self._generate_single(request)
    return output
```

### 8.3 åˆ†å±‚æ¶æ„ (Layered Architecture)

```
User API Layer:    LLMEngine
                      â†“
Business Logic:    InputProcessor, Sampler
                      â†“
Execution Layer:   GPUExecutor â†’ GPUWorker â†’ ModelRunner
                      â†“
Model Layer:       Transformer Model
                      â†“
Hardware:          GPU/CUDA
```

### 8.4 å…³é”®ä¼˜åŒ–æŠ€æœ¯

1. **KV Cache**: é¿å…é‡å¤è®¡ç®—
2. **Fused Operations**: RMSNorm + Residual, SiLU + Mul
3. **Merged Linear**: QKV projection, Gate+Up projection
4. **GQA**: å‡å°‘ KV cache å¤§å°
5. **RoPE**: é«˜æ•ˆä½ç½®ç¼–ç 

---

## 9. å¸¸è§é—®é¢˜è§£ç­”

### Q1: ä¸ºä»€ä¹ˆ Prefill å’Œ Decode è¦åˆ†å¼€å¤„ç†ï¼Ÿ

**A**: è®¡ç®—ç‰¹æ€§ä¸åŒ
- Prefill: å¤šä¸ª tokenï¼Œè®¡ç®—å¯†é›†å‹ï¼ˆGPU åˆ©ç”¨ç‡é«˜ï¼‰
- Decode: å•ä¸ª tokenï¼Œå†…å­˜å¸¦å®½å¯†é›†å‹ï¼ˆIO boundï¼‰
- åˆ†å¼€å¤„ç†å¯ä»¥é’ˆå¯¹æ€§ä¼˜åŒ–

### Q2: ä¸ºä»€ä¹ˆ M1 æ”¹ç”¨ HuggingFace æ¨¡å‹ï¼Ÿ

**A**: 
- è‡ªå®šä¹‰æ¨¡å‹ä¸ HF æ¶æ„ä¸å®Œå…¨åŒ¹é…
- HF æœ‰ q_norm, k_normï¼Œæˆ‘ä»¬æ²¡æœ‰
- æƒé‡æ˜ å°„å¤æ‚ï¼Œä¸ºç¨³å®šæ€§é€‰æ‹© HF

### Q3: past_key_values å’Œæˆ‘ä»¬çš„ kv_cache æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A**:
- HF çš„ `past_key_values`: List of tuplesï¼Œæ¯å±‚ä¸€ä¸ª tuple
- æˆ‘ä»¬çš„ `kv_cache`: åŒæ ·ç»“æ„ï¼Œä½†æˆ‘ä»¬æ‰‹åŠ¨ç®¡ç†
- M1 ç”¨ HF æ¨¡å‹ï¼Œæ‰€ä»¥ç”¨ `past_key_values`

### Q4: ä¸ºä»€ä¹ˆé‡‡æ ·é¡ºåºæ˜¯ temperature â†’ min_p â†’ top_k â†’ top_pï¼Ÿ

**A**:
- Temperature: è°ƒæ•´åˆ†å¸ƒé™¡å³­åº¦ï¼ˆå½±å“æ‰€æœ‰åç»­ï¼‰
- Min_p: å»é™¤é•¿å°¾ï¼ˆç²—è¿‡æ»¤ï¼‰
- Top_k: å›ºå®šæ•°é‡è¿‡æ»¤
- Top_p: åŠ¨æ€æ•°é‡è¿‡æ»¤ï¼ˆç²¾ç»†è°ƒæ•´ï¼‰

### Q5: å¦‚ä½•éªŒè¯ KV Cache æ­£ç¡®å·¥ä½œï¼Ÿ

**A**:
```python
# æ–¹æ³•1: å¯¹æ¯”è¾“å‡ºï¼ˆåº”è¯¥ä¸€è‡´ï¼‰
output_with_cache = engine.generate(...)
output_without_cache = hf_model.generate(..., use_cache=False)

# æ–¹æ³•2: æ£€æŸ¥é€Ÿåº¦ï¼ˆdecode åº”è¯¥å¿«ï¼‰
print(f"TTFT: {output.metrics['ttft']}")  # Prefill æ—¶é—´
print(f"TPOT: {output.metrics['tpot']}")  # Decode æ—¶é—´ï¼ˆåº”è¯¥ < TTFTï¼‰

# æ–¹æ³•3: æ£€æŸ¥ cache å¤§å°
print(f"Cache shape: {past_key_values[0][0].shape}")  # åº”è¯¥é€æ¸å¢å¤§
```

---

## 10. ä¸‹ä¸€æ­¥ï¼šM2 é¢„å‘Š

M1 å®ç°äº†å•è¯·æ±‚æ¨ç†ï¼ŒM2 å°†å®ç°ï¼š

1. **Scheduler**: è¯·æ±‚é˜Ÿåˆ—ç®¡ç†
2. **Continuous Batching**: åŠ¨æ€æ‰¹å¤„ç†
3. **å¼‚æ­¥æ¥å£**: add_request(), step()
4. **çŠ¶æ€ç®¡ç†**: WAITING â†’ RUNNING â†’ FINISHED

**æ ¸å¿ƒæ”¹åŠ¨**ï¼š
```python
# M1: åŒæ­¥å•è¯·æ±‚
output = engine.generate(prompt, params)

# M2: å¼‚æ­¥å¤šè¯·æ±‚
request_id = await engine.add_request(prompt, params)
while not finished:
    outputs = engine.step()  # å¤„ç†ä¸€æ‰¹è¯·æ±‚
    for output in outputs:
        if output.request_id == request_id:
            ...
```

---

**æ–‡æ¡£å®Œæˆï¼**

æœ¬æ–‡æ¡£è¯¦ç»†è®²è§£äº† M1 çš„æ¯ä¸ªç±»å’Œå‡½æ•°ï¼Œé€‚åˆå‘å°ç™½è®²è§£æ•´ä¸ªæ¨ç†æµç¨‹ã€‚

