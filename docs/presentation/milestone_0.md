# Milestone 0 å£è¿°å±•ç¤ºæ–‡æ¡£

> æœ¬æ–‡æ¡£ä»¥ç±»/å‡½æ•°ä¸ºå•ä½ï¼Œè¯¦ç»†è®²è§£ M0 çš„å®ç°è¿‡ç¨‹ï¼Œé€‚åˆå‘å°ç™½è®²è§£

---

## ğŸ“‹ æ–‡æ¡£ç»“æ„

æœ¬æ–‡æ¡£æŒ‰ç…§å¼€å‘é¡ºåºè®²è§£ï¼š

1. [é¡¹ç›®åˆå§‹åŒ–ä¸ç¯å¢ƒè„šæœ¬](#1-é¡¹ç›®åˆå§‹åŒ–ä¸ç¯å¢ƒè„šæœ¬)
2. [é…ç½®ç³»ç»Ÿå®ç°](#2-é…ç½®ç³»ç»Ÿå®ç°)
3. [é‡‡æ ·å‚æ•°å®ç°](#3-é‡‡æ ·å‚æ•°å®ç°)
4. [è¯·æ±‚å’Œåºåˆ—å®ç°](#4-è¯·æ±‚å’Œåºåˆ—å®ç°)
5. [è¾“å‡ºæ ¼å¼å®ç°](#5-è¾“å‡ºæ ¼å¼å®ç°)
6. [æ¨¡å‹åŠ è½½å™¨å®ç°](#6-æ¨¡å‹åŠ è½½å™¨å®ç°)
7. [å·¥å…·å‡½æ•°å®ç°](#7-å·¥å…·å‡½æ•°å®ç°)
8. [åŒ…å¯¼å‡ºå’Œæµ‹è¯•](#8-åŒ…å¯¼å‡ºå’Œæµ‹è¯•)

---

## 1. é¡¹ç›®åˆå§‹åŒ–ä¸ç¯å¢ƒè„šæœ¬

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦ç¯å¢ƒè„šæœ¬ï¼Ÿ

**åœºæ™¯æ€è€ƒ**ï¼š
```
å¼€å‘è€… A: "æ€ä¹ˆå®‰è£…ä¾èµ–ï¼Ÿ"
å¼€å‘è€… B: "Python ç‰ˆæœ¬ä¸å¯¹ï¼Œæ€ä¹ˆåŠï¼Ÿ"
å¼€å‘è€… C: "CUDA ä¸å¯ç”¨ï¼Œå¦‚ä½•æ£€æŸ¥ï¼Ÿ"
```

**è§£å†³æ–¹æ¡ˆ**ï¼šåˆ›å»ºè‡ªåŠ¨åŒ–ç¯å¢ƒè®¾ç½®è„šæœ¬

### 1.2 setup_env.sh å®ç°ï¼ˆLinux/macOSï¼‰

#### ç¬¬ä¸€æ­¥ï¼šå®šä¹‰è¾…åŠ©å‡½æ•°

**ä¸ºä»€ä¹ˆ**ï¼šè®©è„šæœ¬è¾“å‡ºæ›´å‹å¥½ï¼Œä¾¿äºç”¨æˆ·ç†è§£è¿›åº¦

```bash
# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'  # No Color

# æ‰“å°è¾…åŠ©å‡½æ•°
print_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}
```

**è®²è§£**ï¼š
- ä½¿ç”¨ ANSI è½¬ä¹‰ç å®ç°å½©è‰²è¾“å‡º
- ç»Ÿä¸€çš„æ¶ˆæ¯æ ¼å¼ï¼Œæ˜“äºé˜…è¯»
- ä¸åŒçº§åˆ«ç”¨ä¸åŒé¢œè‰²ï¼ˆä¿¡æ¯=è“ï¼ŒæˆåŠŸ=ç»¿ï¼Œé”™è¯¯=çº¢ï¼‰

#### ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥ Python ç‰ˆæœ¬

**ä¸ºä»€ä¹ˆ**ï¼šé¡¹ç›®éœ€è¦ Python 3.10+ï¼Œéœ€è¦ç¡®ä¿ç¯å¢ƒç¬¦åˆè¦æ±‚

```bash
check_python_version() {
    print_header "æ£€æŸ¥ Python ç‰ˆæœ¬"
    
    # å°è¯•ä¸åŒçš„ Python å‘½ä»¤
    for cmd in python3.10 python3.11 python3.12 python3 python; do
        if command -v $cmd &> /dev/null; then
            PYTHON_CMD=$cmd
            PYTHON_VER=$($cmd --version 2>&1 | awk '{print $2}')
            print_info "æ‰¾åˆ° Python: $cmd (ç‰ˆæœ¬ $PYTHON_VER)"
            
            # æ£€æŸ¥ç‰ˆæœ¬æ˜¯å¦ >= 3.10
            MAJOR=$(echo $PYTHON_VER | cut -d. -f1)
            MINOR=$(echo $PYTHON_VER | cut -d. -f2)
            
            if [ "$MAJOR" -ge 3 ] && [ "$MINOR" -ge 10 ]; then
                print_success "Python ç‰ˆæœ¬ç¬¦åˆè¦æ±‚ (>= 3.10)"
                return 0
            fi
        fi
    done
    
    print_error "æœªæ‰¾åˆ° Python 3.10 æˆ–æ›´é«˜ç‰ˆæœ¬"
    exit 1
}
```

**è®²è§£**ï¼š
1. **å¾ªç¯å°è¯•**ï¼šä¸åŒç³»ç»Ÿ Python å‘½ä»¤ä¸åŒï¼ˆpython3.10ã€python3ã€pythonï¼‰
2. **ç‰ˆæœ¬è§£æ**ï¼šç”¨ `awk` å’Œ `cut` è§£æç‰ˆæœ¬å·
3. **æ•°å€¼æ¯”è¾ƒ**ï¼šæ£€æŸ¥ä¸»ç‰ˆæœ¬å· >= 3 ä¸”æ¬¡ç‰ˆæœ¬å· >= 10
4. **å‹å¥½æç¤º**ï¼šæ‰¾ä¸åˆ°æ—¶ç»™å‡ºæ˜ç¡®çš„é”™è¯¯ä¿¡æ¯

#### ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

**ä¸ºä»€ä¹ˆ**ï¼šéš”ç¦»é¡¹ç›®ä¾èµ–ï¼Œé¿å…æ±¡æŸ“ç³»ç»Ÿ Python

```bash
create_venv() {
    print_header "åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ"
    
    if [ -d "$VENV_DIR" ]; then
        print_warning "è™šæ‹Ÿç¯å¢ƒå·²å­˜åœ¨: $VENV_DIR"
        read -p "æ˜¯å¦åˆ é™¤å¹¶é‡æ–°åˆ›å»ºï¼Ÿ(y/N) " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            print_info "åˆ é™¤æ—§çš„è™šæ‹Ÿç¯å¢ƒ..."
            rm -rf $VENV_DIR
        else
            print_info "è·³è¿‡è™šæ‹Ÿç¯å¢ƒåˆ›å»º"
            return 0
        fi
    fi
    
    print_info "åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ: $VENV_DIR"
    $PYTHON_CMD -m venv $VENV_DIR
    print_success "è™šæ‹Ÿç¯å¢ƒåˆ›å»ºæˆåŠŸ"
}
```

**è®²è§£**ï¼š
1. **æ£€æŸ¥å­˜åœ¨**ï¼šé¿å…è¦†ç›–å·²æœ‰ç¯å¢ƒ
2. **ç”¨æˆ·ç¡®è®¤**ï¼šè®©ç”¨æˆ·å†³å®šæ˜¯å¦é‡å»º
3. **ä½¿ç”¨æ‰¾åˆ°çš„ Python**ï¼šç”¨ `$PYTHON_CMD` ç¡®ä¿ç‰ˆæœ¬æ­£ç¡®

#### ç¬¬å››æ­¥ï¼šå®‰è£…ä¾èµ–

**ä¸ºä»€ä¹ˆ**ï¼šè‡ªåŠ¨åŒ–å®‰è£…è¿‡ç¨‹ï¼Œå¤„ç†å¯é€‰ä¾èµ–

```bash
install_dependencies() {
    print_header "å®‰è£…ä¾èµ–"
    
    print_info "å®‰è£…åŸºç¡€ä¾èµ–..."
    pip install -r requirements.txt -q
    print_success "åŸºç¡€ä¾èµ–å®‰è£…æˆåŠŸ"
    
    # è¯¢é—®æ˜¯å¦å®‰è£…å¯é€‰ä¾èµ–
    echo ""
    print_info "å¯é€‰ä¾èµ–ï¼š"
    echo "  1) Flash Attention 2 (éœ€è¦ CUDAï¼Œç¼–è¯‘æ—¶é—´è¾ƒé•¿)"
    echo "  2) AutoGPTQ (ç”¨äº M7 é‡åŒ–æ”¯æŒ)"
    echo "  3) è·³è¿‡å¯é€‰ä¾èµ–"
    read -p "è¯·é€‰æ‹© (1/2/3, é»˜è®¤=3): " -n 1 -r
    echo
    
    case $REPLY in
        1)
            print_info "å®‰è£… Flash Attention 2..."
            print_warning "è¿™å¯èƒ½éœ€è¦ 10-20 åˆ†é’Ÿ..."
            pip install flash-attn --no-build-isolation
            print_success "Flash Attention 2 å®‰è£…æˆåŠŸ"
            ;;
        2)
            print_info "å®‰è£… AutoGPTQ..."
            pip install auto-gptq optimum
            print_success "AutoGPTQ å®‰è£…æˆåŠŸ"
            ;;
        *)
            print_info "è·³è¿‡å¯é€‰ä¾èµ–"
            ;;
    esac
}
```

**è®²è§£**ï¼š
1. **é™é»˜å®‰è£…**ï¼š`-q` å‚æ•°å‡å°‘è¾“å‡ºï¼Œé¿å…ä¿¡æ¯è¿‡è½½
2. **äº¤äº’é€‰æ‹©**ï¼šè®©ç”¨æˆ·å†³å®šå®‰è£…å“ªäº›å¯é€‰ä¾èµ–
3. **æ¸…æ™°æç¤º**ï¼šè¯´æ˜å®‰è£…æ—¶é—´å’Œç”¨é€”

#### ç¬¬äº”æ­¥ï¼šéªŒè¯å®‰è£…

**ä¸ºä»€ä¹ˆ**ï¼šç¡®ä¿æ‰€æœ‰ç»„ä»¶æ­£ç¡®å®‰è£…ï¼Œæå‰å‘ç°é—®é¢˜

```bash
verify_installation() {
    print_header "éªŒè¯å®‰è£…"
    
    print_info "æ£€æŸ¥é¡¹ç›®å¯¼å…¥..."
    python -c "import folovllm; print('âœ“ folovllm å¯¼å…¥æˆåŠŸ')"
    python -c "import torch; print(f'âœ“ PyTorch {torch.__version__}')"
    python -c "import transformers; print(f'âœ“ Transformers {transformers.__version__}')"
    
    # æ£€æŸ¥ CUDA
    echo ""
    print_info "æ£€æŸ¥ CUDA å¯ç”¨æ€§..."
    python -c "import torch; print('âœ“ CUDA å¯ç”¨' if torch.cuda.is_available() else 'âœ— CUDA ä¸å¯ç”¨')"
    
    if python -c "import torch; exit(0 if torch.cuda.is_available() else 1)"; then
        python -c "import torch; print(f'  GPU: {torch.cuda.get_device_name(0)}')"
        python -c "import torch; print(f'  æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')"
    fi
    
    print_success "å®‰è£…éªŒè¯é€šè¿‡"
}
```

**è®²è§£**ï¼š
1. **å¯¼å…¥æµ‹è¯•**ï¼šç”¨ `python -c` å¿«é€Ÿæµ‹è¯•å¯¼å…¥
2. **CUDA æ£€æŸ¥**ï¼šæ£€æµ‹ GPU å¯ç”¨æ€§å’Œæ˜¾å­˜
3. **æ¡ä»¶æ‰§è¡Œ**ï¼šåªåœ¨ CUDA å¯ç”¨æ—¶æ˜¾ç¤º GPU ä¿¡æ¯

### 1.3 activate.sh å®ç°

**ä¸ºä»€ä¹ˆ**ï¼šæä¾›å¿«æ·çš„è™šæ‹Ÿç¯å¢ƒæ¿€æ´»æ–¹å¼

```bash
#!/bin/bash
# å¿«é€Ÿæ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate
echo "âœ“ FoloVLLM è™šæ‹Ÿç¯å¢ƒå·²æ¿€æ´»"
echo "Python: $(python --version)"
echo "ä½ç½®: $(which python)"
```

**è®²è§£**ï¼š
- ç®€å•çš„æ¿€æ´»è„šæœ¬ï¼Œé¿å…è¾“å…¥é•¿å‘½ä»¤
- æ˜¾ç¤º Python ç‰ˆæœ¬å’Œè·¯å¾„ï¼Œä¾¿äºç¡®è®¤

### 1.4 .gitignore å®ç°

**ä¸ºä»€ä¹ˆ**ï¼šé˜²æ­¢ä¸å¿…è¦çš„æ–‡ä»¶è¿›å…¥ç‰ˆæœ¬æ§åˆ¶

```gitignore
# Python
__pycache__/
*.py[cod]
venv/
*.egg-info/

# IDE
.vscode/
.cursor/
.idea/

# æ¨¡å‹æ–‡ä»¶
*.bin
*.safetensors
*.pth

# æ—¥å¿—å’Œä¸´æ—¶æ–‡ä»¶
*.log
logs/
tmp/
```

**è®²è§£**ï¼š
1. **Python ç›¸å…³**ï¼šå¿½ç•¥ç¼–è¯‘æ–‡ä»¶ã€è™šæ‹Ÿç¯å¢ƒ
2. **IDE é…ç½®**ï¼šä¸åŒå¼€å‘è€…å¯èƒ½ç”¨ä¸åŒ IDE
3. **å¤§æ–‡ä»¶**ï¼šæ¨¡å‹æ–‡ä»¶ä¸åº”è¿›å…¥ git
4. **ä¸´æ—¶æ–‡ä»¶**ï¼šæ—¥å¿—å’Œç¼“å­˜æ–‡ä»¶

---

## 2. é…ç½®ç³»ç»Ÿå®ç°

### 2.1 æ•´ä½“è®¾è®¡æ€è·¯

**ç›®æ ‡**ï¼šåˆ›å»ºä¸€ä¸ªåˆ†å±‚ã€ç±»å‹å®‰å…¨ã€æ˜“æ‰©å±•çš„é…ç½®ç³»ç»Ÿ

**å±‚æ¬¡ç»“æ„**ï¼š
```
EngineConfig (é¡¶å±‚)
    â”œâ”€â”€ ModelConfig (æ¨¡å‹é…ç½®)
    â”œâ”€â”€ CacheConfig (ç¼“å­˜é…ç½®)
    â””â”€â”€ SchedulerConfig (è°ƒåº¦é…ç½®)
```

### 2.2 ModelConfig å®ç°

#### ç¬¬ä¸€æ­¥ï¼šå®šä¹‰åŸºæœ¬ç»“æ„

**æ€è€ƒ**ï¼šæ¨¡å‹é…ç½®éœ€è¦å“ªäº›ä¿¡æ¯ï¼Ÿ

```python
from dataclasses import dataclass
from typing import Literal, Optional

ModelDType = Literal["auto", "half", "float16", "bfloat16", "float", "float32"]

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    model: str  # å¿…éœ€ï¼šæ¨¡å‹è·¯å¾„
    tokenizer: Optional[str] = None  # å¯é€‰ï¼štokenizer è·¯å¾„
    tokenizer_mode: str = "auto"  # é»˜è®¤ï¼šfast tokenizer
    trust_remote_code: bool = False  # é»˜è®¤ï¼šä¸ä¿¡ä»»è¿œç¨‹ä»£ç 
    dtype: ModelDType = "auto"  # é»˜è®¤ï¼šè‡ªåŠ¨é€‰æ‹©
    max_model_len: Optional[int] = None  # å¯é€‰ï¼šä»æ¨¡å‹æ¨æ–­
    seed: int = 0  # é»˜è®¤ï¼šå›ºå®šç§å­
```

**è®²è§£**ï¼š
- **å¿…éœ€å‚æ•°**ï¼š`model` æ˜¯å”¯ä¸€å¿…éœ€çš„
- **åˆç†é»˜è®¤å€¼**ï¼šå¤§éƒ¨åˆ†å‚æ•°æœ‰åˆç†çš„é»˜è®¤å€¼
- **ç±»å‹çº¦æŸ**ï¼š`Literal` é™åˆ¶ dtype çš„å–å€¼
- **å¯é€‰å‚æ•°**ï¼šç”¨ `Optional` æ ‡è®°

#### ç¬¬äºŒæ­¥ï¼šæ·»åŠ åå¤„ç†é€»è¾‘

**æ€è€ƒ**ï¼šåˆ›å»ºåéœ€è¦åšä»€ä¹ˆï¼Ÿ

```python
@dataclass
class ModelConfig:
    # ... å­—æ®µå®šä¹‰ ...
    
    def __post_init__(self):
        """åˆ›å»ºåçš„åˆå§‹åŒ–é€»è¾‘"""
        # 1. tokenizer é»˜è®¤ä½¿ç”¨ model è·¯å¾„
        if self.tokenizer is None:
            self.tokenizer = self.model
        
        # 2. å°†å­—ç¬¦ä¸² dtype è½¬æ¢ä¸º torch.dtype
        if isinstance(self.dtype, str):
            dtype_map = {
                "auto": None,
                "half": torch.float16,
                "float16": torch.float16,
                "bfloat16": torch.bfloat16,
                "float": torch.float32,
                "float32": torch.float32,
            }
            self.torch_dtype = dtype_map.get(self.dtype)
        else:
            self.torch_dtype = self.dtype
```

**è®²è§£**ï¼š
1. **è‡ªåŠ¨è¡¥å…¨**ï¼štokenizer æœªæŒ‡å®šæ—¶ä½¿ç”¨ model è·¯å¾„
2. **ç±»å‹è½¬æ¢**ï¼šå­—ç¬¦ä¸² â†’ torch.dtypeï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨
3. **ä¿å­˜ç»“æœ**ï¼šè½¬æ¢ç»“æœä¿å­˜åˆ° `torch_dtype`

**ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡**ï¼š
- ç”¨æˆ·å¯ä»¥å†™ `dtype="float16"`ï¼ˆæ˜“è¯»ï¼‰
- å†…éƒ¨ä½¿ç”¨ `torch.float16`ï¼ˆç±»å‹æ­£ç¡®ï¼‰

### 2.3 CacheConfig å®ç°

#### ç¬¬ä¸€æ­¥ï¼šå®šä¹‰é…ç½®é¡¹

**æ€è€ƒ**ï¼šKV Cache éœ€è¦å“ªäº›é…ç½®ï¼Ÿ

```python
@dataclass
class CacheConfig:
    """KV Cache é…ç½®"""
    block_size: int = 16  # æ¯ä¸ª block çš„ token æ•°
    gpu_memory_utilization: float = 0.9  # GPU æ˜¾å­˜åˆ©ç”¨ç‡
    swap_space: float = 4.0  # CPU swap ç©ºé—´ (GiB)
    enable_prefix_caching: bool = False  # å‰ç¼€ç¼“å­˜ï¼ˆM6ï¼‰
```

**è®²è§£**ï¼š
- **block_size**ï¼šPagedAttention çš„åŸºæœ¬å•ä½ï¼ˆM3 ä½¿ç”¨ï¼‰
- **gpu_memory_utilization**ï¼šæ§åˆ¶æ˜¾å­˜ä½¿ç”¨ï¼Œé¿å… OOM
- **swap_space**ï¼šæ˜¾å­˜ä¸è¶³æ—¶äº¤æ¢åˆ° CPU
- **enable_prefix_caching**ï¼šé¢„ç•™ç»™ M6

#### ç¬¬äºŒæ­¥ï¼šæ·»åŠ éªŒè¯é€»è¾‘

**æ€è€ƒ**ï¼šå“ªäº›é…ç½®å€¼æ˜¯ä¸åˆæ³•çš„ï¼Ÿ

```python
@dataclass
class CacheConfig:
    # ... å­—æ®µå®šä¹‰ ...
    
    def __post_init__(self):
        """éªŒè¯é…ç½®åˆæ³•æ€§"""
        # éªŒè¯ block_size
        if self.block_size <= 0:
            raise ValueError(
                f"block_size must be positive, got {self.block_size}"
            )
        
        # éªŒè¯ gpu_memory_utilization
        if not 0 < self.gpu_memory_utilization <= 1:
            raise ValueError(
                f"gpu_memory_utilization must be in (0, 1], "
                f"got {self.gpu_memory_utilization}"
            )
```

**è®²è§£**ï¼š
1. **æå‰éªŒè¯**ï¼šåœ¨åˆ›å»ºæ—¶å°±å‘ç°é”™è¯¯ï¼Œè€Œä¸æ˜¯è¿è¡Œæ—¶
2. **æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯**ï¼šå‘Šè¯‰ç”¨æˆ·å“ªé‡Œé”™äº†
3. **èŒƒå›´æ£€æŸ¥**ï¼šç¡®ä¿å‚æ•°åœ¨åˆç†èŒƒå›´å†…

**ä¸ºä»€ä¹ˆè¿™æ ·åš**ï¼š
- æ—©å‘ç°æ—©ä¿®å¤
- é¿å…è¿è¡Œåˆ°ä¸€åŠæ‰å‡ºé”™
- é”™è¯¯ä¿¡æ¯æ¸…æ™°ï¼Œæ˜“äºè°ƒè¯•

### 2.4 SchedulerConfig å®ç°

#### é¢„ç•™è®¾è®¡

**æ€è€ƒ**ï¼šè°ƒåº¦å™¨éœ€è¦å“ªäº›é…ç½®ï¼Ÿ

```python
@dataclass
class SchedulerConfig:
    """è°ƒåº¦å™¨é…ç½®"""
    max_num_batched_tokens: Optional[int] = None  # æœ€å¤§æ‰¹å¤„ç† token æ•°
    max_num_seqs: int = 256  # æœ€å¤§åºåˆ—æ•°
    max_model_len: Optional[int] = None  # æœ€å¤§åºåˆ—é•¿åº¦
    enable_chunked_prefill: bool = False  # åˆ†å—é¢„å¡«å……ï¼ˆM5ï¼‰
    
    def __post_init__(self):
        """åå¤„ç†"""
        # é¢„ç•™ï¼šåç»­ä¼šä» engine config åŒæ­¥ max_model_len
        pass
```

**è®²è§£**ï¼š
- **é¢„ç•™æ¥å£**ï¼šè™½ç„¶ M0 ä¸ç”¨ï¼Œä½†é¢„ç•™ç»™ M2
- **å¯é€‰å‚æ•°**ï¼šå¤§éƒ¨åˆ†éƒ½æ˜¯å¯é€‰çš„
- **ç©ºçš„åå¤„ç†**ï¼šæ˜ç¡®è¡¨ç¤ºæœ‰è¿™ä¸ªé˜¶æ®µï¼Œä½†æš‚æ— é€»è¾‘

### 2.5 EngineConfig å®ç°

#### ç»„è£…é…ç½®

**æ€è€ƒ**ï¼šå¦‚ä½•ç»„åˆä¸‰ä¸ªé…ç½®ç±»ï¼Ÿ

```python
from dataclasses import dataclass, field

@dataclass
class EngineConfig:
    """å¼•æ“ç»Ÿä¸€é…ç½®"""
    model_config: ModelConfig  # å¿…éœ€
    cache_config: CacheConfig = field(default_factory=CacheConfig)  # å¯é€‰ï¼Œæœ‰é»˜è®¤å€¼
    scheduler_config: SchedulerConfig = field(default_factory=SchedulerConfig)  # å¯é€‰
    
    def __post_init__(self):
        """é…ç½®åŒæ­¥"""
        # å°† max_model_len åŒæ­¥åˆ° scheduler_config
        if self.scheduler_config.max_model_len is None:
            self.scheduler_config.max_model_len = self.model_config.max_model_len
```

**è®²è§£**ï¼š
1. **å¿…éœ€çš„ model_config**ï¼šå¿…é¡»æ˜ç¡®æŒ‡å®šæ¨¡å‹
2. **å¯é€‰çš„å­é…ç½®**ï¼šç”¨ `field(default_factory=...)` æä¾›é»˜è®¤å€¼
3. **é…ç½®åŒæ­¥**ï¼šé¿å…é…ç½®ä¸ä¸€è‡´

**ä¸ºä»€ä¹ˆç”¨ default_factory**ï¼š
```python
# âŒ é”™è¯¯ï¼šæ‰€æœ‰å®ä¾‹å…±äº«åŒä¸€ä¸ªå¯¹è±¡
cache_config: CacheConfig = CacheConfig()

# âœ… æ­£ç¡®ï¼šæ¯ä¸ªå®ä¾‹æœ‰è‡ªå·±çš„å¯¹è±¡
cache_config: CacheConfig = field(default_factory=CacheConfig)
```

---

## 3. é‡‡æ ·å‚æ•°å®ç°

### 3.1 è®¾è®¡æ€è·¯

**ç›®æ ‡**ï¼šæ”¯æŒå¤šç§é‡‡æ ·ç­–ç•¥ï¼Œå‚æ•°éªŒè¯ï¼Œæ˜“äºæ‰©å±•

### 3.2 SamplingParams å®ç°

#### ç¬¬ä¸€æ­¥ï¼šå®šä¹‰é‡‡æ ·ç±»å‹

**æ€è€ƒ**ï¼šæœ‰å“ªäº›é‡‡æ ·ç±»å‹ï¼Ÿ

```python
from enum import IntEnum

class SamplingType(IntEnum):
    """é‡‡æ ·ç±»å‹"""
    GREEDY = 0  # è´ªå¿ƒï¼šæ€»æ˜¯é€‰æœ€å¤§æ¦‚ç‡
    RANDOM = 1  # éšæœºï¼šæŒ‰æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·
```

**è®²è§£**ï¼š
- **IntEnum**ï¼šæ—¢æœ‰åå­—åˆæœ‰æ•´æ•°å€¼
- **ä¸¤ç§åŸºæœ¬ç±»å‹**ï¼šç¡®å®šæ€§ï¼ˆè´ªå¿ƒï¼‰å’Œéšæœºæ€§

#### ç¬¬äºŒæ­¥ï¼šå®šä¹‰å‚æ•°ç»“æ„

**æ€è€ƒ**ï¼šé‡‡æ ·éœ€è¦å“ªäº›å‚æ•°ï¼Ÿ

```python
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class SamplingParams:
    """é‡‡æ ·å‚æ•°"""
    
    # è¾“å‡ºæ•°é‡
    n: int = 1  # è¿”å›å‡ ä¸ªåºåˆ—
    best_of: Optional[int] = None  # ç”Ÿæˆå‡ ä¸ªå€™é€‰
    
    # é‡‡æ ·ç­–ç•¥
    temperature: float = 1.0  # æ¸©åº¦
    top_p: float = 1.0  # Nucleus sampling
    top_k: int = -1  # Top-k sampling (-1 è¡¨ç¤ºç¦ç”¨)
    min_p: float = 0.0  # æœ€å°æ¦‚ç‡é˜ˆå€¼
    
    # åœæ­¢æ¡ä»¶
    stop: Optional[List[str]] = None  # åœæ­¢å­—ç¬¦ä¸²
    stop_token_ids: Optional[List[int]] = None  # åœæ­¢ token ID
    max_tokens: Optional[int] = 16  # æœ€å¤§ç”Ÿæˆé•¿åº¦
    min_tokens: int = 0  # æœ€å°ç”Ÿæˆé•¿åº¦
    
    # å…¶ä»–
    seed: Optional[int] = None  # éšæœºç§å­
    skip_special_tokens: bool = True  # è·³è¿‡ç‰¹æ®Š token
```

**è®²è§£**ï¼š
- **åˆ†ç»„**ï¼šæŒ‰åŠŸèƒ½åˆ†ç»„ï¼ˆè¾“å‡ºã€é‡‡æ ·ã€åœæ­¢ã€å…¶ä»–ï¼‰
- **é»˜è®¤å€¼**ï¼šæ¯ä¸ªå‚æ•°éƒ½æœ‰åˆç†çš„é»˜è®¤å€¼
- **å¯é€‰å‚æ•°**ï¼šç”¨ `Optional` æ ‡è®°

#### ç¬¬ä¸‰æ­¥ï¼šå‚æ•°éªŒè¯

**æ€è€ƒ**ï¼šå“ªäº›å‚æ•°ç»„åˆæ˜¯ä¸åˆæ³•çš„ï¼Ÿ

```python
@dataclass
class SamplingParams:
    # ... å­—æ®µå®šä¹‰ ...
    
    def __post_init__(self):
        """å‚æ•°éªŒè¯"""
        
        # 1. best_of é»˜è®¤ç­‰äº n
        if self.best_of is None:
            self.best_of = self.n
        
        # 2. éªŒè¯ n
        if self.n < 1:
            raise ValueError(f"n must be at least 1, got {self.n}")
        
        # 3. éªŒè¯ best_of >= n
        if self.best_of < self.n:
            raise ValueError(
                f"best_of ({self.best_of}) must be >= n ({self.n})"
            )
        
        # 4. éªŒè¯ temperature
        if self.temperature < 0:
            raise ValueError(
                f"temperature must be non-negative, got {self.temperature}"
            )
        
        # 5. éªŒè¯ top_p
        if not 0 < self.top_p <= 1:
            raise ValueError(f"top_p must be in (0, 1], got {self.top_p}")
        
        # 6. éªŒè¯ top_k
        if self.top_k < -1 or self.top_k == 0:
            raise ValueError(
                f"top_k must be -1 (disabled) or >= 1, got {self.top_k}"
            )
        
        # 7. æ£€æŸ¥æœªå®ç°çš„åŠŸèƒ½
        if self.use_beam_search:
            raise NotImplementedError(
                "Beam search is not supported in M0-M1"
            )
        
        # 8. åˆå§‹åŒ–åˆ—è¡¨
        if self.stop is None:
            self.stop = []
        if self.stop_token_ids is None:
            self.stop_token_ids = []
```

**è®²è§£**ï¼š
1. **è‡ªåŠ¨è®¾ç½®**ï¼šbest_of é»˜è®¤ç­‰äº n
2. **èŒƒå›´æ£€æŸ¥**ï¼šç¡®ä¿æ¯ä¸ªå‚æ•°åœ¨åˆç†èŒƒå›´
3. **é€»è¾‘æ£€æŸ¥**ï¼šbest_of å¿…é¡» >= nï¼ˆå¦åˆ™æ— æ³•é€‰å‡º n ä¸ªï¼‰
4. **åŠŸèƒ½æ£€æŸ¥**ï¼šæ˜ç¡®æ ‡è®°æœªå®ç°çš„åŠŸèƒ½
5. **é»˜è®¤å€¼**ï¼šç©ºåˆ—è¡¨è€Œä¸æ˜¯ None

**ä¸ºä»€ä¹ˆè¿™æ ·éªŒè¯**ï¼š
- æå‰å‘ç°é…ç½®é”™è¯¯
- æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯
- é¿å…è¿è¡Œæ—¶æ„å¤–è¡Œä¸º

#### ç¬¬å››æ­¥ï¼šæ·»åŠ è¾…åŠ©æ–¹æ³•

**æ€è€ƒ**ï¼šå¦‚ä½•æ–¹ä¾¿åœ°è·å–é‡‡æ ·ç±»å‹ï¼Ÿ

```python
@dataclass
class SamplingParams:
    # ... å­—æ®µå’ŒéªŒè¯ ...
    
    @property
    def sampling_type(self) -> SamplingType:
        """æ ¹æ®å‚æ•°åˆ¤æ–­é‡‡æ ·ç±»å‹"""
        if self.temperature == 0.0:
            return SamplingType.GREEDY
        else:
            return SamplingType.RANDOM
```

**è®²è§£**ï¼š
- **@property**ï¼šåƒè®¿é—®å±æ€§ä¸€æ ·è°ƒç”¨æ–¹æ³•
- **åˆ¤æ–­é€»è¾‘**ï¼štemperature=0 è¡¨ç¤ºè´ªå¿ƒï¼Œå¦åˆ™éšæœº
- **ä½¿ç”¨**ï¼š`params.sampling_type` è€Œä¸æ˜¯ `params.sampling_type()`

---

## 4. è¯·æ±‚å’Œåºåˆ—å®ç°

### 4.1 è®¾è®¡æ€è·¯

**ç›®æ ‡**ï¼šä¸‰å±‚æŠ½è±¡ç®¡ç†ç”ŸæˆçŠ¶æ€

```
Request (ä¸€ä¸ªæ¨ç†è¯·æ±‚)
  â”œâ”€â”€ Sequence 1 (å€™é€‰åºåˆ— 1)
  â”‚     â””â”€â”€ SequenceData (token IDs)
  â”œâ”€â”€ Sequence 2 (å€™é€‰åºåˆ— 2)
  â””â”€â”€ ...
```

### 4.2 SequenceData å®ç°

#### çº¯æ•°æ®å®¹å™¨

**æ€è€ƒ**ï¼šåºåˆ—éœ€è¦ä¿å­˜å“ªäº›æ•°æ®ï¼Ÿ

```python
from dataclasses import dataclass, field
from typing import List

@dataclass
class SequenceData:
    """åºåˆ—æ•°æ®ï¼ˆçº¯æ•°æ®ï¼Œæ— çŠ¶æ€ï¼‰"""
    prompt_token_ids: List[int]  # è¾“å…¥ token
    output_token_ids: List[int] = field(default_factory=list)  # è¾“å‡º token
```

**è®²è§£**ï¼š
- **ä¸¤ä¸ªåˆ—è¡¨**ï¼šè¾“å…¥å’Œè¾“å‡ºåˆ†å¼€å­˜å‚¨
- **default_factory**ï¼šæ¯ä¸ªå®ä¾‹æœ‰è‡ªå·±çš„åˆ—è¡¨

#### æ·»åŠ æ“ä½œæ–¹æ³•

**æ€è€ƒ**ï¼šéœ€è¦å“ªäº›å¸¸ç”¨æ“ä½œï¼Ÿ

```python
@dataclass
class SequenceData:
    prompt_token_ids: List[int]
    output_token_ids: List[int] = field(default_factory=list)
    
    def get_len(self) -> int:
        """æ€»é•¿åº¦"""
        return len(self.prompt_token_ids) + len(self.output_token_ids)
    
    def get_prompt_len(self) -> int:
        """è¾“å…¥é•¿åº¦"""
        return len(self.prompt_token_ids)
    
    def get_output_len(self) -> int:
        """è¾“å‡ºé•¿åº¦"""
        return len(self.output_token_ids)
    
    def get_token_ids(self) -> List[int]:
        """æ‰€æœ‰ token"""
        return self.prompt_token_ids + self.output_token_ids
    
    def get_last_token_id(self) -> int:
        """æœ€åä¸€ä¸ª token"""
        if self.output_token_ids:
            return self.output_token_ids[-1]
        return self.prompt_token_ids[-1]
    
    def add_token_id(self, token_id: int):
        """æ·»åŠ æ–° token"""
        self.output_token_ids.append(token_id)
```

**è®²è§£**ï¼š
- **å‘½åè§„èŒƒ**ï¼šget_ å‰ç¼€è¡¨ç¤ºæŸ¥è¯¢æ“ä½œ
- **è¾¹ç•Œå¤„ç†**ï¼š`get_last_token_id` è€ƒè™‘äº† output ä¸ºç©ºçš„æƒ…å†µ
- **ä¿®æ”¹æ“ä½œ**ï¼šåªæœ‰ `add_token_id` ä¼šä¿®æ”¹æ•°æ®

### 4.3 Sequence å®ç°

#### ç¬¬ä¸€æ­¥ï¼šå®šä¹‰çŠ¶æ€æšä¸¾

**æ€è€ƒ**ï¼šåºåˆ—æœ‰å“ªäº›çŠ¶æ€ï¼Ÿ

```python
from enum import Enum

class SequenceStatus(Enum):
    """åºåˆ—çŠ¶æ€"""
    WAITING = "waiting"  # ç­‰å¾…è°ƒåº¦
    RUNNING = "running"  # æ­£åœ¨ç”Ÿæˆ
    SWAPPED = "swapped"  # è¢«æ¢å‡ºåˆ° CPU
    FINISHED_STOPPED = "finished_stopped"  # é‡åˆ°åœæ­¢æ¡ä»¶
    FINISHED_LENGTH_CAPPED = "finished_length_capped"  # è¾¾åˆ°æœ€å¤§é•¿åº¦
    FINISHED_ABORTED = "finished_aborted"  # è¢«ç”¨æˆ·ä¸­æ­¢
    FINISHED_IGNORED = "finished_ignored"  # è¢«å¿½ç•¥ï¼ˆbest_of > nï¼‰
    
    def is_finished(self) -> bool:
        """æ˜¯å¦å·²å®Œæˆ"""
        return self in [
            SequenceStatus.FINISHED_STOPPED,
            SequenceStatus.FINISHED_LENGTH_CAPPED,
            SequenceStatus.FINISHED_ABORTED,
            SequenceStatus.FINISHED_IGNORED,
        ]
```

**è®²è§£**ï¼š
- **æ¸…æ™°çš„å‘½å**ï¼šçŠ¶æ€ååæ˜ äº†åºåˆ—æ‰€å¤„çš„é˜¶æ®µ
- **è¾…åŠ©æ–¹æ³•**ï¼š`is_finished()` æ–¹ä¾¿åˆ¤æ–­
- **ä¸‰ç±»çŠ¶æ€**ï¼šè¿è¡Œä¸­ã€ç­‰å¾…ä¸­ã€å®Œæˆ

#### ç¬¬äºŒæ­¥ï¼šå®šä¹‰ Sequence ç±»

**æ€è€ƒ**ï¼šåºåˆ—éœ€è¦å“ªäº›ä¿¡æ¯ï¼Ÿ

```python
from dataclasses import dataclass, field

@dataclass
class Sequence:
    """ä¸€ä¸ªç”Ÿæˆåºåˆ—"""
    seq_id: str  # å”¯ä¸€æ ‡è¯†
    request_id: str  # æ‰€å±è¯·æ±‚
    data: SequenceData  # æ•°æ®
    sampling_params: SamplingParams  # é‡‡æ ·å‚æ•°
    status: SequenceStatus = SequenceStatus.WAITING  # çŠ¶æ€
    
    # M3: KV Cache blocks (é¢„ç•™)
    block_ids: List[int] = field(default_factory=list)
```

**è®²è§£**ï¼š
- **æ ‡è¯†ä¿¡æ¯**ï¼šseq_idï¼ˆè‡ªå·±ï¼‰å’Œ request_idï¼ˆæ‰€å±ï¼‰
- **æ•°æ®å’Œå‚æ•°**ï¼šåˆ†ç¦»å…³æ³¨ç‚¹
- **çŠ¶æ€**ï¼šé»˜è®¤ä¸º WAITING
- **é¢„ç•™å­—æ®µ**ï¼šblock_ids ç”¨äº M3

#### ç¬¬ä¸‰æ­¥ï¼šæ·»åŠ ä»£ç†æ–¹æ³•

**æ€è€ƒ**ï¼šå¦‚ä½•æ–¹ä¾¿åœ°è®¿é—® SequenceData çš„æ–¹æ³•ï¼Ÿ

```python
@dataclass
class Sequence:
    # ... å­—æ®µå®šä¹‰ ...
    
    def get_len(self) -> int:
        """ä»£ç†åˆ° data.get_len()"""
        return self.data.get_len()
    
    def get_prompt_len(self) -> int:
        return self.data.get_prompt_len()
    
    def get_output_len(self) -> int:
        return self.data.get_output_len()
    
    def get_token_ids(self) -> List[int]:
        return self.data.get_token_ids()
    
    def get_last_token_id(self) -> int:
        return self.data.get_last_token_id()
    
    def add_token_id(self, token_id: int):
        return self.data.add_token_id(token_id)
```

**è®²è§£**ï¼š
- **ä»£ç†æ¨¡å¼**ï¼šSequence ä»£ç†åˆ° SequenceData
- **ä¾¿åˆ©æ€§**ï¼š`seq.get_len()` è€Œä¸æ˜¯ `seq.data.get_len()`

#### ç¬¬å››æ­¥ï¼šå®ç° fork æ–¹æ³•

**æ€è€ƒ**ï¼šå¦‚ä½•å¤åˆ¶ä¸€ä¸ªåºåˆ—ï¼Ÿ

```python
@dataclass
class Sequence:
    # ... å…¶ä»–æ–¹æ³• ...
    
    def fork(self, new_seq_id: str) -> "Sequence":
        """å¤åˆ¶åºåˆ—ï¼ˆç”¨äº beam searchï¼‰"""
        # æ·±æ‹·è´æ•°æ®
        new_data = SequenceData(
            prompt_token_ids=self.data.prompt_token_ids.copy(),
            output_token_ids=self.data.output_token_ids.copy(),
        )
        
        # åˆ›å»ºæ–°åºåˆ—
        return Sequence(
            seq_id=new_seq_id,
            request_id=self.request_id,  # ç›¸åŒçš„ request
            data=new_data,
            sampling_params=self.sampling_params,  # å…±äº«
            status=self.status,
            block_ids=self.block_ids.copy(),  # æ·±æ‹·è´
        )
```

**è®²è§£**ï¼š
1. **æ·±æ‹·è´æ•°æ®**ï¼šé¿å…å…±äº«çŠ¶æ€
2. **ä¿ç•™ request_id**ï¼šfork çš„åºåˆ—å±äºåŒä¸€ä¸ªè¯·æ±‚
3. **å…±äº« sampling_params**ï¼šä¸å¯å˜çš„å¯ä»¥å…±äº«
4. **æ‹·è´ block_ids**ï¼šåˆ—è¡¨éœ€è¦æ·±æ‹·è´

**ä¸ºä»€ä¹ˆéœ€è¦ fork**ï¼š
- Beam searchï¼šæ¯æ¬¡æ‰©å±•éƒ½ fork å¤šä¸ªå€™é€‰
- Speculative decodingï¼šéªŒè¯æ¨æµ‹åºåˆ—

### 4.4 Request å®ç°

#### ç¬¬ä¸€æ­¥ï¼šå®šä¹‰è¯·æ±‚çŠ¶æ€

**æ€è€ƒ**ï¼šè¯·æ±‚æœ‰å“ªäº›çŠ¶æ€ï¼Ÿï¼ˆä¸ Sequence ç±»ä¼¼ï¼‰

```python
class RequestStatus(Enum):
    """è¯·æ±‚çŠ¶æ€"""
    WAITING = "waiting"
    RUNNING = "running"
    SWAPPED = "swapped"
    FINISHED_STOPPED = "finished_stopped"
    FINISHED_LENGTH_CAPPED = "finished_length_capped"
    FINISHED_ABORTED = "finished_aborted"
```

#### ç¬¬äºŒæ­¥ï¼šå®šä¹‰ Request ç±»

**æ€è€ƒ**ï¼šè¯·æ±‚éœ€è¦ç®¡ç†ä»€ä¹ˆï¼Ÿ

```python
import time
from typing import Dict

@dataclass
class Request:
    """ä¸€ä¸ªæ¨ç†è¯·æ±‚"""
    request_id: str  # å”¯ä¸€æ ‡è¯†
    prompt: str  # åŸå§‹æ–‡æœ¬
    prompt_token_ids: List[int]  # ç¼–ç åçš„ token
    sampling_params: SamplingParams  # é‡‡æ ·å‚æ•°
    arrival_time: float = field(default_factory=time.time)  # åˆ°è¾¾æ—¶é—´
    
    # è¯¥è¯·æ±‚çš„æ‰€æœ‰åºåˆ—
    sequences: Dict[str, Sequence] = field(default_factory=dict)
    
    # çŠ¶æ€
    status: RequestStatus = RequestStatus.WAITING
```

**è®²è§£**ï¼š
- **åŸå§‹ä¿¡æ¯**ï¼šä¿å­˜åŸå§‹ prompt ç”¨äºè¾“å‡º
- **æ—¶é—´æˆ³**ï¼šè®°å½•åˆ°è¾¾æ—¶é—´ï¼Œç”¨äºç»Ÿè®¡å»¶è¿Ÿ
- **åºåˆ—å­—å…¸**ï¼š`seq_id -> Sequence` çš„æ˜ å°„
- **çŠ¶æ€**ï¼šè¯·æ±‚çº§åˆ«çš„çŠ¶æ€

#### ç¬¬ä¸‰æ­¥ï¼šåˆå§‹åŒ–åºåˆ—

**æ€è€ƒ**ï¼šä»€ä¹ˆæ—¶å€™åˆ›å»ºåºåˆ—ï¼Ÿ

```python
@dataclass
class Request:
    # ... å­—æ®µå®šä¹‰ ...
    
    def __post_init__(self):
        """åˆ›å»ºåºåˆ—"""
        if not self.sequences:
            # æ ¹æ® best_of åˆ›å»ºå¤šä¸ªåºåˆ—
            for i in range(self.sampling_params.best_of):
                seq_id = f"{self.request_id}-{i}"
                
                # åˆ›å»º SequenceData
                seq_data = SequenceData(
                    prompt_token_ids=self.prompt_token_ids.copy()
                )
                
                # åˆ›å»º Sequence
                seq = Sequence(
                    seq_id=seq_id,
                    request_id=self.request_id,
                    data=seq_data,
                    sampling_params=self.sampling_params,
                )
                
                self.sequences[seq_id] = seq
```

**è®²è§£**ï¼š
1. **æ£€æŸ¥æ˜¯å¦å·²åˆ›å»º**ï¼šé¿å…é‡å¤åˆ›å»º
2. **best_of ä¸ªåºåˆ—**ï¼šç”Ÿæˆå¤šä¸ªå€™é€‰
3. **å”¯ä¸€çš„ seq_id**ï¼š`request_id-ç´¢å¼•`
4. **æ·±æ‹·è´ prompt**ï¼šæ¯ä¸ªåºåˆ—ç‹¬ç«‹

#### ç¬¬å››æ­¥ï¼šæ·»åŠ è¾…åŠ©æ–¹æ³•

**æ€è€ƒ**ï¼šå¦‚ä½•æ–¹ä¾¿åœ°æ“ä½œåºåˆ—ï¼Ÿ

```python
@dataclass
class Request:
    # ... å…¶ä»–æ–¹æ³• ...
    
    def get_seqs(self, status: Optional[SequenceStatus] = None) -> List[Sequence]:
        """è·å–åºåˆ—ï¼ˆå¯æŒ‰çŠ¶æ€è¿‡æ»¤ï¼‰"""
        if status is None:
            return list(self.sequences.values())
        return [seq for seq in self.sequences.values() if seq.status == status]
    
    def get_num_seqs(self, status: Optional[SequenceStatus] = None) -> int:
        """è·å–åºåˆ—æ•°é‡"""
        return len(self.get_seqs(status))
    
    def is_finished(self) -> bool:
        """æ˜¯å¦æ‰€æœ‰åºåˆ—éƒ½å®Œæˆ"""
        return all(seq.is_finished() for seq in self.sequences.values())
```

**è®²è§£**ï¼š
- **è¿‡æ»¤åŠŸèƒ½**ï¼šå¯ä»¥åªè·å–ç‰¹å®šçŠ¶æ€çš„åºåˆ—
- **è®¡æ•°æ–¹æ³•**ï¼šå¿«é€Ÿè·å–æ•°é‡
- **å®Œæˆåˆ¤æ–­**ï¼šæ‰€æœ‰åºåˆ—éƒ½å®Œæˆæ‰ç®—å®Œæˆ

---

## 5. è¾“å‡ºæ ¼å¼å®ç°

### 5.1 è®¾è®¡æ€è·¯

**ç›®æ ‡**ï¼šå®šä¹‰æ¸…æ™°çš„è¾“å‡ºæ ¼å¼

```
RequestOutput (è¯·æ±‚çº§åˆ«)
  â””â”€â”€ CompletionOutput[] (æ¯ä¸ªåºåˆ—ä¸€ä¸ª)
```

### 5.2 CompletionOutput å®ç°

**æ€è€ƒ**ï¼šä¸€ä¸ªå®Œæˆçš„åºåˆ—åŒ…å«ä»€ä¹ˆä¿¡æ¯ï¼Ÿ

```python
@dataclass
class CompletionOutput:
    """å•ä¸ªå®Œæˆåºåˆ—çš„è¾“å‡º"""
    index: int  # åºåˆ—ç´¢å¼•ï¼ˆ0 åˆ° n-1ï¼‰
    text: str  # ç”Ÿæˆçš„æ–‡æœ¬
    token_ids: List[int]  # ç”Ÿæˆçš„ token ID
    cumulative_logprob: Optional[float] = None  # ç´¯ç§¯å¯¹æ•°æ¦‚ç‡
    logprobs: Optional[List[float]] = None  # æ¯ä¸ª token çš„æ¦‚ç‡
    finish_reason: Optional[str] = None  # å®ŒæˆåŸå› 
    
    def finished(self) -> bool:
        """æ˜¯å¦å®Œæˆ"""
        return self.finish_reason is not None
```

**è®²è§£**ï¼š
- **åŸºæœ¬ä¿¡æ¯**ï¼šæ–‡æœ¬å’Œ token
- **è´¨é‡æŒ‡æ ‡**ï¼šlogprob ç”¨äºæ’åºï¼ˆbest_ofï¼‰
- **å®ŒæˆåŸå› **ï¼š'stop'ï¼ˆåœæ­¢æ¡ä»¶ï¼‰æˆ– 'length'ï¼ˆè¾¾åˆ°æœ€å¤§é•¿åº¦ï¼‰
- **é¢„ç•™å­—æ®µ**ï¼šlogprobs åœ¨ M1+ å®ç°

### 5.3 RequestOutput å®ç°

**æ€è€ƒ**ï¼šè¯·æ±‚è¾“å‡ºéœ€è¦åŒ…å«ä»€ä¹ˆï¼Ÿ

```python
@dataclass
class RequestOutput:
    """è¯·æ±‚è¾“å‡º"""
    request_id: str  # è¯·æ±‚æ ‡è¯†
    prompt: str  # åŸå§‹è¾“å…¥
    prompt_token_ids: List[int]  # è¾“å…¥ token
    outputs: List[CompletionOutput]  # æ‰€æœ‰å®Œæˆçš„åºåˆ—
    finished: bool  # æ˜¯å¦å®Œæˆ
    
    # é¢„ç•™ï¼šæ€§èƒ½æŒ‡æ ‡
    metrics: Optional[dict] = None
    
    def __repr__(self) -> str:
        """å‹å¥½çš„å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (
            f"RequestOutput(request_id={self.request_id}, "
            f"prompt={self.prompt[:50]}..., "
            f"outputs={len(self.outputs)}, finished={self.finished})"
        )
```

**è®²è§£**ï¼š
- **è¾“å…¥ä¿¡æ¯**ï¼šä¿å­˜åŸå§‹è¾“å…¥ç”¨äºä¸Šä¸‹æ–‡
- **è¾“å‡ºåˆ—è¡¨**ï¼šå¯èƒ½æœ‰å¤šä¸ªï¼ˆn > 1ï¼‰
- **å®Œæˆæ ‡å¿—**ï¼šæ˜¯å¦æ‰€æœ‰åºåˆ—éƒ½å®Œæˆ
- **å‹å¥½æ˜¾ç¤º**ï¼šæˆªæ–­é•¿æ–‡æœ¬é¿å…åˆ·å±

---

## 6. æ¨¡å‹åŠ è½½å™¨å®ç°

### 6.1 è®¾è®¡æ€è·¯

**ç›®æ ‡**ï¼šå°è£… HuggingFace æ¨¡å‹åŠ è½½é€»è¾‘ï¼Œè‡ªåŠ¨å¤„ç†é…ç½®

### 6.2 ModelLoader ç±»å®ç°

#### ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–

**æ€è€ƒ**ï¼šåŠ è½½å™¨éœ€è¦ä»€ä¹ˆä¿¡æ¯ï¼Ÿ

```python
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer

class ModelLoader:
    """HuggingFace æ¨¡å‹åŠ è½½å™¨"""
    
    def __init__(self, model_config: ModelConfig):
        """åˆå§‹åŒ–
        
        Args:
            model_config: æ¨¡å‹é…ç½®
        """
        self.model_config = model_config
```

**è®²è§£**ï¼š
- ç®€å•çš„åˆå§‹åŒ–ï¼Œåªä¿å­˜é…ç½®
- å®é™…åŠ è½½åœ¨è°ƒç”¨ load_model æ—¶è¿›è¡Œ

#### ç¬¬äºŒæ­¥ï¼šå®ç° load_model æ–¹æ³•

**æ€è€ƒ**ï¼šåŠ è½½æ¨¡å‹éœ€è¦å“ªäº›æ­¥éª¤ï¼Ÿ

```python
class ModelLoader:
    # ... __init__ ...
    
    def load_model(self, device: str = "cuda") -> PreTrainedModel:
        """åŠ è½½æ¨¡å‹
        
        Args:
            device: è®¾å¤‡ï¼ˆ'cuda' æˆ– 'cpu'ï¼‰
            
        Returns:
            åŠ è½½çš„æ¨¡å‹
        """
        model_path = self.model_config.model
        
        # 1. ç¡®å®š dtype
        dtype = self._get_dtype()
        
        # 2. åŠ è½½é…ç½®
        hf_config = AutoConfig.from_pretrained(
            model_path,
            trust_remote_code=self.model_config.trust_remote_code,
        )
        
        # 3. æ¨æ–­ max_model_len
        if self.model_config.max_model_len is None:
            if hasattr(hf_config, "max_position_embeddings"):
                self.model_config.max_model_len = hf_config.max_position_embeddings
            else:
                self.model_config.max_model_len = 2048  # é»˜è®¤å€¼
        
        # 4. æ‰“å°ä¿¡æ¯
        print(f"Loading model from {model_path}...")
        print(f"  - dtype: {dtype}")
        print(f"  - max_model_len: {self.model_config.max_model_len}")
        print(f"  - device: {device}")
        
        # 5. åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            config=hf_config,
            torch_dtype=dtype,
            trust_remote_code=self.model_config.trust_remote_code,
            low_cpu_mem_usage=True,  # å‡å°‘ CPU å†…å­˜å³°å€¼
        )
        
        # 6. ç§»åŠ¨åˆ°è®¾å¤‡
        model = model.to(device)
        model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        # 7. æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"Model loaded successfully!")
        print(f"  - Model type: {type(model).__name__}")
        print(f"  - Number of parameters: {self._count_parameters(model):,}")
        
        return model
```

**è®²è§£**ï¼š
1. **åˆ†æ­¥éª¤**ï¼šæ¯ä¸ªæ­¥éª¤åŠŸèƒ½å•ä¸€ï¼Œæ˜“äºç†è§£
2. **è‡ªåŠ¨æ¨æ–­**ï¼šmax_model_len ä»æ¨¡å‹é…ç½®è·å–
3. **å‹å¥½è¾“å‡º**ï¼šæ‰“å°åŠ è½½è¿›åº¦
4. **ä¼˜åŒ–é€‰é¡¹**ï¼š`low_cpu_mem_usage=True` å‡å°‘å†…å­˜å ç”¨
5. **è¯„ä¼°æ¨¡å¼**ï¼š`eval()` å…³é—­ dropout ç­‰è®­ç»ƒç‰¹æ€§

#### ç¬¬ä¸‰æ­¥ï¼šå®ç° load_tokenizer æ–¹æ³•

**æ€è€ƒ**ï¼šåŠ è½½ tokenizer éœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ

```python
class ModelLoader:
    # ... å…¶ä»–æ–¹æ³• ...
    
    def load_tokenizer(self) -> PreTrainedTokenizer:
        """åŠ è½½ tokenizer
        
        Returns:
            åŠ è½½çš„ tokenizer
        """
        tokenizer_path = self.model_config.tokenizer or self.model_config.model
        
        print(f"Loading tokenizer from {tokenizer_path}...")
        
        # ç¡®å®šæ˜¯å¦ä½¿ç”¨ fast tokenizer
        use_fast = self.model_config.tokenizer_mode == "auto"
        
        # åŠ è½½ tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_path,
            use_fast=use_fast,
            trust_remote_code=self.model_config.trust_remote_code,
            padding_side="left",  # ç”Ÿæˆä»»åŠ¡ç”¨ left padding
        )
        
        # è®¾ç½® pad_token
        if tokenizer.pad_token is None:
            if tokenizer.eos_token:
                tokenizer.pad_token = tokenizer.eos_token
            else:
                tokenizer.add_special_tokens({"pad_token": "[PAD]"})
        
        print(f"Tokenizer loaded successfully!")
        print(f"  - Vocab size: {tokenizer.vocab_size}")
        print(f"  - Special tokens: pad={tokenizer.pad_token}, eos={tokenizer.eos_token}")
        
        return tokenizer
```

**è®²è§£**ï¼š
1. **è·¯å¾„å¤„ç†**ï¼štokenizer_path é»˜è®¤ä½¿ç”¨ model_path
2. **fast tokenizer**ï¼šé»˜è®¤ä½¿ç”¨ fast ç‰ˆæœ¬ï¼ˆæ›´å¿«ï¼‰
3. **padding_side**ï¼šleft padding é€‚åˆç”Ÿæˆä»»åŠ¡
4. **pad_token å¤„ç†**ï¼šè‡ªåŠ¨è®¾ç½®ç¼ºå¤±çš„ pad_token

#### ç¬¬å››æ­¥ï¼šè¾…åŠ©æ–¹æ³•

**æ€è€ƒ**ï¼šéœ€è¦å“ªäº›è¾…åŠ©åŠŸèƒ½ï¼Ÿ

```python
class ModelLoader:
    # ... å…¶ä»–æ–¹æ³• ...
    
    def _get_dtype(self) -> torch.dtype:
        """è·å– torch dtype"""
        if self.model_config.torch_dtype is not None:
            return self.model_config.torch_dtype
        
        # é»˜è®¤ï¼šGPU ç”¨ FP16ï¼ŒCPU ç”¨ FP32
        if torch.cuda.is_available():
            return torch.float16
        else:
            return torch.float32
    
    @staticmethod
    def _count_parameters(model: PreTrainedModel) -> int:
        """ç»Ÿè®¡å‚æ•°æ•°é‡"""
        return sum(p.numel() for p in model.parameters())
```

**è®²è§£**ï¼š
- **dtype é€‰æ‹©**ï¼šæ ¹æ®ç¡¬ä»¶è‡ªåŠ¨é€‰æ‹©
- **å‚æ•°ç»Ÿè®¡**ï¼š`numel()` è·å–å…ƒç´ æ•°é‡

#### ç¬¬äº”æ­¥ï¼šä¾¿æ·æ–¹æ³•

**æ€è€ƒ**ï¼šå¦‚ä½•ä¸€æ¬¡æ€§åŠ è½½æ¨¡å‹å’Œ tokenizerï¼Ÿ

```python
class ModelLoader:
    # ... å…¶ä»–æ–¹æ³• ...
    
    def load_model_and_tokenizer(
        self, device: str = "cuda"
    ) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:
        """åŒæ—¶åŠ è½½æ¨¡å‹å’Œ tokenizer
        
        Args:
            device: è®¾å¤‡
            
        Returns:
            (model, tokenizer) å…ƒç»„
        """
        model = self.load_model(device)
        tokenizer = self.load_tokenizer()
        return model, tokenizer

# ä¾¿æ·å‡½æ•°
def get_model_and_tokenizer(
    model_config: ModelConfig, device: str = "cuda"
) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:
    """ä¾¿æ·çš„åŠ è½½å‡½æ•°
    
    Args:
        model_config: æ¨¡å‹é…ç½®
        device: è®¾å¤‡
        
    Returns:
        (model, tokenizer) å…ƒç»„
    """
    loader = ModelLoader(model_config)
    return loader.load_model_and_tokenizer(device)
```

**è®²è§£**ï¼š
- **ä¸€æ¬¡æ€§åŠ è½½**ï¼šé¿å…åˆ†ä¸¤æ¬¡è°ƒç”¨
- **ä¾¿æ·å‡½æ•°**ï¼šæ›´ç®€æ´çš„ API

---

## 7. å·¥å…·å‡½æ•°å®ç°

### 7.1 common.py å·¥å…·å‡½æ•°

#### éšæœºç§å­ç®¡ç†

**æ€è€ƒ**ï¼šå¦‚ä½•ç¡®ä¿ç»“æœå¯å¤ç°ï¼Ÿ

```python
import random
import numpy as np
import torch

def set_random_seed(seed: int) -> None:
    """è®¾ç½®å…¨å±€éšæœºç§å­
    
    Args:
        seed: éšæœºç§å­
    """
    random.seed(seed)  # Python random
    np.random.seed(seed)  # NumPy
    torch.manual_seed(seed)  # PyTorch CPU
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)  # PyTorch GPU
```

**è®²è§£**ï¼š
- **å¤šä¸ªåº“**ï¼šPythonã€NumPyã€PyTorch éƒ½éœ€è¦è®¾ç½®
- **CUDA**ï¼šGPU ä¹Ÿæœ‰ç‹¬ç«‹çš„éšæœºæ•°ç”Ÿæˆå™¨

#### è¯·æ±‚ ID ç”Ÿæˆ

**æ€è€ƒ**ï¼šå¦‚ä½•ç”Ÿæˆå”¯ä¸€çš„è¯·æ±‚ IDï¼Ÿ

```python
import uuid

def generate_request_id() -> str:
    """ç”Ÿæˆå”¯ä¸€çš„è¯·æ±‚ ID
    
    Returns:
        UUID å­—ç¬¦ä¸²
    """
    return str(uuid.uuid4())
```

**è®²è§£**ï¼š
- **UUID**ï¼šå…¨å±€å”¯ä¸€æ ‡è¯†ç¬¦
- **uuid4**ï¼šåŸºäºéšæœºæ•°ï¼Œå†²çªæ¦‚ç‡æä½

#### GPU æ˜¾å­˜ç›‘æ§

**æ€è€ƒ**ï¼šå¦‚ä½•ç›‘æ§ GPU æ˜¾å­˜ä½¿ç”¨ï¼Ÿ

```python
def get_gpu_memory_info(device: int = 0) -> dict:
    """è·å– GPU æ˜¾å­˜ä¿¡æ¯
    
    Args:
        device: GPU è®¾å¤‡ç´¢å¼•
        
    Returns:
        æ˜¾å­˜ä¿¡æ¯å­—å…¸
    """
    if not torch.cuda.is_available():
        return {}
    
    allocated = torch.cuda.memory_allocated(device) / (1024 ** 3)
    reserved = torch.cuda.memory_reserved(device) / (1024 ** 3)
    total = torch.cuda.get_device_properties(device).total_memory / (1024 ** 3)
    free = total - allocated
    
    return {
        "allocated_gb": round(allocated, 2),
        "reserved_gb": round(reserved, 2),
        "free_gb": round(free, 2),
        "total_gb": round(total, 2),
    }
```

**è®²è§£**ï¼š
- **æ£€æŸ¥ CUDA**ï¼šå…ˆæ£€æŸ¥æ˜¯å¦å¯ç”¨
- **ä¸‰ä¸ªæŒ‡æ ‡**ï¼šå·²åˆ†é…ã€å·²é¢„ç•™ã€æ€»æ˜¾å­˜
- **å•ä½è½¬æ¢**ï¼šå­—èŠ‚ â†’ GB

#### è®¾å¤‡ç®¡ç†

**æ€è€ƒ**ï¼šå¦‚ä½•è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ï¼Ÿ

```python
def get_device(device: Optional[str] = None) -> torch.device:
    """è·å– torch device
    
    Args:
        device: è®¾å¤‡å­—ç¬¦ä¸²ï¼ŒNone è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
        
    Returns:
        torch.device
    """
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    return torch.device(device)
```

**è®²è§£**ï¼š
- **è‡ªåŠ¨é€‰æ‹©**ï¼šæœ‰ CUDA ç”¨ CUDAï¼Œå¦åˆ™ç”¨ CPU
- **æ˜¾å¼æŒ‡å®š**ï¼šä¹Ÿå¯ä»¥æ‰‹åŠ¨æŒ‡å®šè®¾å¤‡

---

## 8. åŒ…å¯¼å‡ºå’Œæµ‹è¯•

### 8.1 __init__.py å®ç°

**æ€è€ƒ**ï¼šå¦‚ä½•è®©ç”¨æˆ·æ–¹ä¾¿åœ°å¯¼å…¥ï¼Ÿ

```python
"""
FoloVLLM: A Lightweight LLM Inference Framework
"""

__version__ = "0.1.0"
__author__ = "FoloVLLM Contributors"

# å¯¼å‡ºæ ¸å¿ƒç±»
from folovllm.config import (
    CacheConfig,
    EngineConfig,
    ModelConfig,
    SchedulerConfig,
)
from folovllm.sampling_params import SamplingParams, SamplingType
from folovllm.request import (
    Request,
    RequestStatus,
    Sequence,
    SequenceData,
    SequenceStatus,
)
from folovllm.outputs import CompletionOutput, RequestOutput
from folovllm.model_loader import ModelLoader, get_model_and_tokenizer
from folovllm.utils.common import (
    generate_request_id,
    get_device,
    get_gpu_memory_info,
    is_cuda_available,
    set_random_seed,
)

__all__ = [
    "__version__",
    # é…ç½®
    "ModelConfig",
    "CacheConfig",
    "SchedulerConfig",
    "EngineConfig",
    # é‡‡æ ·
    "SamplingParams",
    "SamplingType",
    # è¯·æ±‚å’Œåºåˆ—
    "Request",
    "RequestStatus",
    "Sequence",
    "SequenceData",
    "SequenceStatus",
    # è¾“å‡º
    "RequestOutput",
    "CompletionOutput",
    # æ¨¡å‹åŠ è½½
    "ModelLoader",
    "get_model_and_tokenizer",
    # å·¥å…·
    "generate_request_id",
    "set_random_seed",
    "get_device",
    "is_cuda_available",
    "get_gpu_memory_info",
]
```

**è®²è§£**ï¼š
- **ç»Ÿä¸€å¯¼å‡º**ï¼šç”¨æˆ·å¯ä»¥ `from folovllm import ModelConfig`
- **__all__**ï¼šæ˜ç¡®å“ªäº›æ˜¯å…¬å…± API
- **åˆ†ç»„**ï¼šæŒ‰åŠŸèƒ½åˆ†ç»„ï¼Œæ˜“äºæŸ¥æ‰¾

### 8.2 æµ‹è¯•å®ç°æ€è·¯

#### å•å…ƒæµ‹è¯•ç»“æ„

**æ€è€ƒ**ï¼šå¦‚ä½•ç»„ç»‡æµ‹è¯•ï¼Ÿ

```python
# tests/unit/test_m0_config.py
class TestModelConfig:
    """æµ‹è¯• ModelConfig"""
    
    def test_basic_creation(self):
        """æµ‹è¯•åŸºæœ¬åˆ›å»º"""
        config = ModelConfig(model="test")
        assert config.model == "test"
        assert config.tokenizer == "test"  # è‡ªåŠ¨è®¾ç½®
    
    def test_dtype_conversion(self):
        """æµ‹è¯• dtype è½¬æ¢"""
        config = ModelConfig(model="test", dtype="float16")
        assert config.torch_dtype == torch.float16
```

**è®²è§£**ï¼š
- **ç±»ç»„ç»‡**ï¼šæ¯ä¸ªç±»ä¸€ä¸ªæµ‹è¯•ç±»
- **å‘½åè§„èŒƒ**ï¼štest_ å‰ç¼€
- **æ¸…æ™°çš„æ–‡æ¡£å­—ç¬¦ä¸²**ï¼šè¯´æ˜æµ‹è¯•å†…å®¹

#### é›†æˆæµ‹è¯•æ€è·¯

**æ€è€ƒ**ï¼šå¦‚ä½•æµ‹è¯•ç«¯åˆ°ç«¯æµç¨‹ï¼Ÿ

```python
# tests/integration/test_m0_model_loading.py
class TestModelLoadingGPU:
    """æµ‹è¯• GPU æ¨¡å‹åŠ è½½"""
    
    @pytest.mark.skipif(
        not torch.cuda.is_available(),
        reason="CUDA not available",
    )
    def test_load_model_and_tokenizer(self):
        """æµ‹è¯•åŠ è½½æ¨¡å‹å’Œ tokenizer"""
        config = ModelConfig(
            model="Qwen/Qwen2.5-0.6B",
            dtype="float16",
        )
        
        try:
            model, tokenizer = get_model_and_tokenizer(config)
            
            # æµ‹è¯• tokenization
            tokens = tokenizer.encode("Hello")
            assert isinstance(tokens, list)
            
            # æµ‹è¯• decoding
            text = tokenizer.decode(tokens)
            assert isinstance(text, str)
            
        except Exception as e:
            pytest.skip(f"Model loading failed: {e}")
```

**è®²è§£**ï¼š
- **æ¡ä»¶è·³è¿‡**ï¼šæ²¡æœ‰ CUDA æ—¶è·³è¿‡
- **å¼‚å¸¸å¤„ç†**ï¼šæ¨¡å‹æœªä¸‹è½½æ—¶è·³è¿‡
- **å®é™…éªŒè¯**ï¼šæµ‹è¯•å®Œæ•´æµç¨‹

---

## 9. æ€»ç»“ï¼šå¼€å‘æµç¨‹å›é¡¾

### 9.1 å¼€å‘é¡ºåº

1. **ç¯å¢ƒè„šæœ¬** â†’ è®©å…¶ä»–å¼€å‘è€…èƒ½å¿«é€Ÿå¼€å§‹
2. **é…ç½®ç³»ç»Ÿ** â†’ å®šä¹‰é¡¹ç›®çš„"è§„åˆ™"
3. **æ•°æ®ç»“æ„** â†’ å®šä¹‰æ ¸å¿ƒæŠ½è±¡ï¼ˆRequest/Sequenceï¼‰
4. **å·¥å…·å‡½æ•°** â†’ æä¾›åŸºç¡€èƒ½åŠ›
5. **æ¨¡å‹åŠ è½½** â†’ è¿æ¥ HuggingFace
6. **æµ‹è¯•** â†’ éªŒè¯æ­£ç¡®æ€§

### 9.2 è®¾è®¡åŸåˆ™

1. **åˆ†å±‚è®¾è®¡**ï¼šé…ç½®ã€æ•°æ®ã€é€»è¾‘åˆ†ç¦»
2. **ç±»å‹å®‰å…¨**ï¼šä½¿ç”¨ç±»å‹æç¤ºå’ŒéªŒè¯
3. **é¢„ç•™æ¥å£**ï¼šä¸ºåç»­ milestone é¢„ç•™
4. **å¯¹é½ vLLM**ï¼šå­¦ä¹ æˆç†Ÿæ¡†æ¶

### 9.3 å…³é”®æŠ€å·§

1. **dataclass**ï¼šå‡å°‘æ ·æ¿ä»£ç 
2. **__post_init__**ï¼šåˆå§‹åŒ–åå¤„ç†å’ŒéªŒè¯
3. **Enum**ï¼šç±»å‹å®‰å…¨çš„å¸¸é‡
4. **ä»£ç†æ¨¡å¼**ï¼šç®€åŒ–æ¥å£

### 9.4 æµ‹è¯•ç­–ç•¥

1. **å•å…ƒæµ‹è¯•**ï¼šæ¯ä¸ªç±»ã€æ¯ä¸ªæ–¹æ³•
2. **å‚æ•°éªŒè¯æµ‹è¯•**ï¼šè¾¹ç•Œæ¡ä»¶
3. **é›†æˆæµ‹è¯•**ï¼šç«¯åˆ°ç«¯æµç¨‹
4. **å¼‚å¸¸å¤„ç†**ï¼šä¼˜é›…åœ°å¤„ç†é”™è¯¯

---

## 10. ä¸‹ä¸€æ­¥ï¼šMilestone 1

### 10.1 M1 éœ€è¦å®ç°ä»€ä¹ˆï¼Ÿ

åŸºäº M0 çš„åŸºç¡€ï¼ŒM1 å°†å®ç°ï¼š

1. **LLMEngine**ï¼šæ¨ç†å¼•æ“ä¸»ç±»
   - ä½¿ç”¨ ModelConfig åŠ è½½æ¨¡å‹
   - ä½¿ç”¨ SamplingParams é…ç½®é‡‡æ ·
   - ç®¡ç† Request ç”Ÿå‘½å‘¨æœŸ

2. **Token ç”Ÿæˆ**ï¼š
   - å®ç° Greedyã€Top-kã€Top-p é‡‡æ ·
   - ä½¿ç”¨ Sequence ç®¡ç†ç”ŸæˆçŠ¶æ€

3. **ç®€å• KV Cache**ï¼š
   - ä¸ºæ¯ä¸ª Sequence åˆ†é…è¿ç»­å†…å­˜

4. **å®Œæ•´æ¨ç†å¾ªç¯**ï¼š
   - Tokenization â†’ Forward â†’ Sampling â†’ Detokenization

### 10.2 M0 å¦‚ä½•æ”¯æŒ M1ï¼Ÿ

- **Request/Sequence**ï¼šç®¡ç†ç”ŸæˆçŠ¶æ€
- **SamplingParams**ï¼šæ§åˆ¶é‡‡æ ·è¡Œä¸º
- **ModelLoader**ï¼šåŠ è½½æ¨¡å‹å’Œ tokenizer
- **å·¥å…·å‡½æ•°**ï¼šè®¾ç½®éšæœºç§å­ã€ç®¡ç†è®¾å¤‡

### 10.3 å­¦ä¹ å»ºè®®

1. **åŠ¨æ‰‹å®éªŒ**ï¼šä¿®æ”¹é…ç½®ï¼Œè§‚å¯Ÿè¡Œä¸º
2. **é˜…è¯»æµ‹è¯•**ï¼šæµ‹è¯•ä»£ç æ˜¯æœ€å¥½çš„æ–‡æ¡£
3. **å¯¹æ¯” vLLM**ï¼šç†è§£è®¾è®¡æ€è·¯
4. **æå‰é¢„ä¹  M1**ï¼šäº†è§£å¦‚ä½•ä½¿ç”¨ M0 çš„æ¥å£

---

**æ­å–œå®Œæˆ Milestone 0ï¼** ğŸ‰

ä½ å·²ç»å­¦ä¹ äº†ï¼š
- âœ… é¡¹ç›®ç¯å¢ƒè®¾ç½®è‡ªåŠ¨åŒ–
- âœ… åˆ†å±‚é…ç½®ç³»ç»Ÿè®¾è®¡
- âœ… é‡‡æ ·ç­–ç•¥åŸç†å’Œå®ç°
- âœ… ä¸‰å±‚åºåˆ—æŠ½è±¡ï¼ˆRequest/Sequence/SequenceDataï¼‰
- âœ… HuggingFace æ¨¡å‹åŠ è½½
- âœ… æµ‹è¯•é©±åŠ¨å¼€å‘

**å‡†å¤‡å¥½å¼€å§‹ Milestone 1 äº†å—ï¼Ÿ** ğŸš€

