# Milestone 2: è¿ç»­æ‰¹å¤„ç† - å£è¿°å±•ç¤ºæ–‡æ¡£

**å±•ç¤ºç›®æ ‡**: ä»¥ç±»/å‡½æ•°ä¸ºå•ä½ï¼Œé€šè¿‡å®Œæ•´çš„æ¨ç†è¿‡ç¨‹å‘å°ç™½è®²è§£å¦‚ä½•å¼€å‘è¿ç»­æ‰¹å¤„ç†ç³»ç»Ÿ

---

## ğŸ¯ å±•ç¤ºå¤§çº²

### å¼€åœºç™½

å¤§å®¶å¥½ï¼ä»Šå¤©æˆ‘è¦å‘å¤§å®¶å±•ç¤ºå¦‚ä½•ä»é›¶å¼€å§‹å®ç°ä¸€ä¸ªè¿ç»­æ‰¹å¤„ç†ç³»ç»Ÿã€‚è¿™æ˜¯ç°ä»£ LLM æ¨ç†æ¡†æ¶çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œèƒ½å¤Ÿå°†ååé‡æå‡ 3-5 å€ã€‚

æˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„æ¨ç†è¿‡ç¨‹ï¼Œçœ‹çœ‹æ¯ä¸ªç±»å’Œå‡½æ•°æ˜¯å¦‚ä½•åä½œçš„ï¼Œä»¥åŠä¸ºä»€ä¹ˆè¦è¿™æ ·è®¾è®¡ã€‚

---

## ğŸ¨ ç³»ç»Ÿæ¶æ„å›¾

### M2 è¿ç»­æ‰¹å¤„ç†ç³»ç»Ÿç±»å›¾

```mermaid
classDiagram
    %% ç”¨æˆ·æ¥å£å±‚
    class LLMEngine {
        +ModelConfig model_config
        +SchedulerConfig scheduler_config
        +EngineCore engine_core
        +InputProcessor processor
        +Sampler sampler
        +generate(prompt) RequestOutput
        +generate_batch(prompts) Dict[str, RequestOutput]
    }

    %% å¼•æ“æ ¸å¿ƒå±‚
    class EngineCore {
        +Scheduler scheduler
        +GPUExecutor executor
        +Sampler sampler
        +InputProcessor processor
        +int iteration
        +step() Dict[str, RequestOutput]
        +add_request(request)
        +has_unfinished_requests() bool
    }

    %% è°ƒåº¦å™¨å±‚
    class Scheduler {
        +FCFSRequestQueue waiting
        +List[Request] running
        +Dict[str, Request] requests
        +Set[str] finished_req_ids
        +schedule() SchedulerOutput
        +update_from_output(output, tokens) Dict[str, RequestOutput]
        +add_request(request)
    }

    class SchedulerInterface {
        <<interface>>
        +schedule() SchedulerOutput
        +update_from_output()
        +add_request(request)
        +finish_requests(req_ids)
    }

    %% è¯·æ±‚é˜Ÿåˆ—
    class FCFSRequestQueue {
        +add_request(request)
        +pop_request() Request
        +peek_request() Request
        +prepend_request(request)
        +__len__() int
        +__bool__() bool
    }

    class RequestQueue {
        <<interface>>
        +add_request(request)
        +pop_request() Request
        +peek_request() Request
    }

    %% è°ƒåº¦å™¨è¾“å‡º
    class SchedulerOutput {
        +List[NewRequestData] scheduled_new_reqs
        +CachedRequestData scheduled_cached_reqs
        +Dict[str, int] num_scheduled_tokens
        +int total_num_scheduled_tokens
        +Set[str] finished_req_ids
    }

    class NewRequestData {
        +str req_id
        +List[int] prompt_token_ids
        +SamplingParams sampling_params
        +int num_computed_tokens
        +List[int] block_ids
    }

    class CachedRequestData {
        +List[str] req_ids
        +List[List[int]] new_token_ids
        +List[int] num_computed_tokens
        +List[int] num_output_tokens
        +make_empty() CachedRequestData
    }

    %% æ‰¹å¤„ç†å±‚
    class InputBatch {
        +List[str] req_ids
        +List[List[int]] token_ids
        +List[int] start_positions
        +List[bool] is_prefill
        +List[int] prompt_lens
        +to_tensors(device) Tuple[Tensor, Tensor, Tensor]
        +int batch_size
    }

    %% æ‰§è¡Œå™¨å±‚
    class GPUExecutor {
        +GPUWorker worker
        +execute_model_batch(input_batch) Dict[str, Tensor]
        +free_request_cache(req_id)
        +clear_kv_caches()
    }

    class GPUWorker {
        +ModelRunner model_runner
        +PreTrainedModel model
        +execute_model_batch(input_batch) Dict[str, Tensor]
        +free_request_cache(req_id)
    }

    class ModelRunner {
        +PreTrainedModel model
        +Dict[str, Any] request_caches
        +execute_model_batch(input_batch) Dict[str, Tensor]
        +free_request_cache(req_id)
        +clear_kv_caches()
    }

    %% æ•°æ®ç»“æ„
    class Request {
        +str request_id
        +str prompt
        +List[int] prompt_token_ids
        +SamplingParams sampling_params
        +Dict[str, Sequence] sequences
        +RequestStatus status
    }

    class Sequence {
        +str seq_id
        +str request_id
        +SequenceData data
        +SamplingParams sampling_params
        +SequenceStatus status
        +add_token_id(token_id)
        +is_finished() bool
    }

    class RequestOutput {
        +str request_id
        +str prompt
        +List[int] prompt_token_ids
        +List[CompletionOutput] outputs
        +bool finished
        +Dict metrics
    }

    %% å…³ç³»
    LLMEngine --> EngineCore
    LLMEngine --> InputProcessor
    LLMEngine --> Sampler

    EngineCore --> Scheduler
    EngineCore --> GPUExecutor
    EngineCore --> Sampler
    EngineCore --> InputProcessor

    Scheduler --|> SchedulerInterface
    Scheduler --> FCFSRequestQueue
    Scheduler --> Request
    Scheduler --> SchedulerOutput

    FCFSRequestQueue --|> RequestQueue

    SchedulerOutput --> NewRequestData
    SchedulerOutput --> CachedRequestData

    GPUExecutor --> GPUWorker
    GPUWorker --> ModelRunner

    Request --> Sequence
    
    %% æ•°æ®æµ
    SchedulerOutput ..> InputBatch : prepare_inputs_from_scheduler_output()
    InputBatch ..> ModelRunner : execute_model_batch()
```

### M2 ç³»ç»Ÿåˆ†å±‚æ¶æ„

```mermaid
graph TB
    subgraph "ç”¨æˆ·æ¥å£å±‚"
        A[LLMEngine]
        A1[generate_batch API]
    end

    subgraph "å¼•æ“åè°ƒå±‚"
        B[EngineCore]
        B1[step ä¸»å¾ªç¯]
    end

    subgraph "è°ƒåº¦å†³ç­–å±‚"
        C[Scheduler]
        C1[FCFSRequestQueue]
        C2[SchedulerOutput]
    end

    subgraph "æ‰¹å¤„ç†å‡†å¤‡å±‚"
        D[InputBatch]
        D1[prepare_inputs_from_scheduler_output]
    end

    subgraph "æ¨¡å‹æ‰§è¡Œå±‚"
        E[GPUExecutor]
        E1[GPUWorker]
        E2[ModelRunner]
    end

    subgraph "é‡‡æ ·ç”Ÿæˆå±‚"
        F[Sampler]
        F1[sample next tokens]
    end

    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> B
    B --> A

    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#fce4ec
    style F fill:#f1f8e9
```

---

## ğŸ”„ ç«¯åˆ°ç«¯æ•°æ®æµå›¾

### å®Œæ•´çš„æ‰¹å¤„ç†æ¨ç†æ•°æ®æµ

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant Engine as LLMEngine
    participant Core as EngineCore
    participant Sched as Scheduler
    participant Queue as FCFSRequestQueue
    participant Batch as InputBatch
    participant Exec as GPUExecutor
    participant Model as ModelRunner
    participant Sampler as Sampler

    Note over User, Sampler: æ‰¹å¤„ç†æ¨ç†å®Œæ•´æµç¨‹

    %% åˆå§‹åŒ–é˜¶æ®µ
    User->>Engine: generate_batch(prompts, sampling_params)
    Engine->>Engine: è½¬æ¢ prompts ä¸º Request å¯¹è±¡
    
    loop ä¸ºæ¯ä¸ª prompt åˆ›å»ºè¯·æ±‚
        Engine->>Core: add_request(request)
        Core->>Sched: add_request(request)
        Sched->>Queue: add_request(request)
    end

    %% ä¸»å¾ªç¯å¼€å§‹
    loop ç›´åˆ°æ‰€æœ‰è¯·æ±‚å®Œæˆ
        Note over Engine, Sampler: === è¿­ä»£ N å¼€å§‹ ===
        
        Engine->>Core: step()
        
        %% è°ƒåº¦é˜¶æ®µ
        Core->>Sched: schedule()
        Sched->>Queue: æ£€æŸ¥ waiting é˜Ÿåˆ—
        Sched->>Sched: åº”ç”¨è°ƒåº¦ç­–ç•¥ (FCFS + Tokené¢„ç®—)
        
        alt æœ‰æ–°è¯·æ±‚å¯æ¥çº³
            Sched->>Queue: pop_request() (æ¥çº³æ–°è¯·æ±‚)
            Sched->>Sched: ç§»åŠ¨åˆ° running é˜Ÿåˆ—
            Note right of Sched: è°ƒåº¦æ•´ä¸ª prompt (prefill)
        end
        
        loop ä¸ºæ¯ä¸ªè¿è¡Œä¸­è¯·æ±‚
            Note right of Sched: è°ƒåº¦ 1 token (decode)
        end
        
        Sched-->>Core: SchedulerOutput
        
        %% æ‰¹å¤„ç†å‡†å¤‡é˜¶æ®µ
        Core->>Batch: prepare_inputs_from_scheduler_output()
        Batch->>Batch: æ„å»º InputBatch
        Note right of Batch: å¤„ç†ä¸å®šé•¿åºåˆ—<br/>åˆ›å»º padding å’Œ mask
        Batch-->>Core: InputBatch
        
        %% æ¨¡å‹æ‰§è¡Œé˜¶æ®µ
        Core->>Exec: execute_model_batch(input_batch)
        Exec->>Model: execute_model_batch(input_batch)
        
        loop ä¸ºæ¯ä¸ªè¯·æ±‚å•ç‹¬æ‰§è¡Œ (M2é™åˆ¶)
            Model->>Model: è·å–è¯·æ±‚çš„ KV cache
            Model->>Model: æ‰§è¡Œæ¨¡å‹å‰å‘ä¼ æ’­
            Model->>Model: æ›´æ–°è¯·æ±‚çš„ KV cache
            Note right of Model: HuggingFace æ¨¡å‹<br/>past_key_values æœºåˆ¶
        end
        
        Model-->>Exec: Dict[req_id, logits]
        Exec-->>Core: Dict[req_id, logits]
        
        %% é‡‡æ ·é˜¶æ®µ
        loop ä¸ºæ¯ä¸ªè¯·æ±‚é‡‡æ ·
            Core->>Sampler: sample(logits, sampling_params)
            Sampler-->>Core: next_token_id
        end
        
        %% çŠ¶æ€æ›´æ–°é˜¶æ®µ
        Core->>Sched: update_from_output(scheduler_output, sampled_tokens)
        
        loop ä¸ºæ¯ä¸ªè¯·æ±‚æ›´æ–°
            Sched->>Sched: æ·»åŠ æ–° token åˆ°åºåˆ—
            Sched->>Sched: æ£€æŸ¥åœæ­¢æ¡ä»¶
            alt è¯·æ±‚å®Œæˆ
                Sched->>Sched: ç§»å‡º running é˜Ÿåˆ—
                Sched->>Sched: æ·»åŠ åˆ° finished_req_ids
            end
        end
        
        Sched-->>Core: Dict[req_id, RequestOutput]
        
        %% æ¸…ç†é˜¶æ®µ
        loop ä¸ºæ¯ä¸ªå®Œæˆçš„è¯·æ±‚
            Core->>Exec: free_request_cache(req_id)
            Exec->>Model: free_request_cache(req_id)
            Model->>Model: åˆ é™¤ KV cache
        end
        
        Core-->>Engine: step_outputs
        Engine->>Engine: æ”¶é›†å®Œæˆçš„è¯·æ±‚è¾“å‡º
    end

    Engine-->>User: Dict[req_id, RequestOutput]
```

### å…³é”®æ•°æ®ç»“æ„è½¬æ¢æµç¨‹

```mermaid
graph LR
    subgraph "è¾“å…¥é˜¶æ®µ"
        A["List[str] prompts"] --> B["List[Request]"]
        B --> C["FCFSRequestQueue"]
    end

    subgraph "è°ƒåº¦é˜¶æ®µ"
        C --> D["SchedulerOutput"]
        D1["NewRequestData"] -.-> D
        D2["CachedRequestData"] -.-> D
    end

    subgraph "æ‰¹å¤„ç†é˜¶æ®µ"
        D --> E["InputBatch"]
        E1["token_ids: List[List[int]]"] -.-> E
        E2["start_positions: List[int]"] -.-> E
        E3["is_prefill: List[bool]"] -.-> E
    end

    subgraph "æ‰§è¡Œé˜¶æ®µ"
        E --> F["Padded Tensors"]
        F1["token_ids: [B, L]"] -.-> F
        F2["attention_mask: [B, L]"] -.-> F
        F3["positions: [B, L]"] -.-> F
    end

    subgraph "è¾“å‡ºé˜¶æ®µ"
        F --> G["Dict[req_id, logits]"]
        G --> H["Dict[req_id, next_token]"]
        H --> I["Dict[req_id, RequestOutput]"]
    end

    style A fill:#e3f2fd
    style D fill:#e8f5e8
    style E fill:#fff3e0
    style F fill:#fce4ec
    style I fill:#f1f8e9
```

### å…·ä½“æ‰¹å¤„ç†ç¤ºä¾‹ï¼š3ä¸ªè¯·æ±‚çš„å®Œæ•´å¤„ç†æµç¨‹

```mermaid
gantt
    title è¿ç»­æ‰¹å¤„ç† vs é™æ€æ‰¹å¤„ç†å¯¹æ¯”
    dateFormat X
    axisFormat %s

    section é™æ€æ‰¹å¤„ç†
    Request 1 (5 tokens)  :done, static1, 0, 5
    Request 2 (20 tokens) :done, static2, 0, 20
    Request 3 (3 tokens)  :done, static3, 0, 20
    GPU ç©ºé—²æ—¶é—´         :crit, idle, 5, 20

    section è¿ç»­æ‰¹å¤„ç†
    Request 1 (5 tokens)  :done, cont1, 0, 5
    Request 2 (20 tokens) :done, cont2, 0, 20
    Request 3 (3 tokens)  :done, cont3, 0, 3
    Request 4 å¼€å§‹        :active, cont4, 3, 8
    Request 5 å¼€å§‹        :active, cont5, 5, 12
```

### æ‰¹å¤„ç†è¿­ä»£æ—¶é—´çº¿è¯¦è§£

```mermaid
timeline
    title è¿ç»­æ‰¹å¤„ç†è¿­ä»£æ—¶é—´çº¿
    
    section è¿­ä»£ 1
        è°ƒåº¦å†³ç­– : æ¥çº³ req-1, req-2, req-3 (prefill)
        æ‰¹å¤„ç†æ‰§è¡Œ : å¤„ç† [5+20+3] = 28 tokens
        é‡‡æ ·ç”Ÿæˆ : ä¸ºæ¯ä¸ªè¯·æ±‚ç”Ÿæˆç¬¬ä¸€ä¸ª token
        çŠ¶æ€æ›´æ–° : æ‰€æœ‰è¯·æ±‚è¿›å…¥ decode é˜¶æ®µ
    
    section è¿­ä»£ 2
        è°ƒåº¦å†³ç­– : req-1, req-2, req-3 ç»§ç»­ decode
                 : æ¥çº³ req-4 (prefill)
        æ‰¹å¤„ç†æ‰§è¡Œ : å¤„ç† [1+1+1+15] = 18 tokens
        é‡‡æ ·ç”Ÿæˆ : ç»§ç»­ç”Ÿæˆ tokens
        çŠ¶æ€æ›´æ–° : æ£€æŸ¥åœæ­¢æ¡ä»¶
    
    section è¿­ä»£ 3
        è°ƒåº¦å†³ç­– : req-3 å®Œæˆï¼Œç§»å‡ºæ‰¹æ¬¡
                 : req-1, req-2, req-4 ç»§ç»­
                 : æ¥çº³ req-5 (prefill)
        æ‰¹å¤„ç†æ‰§è¡Œ : å¤„ç† [1+1+1+8] = 11 tokens
        é‡‡æ ·ç”Ÿæˆ : ç»§ç»­ç”Ÿæˆ
        çŠ¶æ€æ›´æ–° : æ¸…ç† req-3 èµ„æº
    
    section è¿­ä»£ N
        è°ƒåº¦å†³ç­– : åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡
        æ‰¹å¤„ç†æ‰§è¡Œ : ä¿æŒ GPU æ»¡è½½
        é‡‡æ ·ç”Ÿæˆ : æŒç»­ç”Ÿæˆ
        çŠ¶æ€æ›´æ–° : ç®¡ç†è¯·æ±‚ç”Ÿå‘½å‘¨æœŸ
```

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šç†è§£é—®é¢˜ - ä¸ºä»€ä¹ˆéœ€è¦è¿ç»­æ‰¹å¤„ç†ï¼Ÿ

### æ¼”ç¤ºï¼šä¼ ç»Ÿæ‰¹å¤„ç†çš„é—®é¢˜

```python
# ä¼ ç»Ÿé™æ€æ‰¹å¤„ç†çš„é—®é¢˜æ¼”ç¤º
def static_batching_demo():
    """æ¼”ç¤ºé™æ€æ‰¹å¤„ç†çš„æ•ˆç‡é—®é¢˜"""
    
    # å‡è®¾æˆ‘ä»¬æœ‰4ä¸ªè¯·æ±‚
    requests = [
        {"id": "req-1", "prompt": "Hi", "expected_tokens": 5},
        {"id": "req-2", "prompt": "What is AI?", "expected_tokens": 50},
        {"id": "req-3", "prompt": "Hello", "expected_tokens": 3},
        {"id": "req-4", "prompt": "Explain quantum computing", "expected_tokens": 100},
    ]
    
    print("é™æ€æ‰¹å¤„ç†æ—¶é—´çº¿ï¼š")
    print("æ—¶é—´ 0: æ‰€æœ‰è¯·æ±‚å¼€å§‹")
    print("æ—¶é—´ 3: req-3 å®Œæˆï¼Œä½†éœ€è¦ç­‰å¾…")
    print("æ—¶é—´ 5: req-1 å®Œæˆï¼Œä½†éœ€è¦ç­‰å¾…")
    print("æ—¶é—´ 50: req-2 å®Œæˆï¼Œä½†éœ€è¦ç­‰å¾…")
    print("æ—¶é—´ 100: req-4 å®Œæˆï¼Œæ‰€æœ‰è¯·æ±‚ç»“æŸ")
    print("GPU åˆ©ç”¨ç‡ï¼š25%ï¼ˆå¤§é‡æ—¶é—´åœ¨ç­‰å¾…ï¼‰")
```

**è®²è§£è¦ç‚¹**:
- çŸ­è¯·æ±‚å®Œæˆå GPU ç©ºé—²
- èµ„æºæµªè´¹ä¸¥é‡
- ååé‡å—æœ€é•¿è¯·æ±‚é™åˆ¶

### è¿ç»­æ‰¹å¤„ç†çš„è§£å†³æ€è·¯

```python
def continuous_batching_demo():
    """æ¼”ç¤ºè¿ç»­æ‰¹å¤„ç†çš„ä¼˜åŠ¿"""
    
    print("è¿ç»­æ‰¹å¤„ç†æ—¶é—´çº¿ï¼š")
    print("è¿­ä»£ 1: [req-1, req-2, req-3, req-4] å¼€å§‹")
    print("è¿­ä»£ 3: [req-1, req-2, req-4, req-5] req-3å®Œæˆï¼Œreq-5åŠ å…¥")
    print("è¿­ä»£ 5: [req-2, req-4, req-5, req-6] req-1å®Œæˆï¼Œreq-6åŠ å…¥")
    print("...")
    print("GPU åˆ©ç”¨ç‡ï¼š75%ï¼ˆå§‹ç»ˆä¿æŒæ»¡è½½ï¼‰")
```

**è®²è§£è¦ç‚¹**:
- æ¯æ¬¡è¿­ä»£ç‹¬ç«‹è°ƒåº¦
- åŠ¨æ€ç»´æŠ¤æ»¡è½½æ‰¹æ¬¡
- å¤§å¹…æå‡èµ„æºåˆ©ç”¨ç‡

---

## ğŸ—ï¸ ç¬¬äºŒéƒ¨åˆ†ï¼šæ ¸å¿ƒç»„ä»¶è®¾è®¡

### 2.1 è¯·æ±‚é˜Ÿåˆ— - ç®¡ç†è¯·æ±‚çš„ç”Ÿå‘½å‘¨æœŸ

#### FCFSRequestQueue ç±»

```python
class FCFSRequestQueue(deque, RequestQueue):
    """First-Come-First-Served è¯·æ±‚é˜Ÿåˆ—
    
    ä¸ºä»€ä¹ˆç»§æ‰¿ dequeï¼Ÿ
    - O(1) çš„æ·»åŠ å’Œå¼¹å‡ºæ“ä½œ
    - å¤©ç„¶æ”¯æŒ FIFO è¯­ä¹‰
    - å†…ç½®çš„è¿­ä»£å™¨æ”¯æŒ
    """
    
    def add_request(self, request: Request) -> None:
        """æ·»åŠ è¯·æ±‚åˆ°é˜Ÿåˆ—æœ«å°¾
        
        è¿™é‡Œä½“ç°äº† FCFS çš„å…¬å¹³æ€§ï¼š
        - å…ˆåˆ°å…ˆæœåŠ¡
        - é¿å…é¥¥é¥¿é—®é¢˜
        """
        self.append(request)
        print(f"ğŸ“¥ è¯·æ±‚ {request.request_id} åŠ å…¥ç­‰å¾…é˜Ÿåˆ—")
    
    def pop_request(self) -> Request:
        """ä»é˜Ÿåˆ—å¤´éƒ¨å¼¹å‡ºè¯·æ±‚
        
        ä¸ºä»€ä¹ˆä»å·¦è¾¹å¼¹å‡ºï¼Ÿ
        - ä¿è¯ FIFO é¡ºåº
        - å®ç°å…¬å¹³è°ƒåº¦
        """
        request = self.popleft()
        print(f"ğŸ“¤ è¯·æ±‚ {request.request_id} ä»ç­‰å¾…é˜Ÿåˆ—ç§»å‡º")
        return request
    
    def prepend_request(self, request: Request) -> None:
        """å°†è¯·æ±‚æ’å…¥é˜Ÿåˆ—å¤´éƒ¨
        
        ä»€ä¹ˆæ—¶å€™ç”¨åˆ°ï¼Ÿ
        - æŠ¢å åçš„è¯·æ±‚æ¢å¤
        - é«˜ä¼˜å…ˆçº§è¯·æ±‚æ’é˜Ÿï¼ˆM3+ï¼‰
        """
        self.appendleft(request)
        print(f"âš¡ è¯·æ±‚ {request.request_id} æ’å…¥é˜Ÿåˆ—å¤´éƒ¨ï¼ˆæŠ¢å æ¢å¤ï¼‰")
```

**æ¼”ç¤ºé˜Ÿåˆ—æ“ä½œ**:
```python
def demo_queue_operations():
    """æ¼”ç¤ºé˜Ÿåˆ—çš„åŸºæœ¬æ“ä½œ"""
    
    queue = FCFSRequestQueue()
    
    # æ·»åŠ è¯·æ±‚
    req1 = Request("req-1", "Hello", [1,2,3], SamplingParams())
    req2 = Request("req-2", "Hi", [4,5], SamplingParams())
    
    queue.add_request(req1)  # ğŸ“¥ è¯·æ±‚ req-1 åŠ å…¥ç­‰å¾…é˜Ÿåˆ—
    queue.add_request(req2)  # ğŸ“¥ è¯·æ±‚ req-2 åŠ å…¥ç­‰å¾…é˜Ÿåˆ—
    
    # å¼¹å‡ºè¯·æ±‚
    first = queue.pop_request()  # ğŸ“¤ è¯·æ±‚ req-1 ä»ç­‰å¾…é˜Ÿåˆ—ç§»å‡º
    
    print(f"é˜Ÿåˆ—é•¿åº¦: {len(queue)}")  # 1
    print(f"ä¸‹ä¸€ä¸ªè¯·æ±‚: {queue.peek_request().request_id}")  # req-2
```

### 2.2 è°ƒåº¦å™¨è¾“å‡º - å®šä¹‰è°ƒåº¦å†³ç­–çš„æ•°æ®ç»“æ„

#### NewRequestData ç±»

```python
@dataclass
class NewRequestData:
    """æ–°è¯·æ±‚çš„è°ƒåº¦æ•°æ®
    
    ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªç±»ï¼Ÿ
    - é¦–æ¬¡è°ƒåº¦çš„è¯·æ±‚éœ€è¦å‘é€å®Œæ•´ä¿¡æ¯ç»™ worker
    - worker ä¼šç¼“å­˜è¿™äº›ä¿¡æ¯ï¼Œé¿å…é‡å¤ä¼ è¾“
    """
    req_id: str                    # è¯·æ±‚å”¯ä¸€æ ‡è¯†
    prompt_token_ids: List[int]    # å®Œæ•´çš„ prompt tokens
    sampling_params: SamplingParams # é‡‡æ ·å‚æ•°
    num_computed_tokens: int = 0   # å·²è®¡ç®—çš„ token æ•°ï¼ˆæ–°è¯·æ±‚ä¸º0ï¼‰
    
    def __repr__(self):
        return f"NewReq({self.req_id}, tokens={len(self.prompt_token_ids)})"
```

#### CachedRequestData ç±»

```python
@dataclass
class CachedRequestData:
    """ç¼“å­˜è¯·æ±‚çš„è°ƒåº¦æ•°æ®
    
    ä¸ºä»€ä¹ˆåªå‘é€å¢é‡ä¿¡æ¯ï¼Ÿ
    - worker å·²ç»ç¼“å­˜äº†è¯·æ±‚çš„åŸºæœ¬ä¿¡æ¯
    - åªéœ€è¦å‘é€æ–°ç”Ÿæˆçš„ token
    - å¤§å¹…å‡å°‘é€šä¿¡å¼€é”€
    """
    req_ids: List[str]              # è¯·æ±‚ ID åˆ—è¡¨
    new_token_ids: List[List[int]]  # æ¯ä¸ªè¯·æ±‚æ–°ç”Ÿæˆçš„ tokens
    num_computed_tokens: List[int]  # æ¯ä¸ªè¯·æ±‚å·²è®¡ç®—çš„æ€» tokens
    num_output_tokens: List[int]    # æ¯ä¸ªè¯·æ±‚å·²è¾“å‡ºçš„ tokens
    
    @classmethod
    def make_empty(cls):
        """åˆ›å»ºç©ºçš„ç¼“å­˜æ•°æ®ï¼ˆå½“æ²¡æœ‰ç»§ç»­è¯·æ±‚æ—¶ï¼‰"""
        return cls([], [], [], [])
```

#### SchedulerOutput ç±»

```python
@dataclass
class SchedulerOutput:
    """è°ƒåº¦å™¨çš„å®Œæ•´è¾“å‡º
    
    è¿™æ˜¯è°ƒåº¦å™¨å’Œæ‰§è¡Œå™¨ä¹‹é—´çš„æ¥å£ï¼š
    - å‘Šè¯‰æ‰§è¡Œå™¨è¦å¤„ç†å“ªäº›è¯·æ±‚
    - æ¯ä¸ªè¯·æ±‚è¦å¤„ç†å¤šå°‘ tokens
    - å“ªäº›è¯·æ±‚å·²ç»å®Œæˆ
    """
    scheduled_new_reqs: List[NewRequestData]     # æ–°è°ƒåº¦çš„è¯·æ±‚
    scheduled_cached_reqs: CachedRequestData     # ç»§ç»­å¤„ç†çš„è¯·æ±‚
    num_scheduled_tokens: Dict[str, int]         # æ¯ä¸ªè¯·æ±‚çš„ token æ•°
    total_num_scheduled_tokens: int              # æ€» token æ•°
    finished_req_ids: Set[str]                   # å·²å®Œæˆçš„è¯·æ±‚ ID
    
    @property
    def is_empty(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ²¡æœ‰ä»»ä½•è¯·æ±‚è¢«è°ƒåº¦"""
        return self.total_num_reqs == 0
    
    def __repr__(self):
        return (f"SchedulerOutput(new={len(self.scheduled_new_reqs)}, "
                f"cached={self.scheduled_cached_reqs.num_reqs}, "
                f"tokens={self.total_num_scheduled_tokens})")
```

### 2.3 è°ƒåº¦å™¨ - è¿ç»­æ‰¹å¤„ç†çš„å¤§è„‘

#### Scheduler ç±»çš„åˆå§‹åŒ–

```python
class Scheduler(SchedulerInterface):
    """è¿ç»­æ‰¹å¤„ç†è°ƒåº¦å™¨
    
    è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿçš„å¤§è„‘ï¼Œè´Ÿè´£ï¼š
    1. ç®¡ç†è¯·æ±‚é˜Ÿåˆ—
    2. åšå‡ºè°ƒåº¦å†³ç­–
    3. ç®¡ç†èµ„æºé¢„ç®—
    """
    
    def __init__(self, model_config: ModelConfig, scheduler_config: SchedulerConfig):
        """åˆå§‹åŒ–è°ƒåº¦å™¨
        
        ä¸ºä»€ä¹ˆéœ€è¦è¿™äº›é…ç½®ï¼Ÿ
        - model_config: äº†è§£æ¨¡å‹çš„é™åˆ¶ï¼ˆæœ€å¤§é•¿åº¦ç­‰ï¼‰
        - scheduler_config: è°ƒåº¦çš„çº¦æŸæ¡ä»¶
        """
        # è°ƒåº¦çº¦æŸ - è¿™æ˜¯æ ¸å¿ƒï¼
        self.max_num_seqs = scheduler_config.max_num_seqs           # æœ€å¤§å¹¶å‘æ•°
        self.max_num_batched_tokens = scheduler_config.max_num_batched_tokens  # token é¢„ç®—
        
        # è¯·æ±‚å­˜å‚¨
        self.requests: Dict[str, Request] = {}  # æ‰€æœ‰è¯·æ±‚çš„å­—å…¸
        
        # é˜Ÿåˆ—ç®¡ç†
        self.waiting: RequestQueue = create_request_queue(SchedulingPolicy.FCFS)
        self.running: List[Request] = []  # æ­£åœ¨è¿è¡Œçš„è¯·æ±‚
        
        # å®Œæˆè¯·æ±‚è¿½è¸ª
        self.finished_req_ids: Set[str] = set()  # éœ€è¦é€šçŸ¥ worker æ¸…ç†çš„è¯·æ±‚
        
        print(f"ğŸ§  è°ƒåº¦å™¨åˆå§‹åŒ–: max_seqs={self.max_num_seqs}, "
              f"max_tokens={self.max_num_batched_tokens}")
```

#### æ ¸å¿ƒè°ƒåº¦æ–¹æ³•

```python
def schedule(self) -> SchedulerOutput:
    """æ ¸å¿ƒè°ƒåº¦æ–¹æ³• - è¿™æ˜¯è¿ç»­æ‰¹å¤„ç†çš„å¿ƒè„ï¼
    
    è°ƒåº¦ç­–ç•¥ï¼š
    1. ä¼˜å…ˆæ¥çº³æ–°è¯·æ±‚ï¼ˆprefillï¼‰
    2. ç„¶åè°ƒåº¦è¿è¡Œä¸­è¯·æ±‚ï¼ˆdecodeï¼‰
    3. ä¸¥æ ¼éµå®ˆ token é¢„ç®—
    """
    print(f"\nğŸ”„ å¼€å§‹è°ƒåº¦ - ç­‰å¾…: {len(self.waiting)}, è¿è¡Œ: {len(self.running)}")
    
    # åˆå§‹åŒ–è°ƒåº¦ç»“æœ
    scheduled_new_reqs = []
    scheduled_cached_req_ids = []
    scheduled_cached_tokens = []
    scheduled_cached_computed = []
    scheduled_cached_output = []
    
    num_scheduled_tokens = {}
    total_tokens = 0
    
    # é˜¶æ®µ1: æ¥çº³æ–°è¯·æ±‚ï¼ˆPrefillï¼‰
    print("ğŸ“‹ é˜¶æ®µ1: æ¥çº³æ–°è¯·æ±‚")
    while (self.waiting and 
           len(self.running) < self.max_num_seqs):
        
        request = self.waiting.peek_request()
        seq = request.get_seqs()[0]  # M2 åªæ”¯æŒ n=1
        prompt_len = seq.get_prompt_len()
        
        # æ£€æŸ¥ token é¢„ç®—
        if total_tokens + prompt_len > self.max_num_batched_tokens:
            print(f"âŒ è¯·æ±‚ {request.request_id} è¶…å‡ºé¢„ç®— "
                  f"({total_tokens} + {prompt_len} > {self.max_num_batched_tokens})")
            break
        
        # æ¥çº³è¯·æ±‚
        request = self.waiting.pop_request()
        request.status = RequestStatus.RUNNING
        seq.status = SequenceStatus.RUNNING
        self.running.append(request)
        
        # è°ƒåº¦æ•´ä¸ª promptï¼ˆprefillï¼‰
        new_req_data = NewRequestData(
            req_id=request.request_id,
            prompt_token_ids=seq.data.prompt_token_ids,
            sampling_params=request.sampling_params,
            num_computed_tokens=0,
        )
        scheduled_new_reqs.append(new_req_data)
        num_scheduled_tokens[request.request_id] = prompt_len
        total_tokens += prompt_len
        
        print(f"âœ… æ¥çº³è¯·æ±‚ {request.request_id} (prefill {prompt_len} tokens)")
    
    # é˜¶æ®µ2: è°ƒåº¦è¿è¡Œä¸­è¯·æ±‚ï¼ˆDecodeï¼‰
    print("ğŸ”„ é˜¶æ®µ2: è°ƒåº¦è¿è¡Œä¸­è¯·æ±‚")
    for request in self.running:
        # è·³è¿‡åˆšåˆšæ¥çº³çš„è¯·æ±‚
        if request.request_id in [r.req_id for r in scheduled_new_reqs]:
            continue
        
        # æ£€æŸ¥ token é¢„ç®—
        if total_tokens + 1 > self.max_num_batched_tokens:
            print(f"âŒ è¯·æ±‚ {request.request_id} decode è¶…å‡ºé¢„ç®—")
            continue
        
        seq = request.get_seqs()[0]
        
        # è·å–æœ€åä¸€ä¸ª tokenï¼ˆç”¨äº decodeï¼‰
        if seq.get_output_len() > 0:
            last_token = seq.data.output_token_ids[-1]
        else:
            # ç¬¬ä¸€æ¬¡ decodeï¼ˆprefill åï¼‰
            last_token = seq.data.prompt_token_ids[-1]
        
        # è°ƒåº¦ 1 ä¸ª tokenï¼ˆdecodeï¼‰
        scheduled_cached_req_ids.append(request.request_id)
        scheduled_cached_tokens.append([last_token])
        scheduled_cached_computed.append(seq.get_len())
        scheduled_cached_output.append(seq.get_output_len())
        
        num_scheduled_tokens[request.request_id] = 1
        total_tokens += 1
        
        print(f"âœ… è°ƒåº¦è¯·æ±‚ {request.request_id} (decode 1 token)")
    
    # æ„å»ºè°ƒåº¦è¾“å‡º
    cached_req_data = CachedRequestData(
        req_ids=scheduled_cached_req_ids,
        new_token_ids=scheduled_cached_tokens,
        num_computed_tokens=scheduled_cached_computed,
        num_output_tokens=scheduled_cached_output,
    )
    
    scheduler_output = SchedulerOutput(
        scheduled_new_reqs=scheduled_new_reqs,
        scheduled_cached_reqs=cached_req_data,
        num_scheduled_tokens=num_scheduled_tokens,
        total_num_scheduled_tokens=total_tokens,
        finished_req_ids=self.finished_req_ids.copy(),
    )
    
    # æ¸…ç©ºå·²å®Œæˆè¯·æ±‚ï¼ˆå·²ç»é€šçŸ¥äº†ï¼‰
    self.finished_req_ids.clear()
    
    print(f"ğŸ“Š è°ƒåº¦å®Œæˆ: {scheduler_output}")
    return scheduler_output
```

**æ¼”ç¤ºè°ƒåº¦è¿‡ç¨‹**:
```python
def demo_scheduling_process():
    """æ¼”ç¤ºå®Œæ•´çš„è°ƒåº¦è¿‡ç¨‹"""
    
    # åˆ›å»ºè°ƒåº¦å™¨
    scheduler = Scheduler(model_config, scheduler_config)
    
    # æ·»åŠ ä¸€äº›è¯·æ±‚
    requests = [
        Request("req-1", "Hello", [1,2,3], SamplingParams()),
        Request("req-2", "Hi there", [4,5,6,7], SamplingParams()),
        Request("req-3", "How are you?", [8,9,10,11,12], SamplingParams()),
    ]
    
    for req in requests:
        scheduler.add_request(req)
    
    # ç¬¬ä¸€æ¬¡è°ƒåº¦
    print("=== ç¬¬ä¸€æ¬¡è°ƒåº¦ ===")
    output1 = scheduler.schedule()
    # è¾“å‡º: æ¥çº³æ‰€æœ‰è¯·æ±‚è¿›è¡Œ prefill
    
    # æ¨¡æ‹Ÿæ‰§è¡Œç»“æœ
    sampled_tokens = {"req-1": 100, "req-2": 101, "req-3": 102}
    
    # æ›´æ–°è°ƒåº¦å™¨
    outputs = scheduler.update_from_output(output1, sampled_tokens)
    
    # ç¬¬äºŒæ¬¡è°ƒåº¦
    print("\n=== ç¬¬äºŒæ¬¡è°ƒåº¦ ===")
    output2 = scheduler.schedule()
    # è¾“å‡º: æ‰€æœ‰è¯·æ±‚è¿›è¡Œ decode
```

---

## ğŸ”§ ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ‰¹å¤„ç†æ‰§è¡Œ

### 3.1 è¾“å…¥æ‰¹æ¬¡å‡†å¤‡

#### InputBatch ç±»

```python
@dataclass
class InputBatch:
    """æ‰¹å¤„ç†è¾“å…¥æ•°æ®
    
    ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªç±»ï¼Ÿ
    - å¤„ç†ä¸å®šé•¿åºåˆ—
    - ç»Ÿä¸€æ‰¹å¤„ç†æ¥å£
    - æ”¯æŒæ··åˆ prefill/decode
    """
    req_ids: List[str]          # è¯·æ±‚ ID åˆ—è¡¨
    token_ids: List[List[int]]  # ä¸å®šé•¿çš„ token åºåˆ—
    start_positions: List[int]  # æ¯ä¸ªåºåˆ—çš„èµ·å§‹ä½ç½®
    is_prefill: List[bool]      # æ˜¯å¦ä¸º prefill é˜¶æ®µ
    prompt_lens: List[int]      # prefill è¯·æ±‚çš„ prompt é•¿åº¦
    
    def to_tensors(self, device: torch.device, pad_token_id: int = 0):
        """è½¬æ¢ä¸ºå¡«å……åçš„å¼ é‡
        
        è¿™æ˜¯æ‰¹å¤„ç†çš„å…³é”®æ­¥éª¤ï¼š
        1. æ‰¾åˆ°æœ€å¤§é•¿åº¦
        2. å¡«å……çŸ­åºåˆ—
        3. åˆ›å»º attention mask
        """
        batch_size = len(self.token_ids)
        max_len = max(len(tokens) for tokens in self.token_ids)
        
        print(f"ğŸ”„ æ‰¹æ¬¡è½¬æ¢: batch_size={batch_size}, max_len={max_len}")
        
        # åˆå§‹åŒ–å¡«å……åçš„å¼ é‡
        padded_token_ids = torch.full(
            (batch_size, max_len), pad_token_id, 
            dtype=torch.long, device=device
        )
        attention_mask = torch.zeros(
            (batch_size, max_len), 
            dtype=torch.long, device=device
        )
        positions = torch.zeros(
            (batch_size, max_len), 
            dtype=torch.long, device=device
        )
        
        # å¡«å……å®é™…æ•°æ®
        for i, tokens in enumerate(self.token_ids):
            seq_len = len(tokens)
            
            # å¡«å…… token IDs
            padded_token_ids[i, :seq_len] = torch.tensor(tokens, device=device)
            
            # åˆ›å»º attention maskï¼ˆ1è¡¨ç¤ºæœ‰æ•ˆï¼Œ0è¡¨ç¤ºå¡«å……ï¼‰
            attention_mask[i, :seq_len] = 1
            
            # åˆ›å»ºä½ç½®ç´¢å¼•
            start_pos = self.start_positions[i]
            positions[i, :seq_len] = torch.arange(
                start_pos, start_pos + seq_len, device=device
            )
            
            print(f"  è¯·æ±‚ {self.req_ids[i]}: {seq_len} tokens, "
                  f"start_pos={start_pos}, prefill={self.is_prefill[i]}")
        
        return padded_token_ids, attention_mask, positions
```

#### æ‰¹æ¬¡å‡†å¤‡å‡½æ•°

```python
def prepare_inputs_from_scheduler_output(
    scheduler_output: SchedulerOutput
) -> InputBatch:
    """ä»è°ƒåº¦å™¨è¾“å‡ºå‡†å¤‡æ‰¹å¤„ç†è¾“å…¥
    
    è¿™ä¸ªå‡½æ•°è¿æ¥äº†è°ƒåº¦å†³ç­–å’Œæ‰§è¡Œï¼š
    - å°†è°ƒåº¦å™¨çš„æŠ½è±¡å†³ç­–è½¬æ¢ä¸ºå…·ä½“çš„æ‰§è¡Œè¾“å…¥
    - å¤„ç† prefill å’Œ decode çš„ä¸åŒéœ€æ±‚
    """
    print(f"ğŸ“¦ å‡†å¤‡æ‰¹å¤„ç†è¾“å…¥: {scheduler_output}")
    
    req_ids = []
    token_ids = []
    start_positions = []
    is_prefill = []
    prompt_lens = []
    
    # å¤„ç†æ–°è¯·æ±‚ï¼ˆprefillï¼‰
    for new_req_data in scheduler_output.scheduled_new_reqs:
        req_ids.append(new_req_data.req_id)
        token_ids.append(new_req_data.prompt_token_ids)  # å®Œæ•´ prompt
        start_positions.append(0)  # ä»ä½ç½® 0 å¼€å§‹
        is_prefill.append(True)
        prompt_lens.append(len(new_req_data.prompt_token_ids))
        
        print(f"  æ–°è¯·æ±‚ {new_req_data.req_id}: prefill {len(new_req_data.prompt_token_ids)} tokens")
    
    # å¤„ç†ç¼“å­˜è¯·æ±‚ï¼ˆdecodeï¼‰
    cached_reqs = scheduler_output.scheduled_cached_reqs
    for i, req_id in enumerate(cached_reqs.req_ids):
        req_ids.append(req_id)
        token_ids.append(cached_reqs.new_token_ids[i])  # åªæœ‰æœ€åä¸€ä¸ª token
        start_positions.append(cached_reqs.num_computed_tokens[i])  # ç»§ç»­ä½ç½®
        is_prefill.append(False)
        prompt_lens.append(0)  # decode ä¸éœ€è¦
        
        print(f"  ç¼“å­˜è¯·æ±‚ {req_id}: decode 1 token at pos {cached_reqs.num_computed_tokens[i]}")
    
    return InputBatch(
        req_ids=req_ids,
        token_ids=token_ids,
        start_positions=start_positions,
        is_prefill=is_prefill,
        prompt_lens=prompt_lens,
    )
```

### 3.2 æ¨¡å‹è¿è¡Œå™¨æ‰¹å¤„ç†

#### ModelRunner çš„æ‰¹å¤„ç†æ–¹æ³•

```python
def execute_model_batch(self, input_batch: InputBatch) -> Dict[str, torch.Tensor]:
    """æ‰¹å¤„ç†æ‰§è¡Œæ¨¡å‹
    
    M2 çš„é™åˆ¶ï¼šæ¯ä¸ªè¯·æ±‚å•ç‹¬æ‰§è¡Œ
    - åŸå› ï¼šHuggingFace æ¨¡å‹çš„ KV cache éš¾ä»¥çœŸæ­£æ‰¹å¤„ç†
    - M3+ å°†ä½¿ç”¨ PagedAttention å®ç°çœŸæ­£çš„æ‰¹å¤„ç†
    """
    print(f"ğŸš€ æ‰§è¡Œæ¨¡å‹æ‰¹å¤„ç†: {input_batch.batch_size} ä¸ªè¯·æ±‚")
    
    if input_batch.batch_size == 0:
        return {}
    
    # è½¬æ¢ä¸ºå¡«å……å¼ é‡
    token_ids, attention_mask, positions = input_batch.to_tensors(self.device)
    
    results = {}
    
    # M2: ä¸ºæ¯ä¸ªè¯·æ±‚å•ç‹¬æ‰§è¡Œ
    for i, req_id in enumerate(input_batch.req_ids):
        print(f"  å¤„ç†è¯·æ±‚ {req_id}")
        
        # æå–è¯¥è¯·æ±‚çš„è¾“å…¥
        req_token_ids = token_ids[i:i+1]  # [1, seq_len]
        req_positions = positions[i:i+1]
        req_attention_mask = attention_mask[i:i+1]
        
        # ç§»é™¤å¡«å……
        seq_len = req_attention_mask[0].sum().item()
        req_token_ids = req_token_ids[:, :seq_len]
        req_positions = req_positions[:, :seq_len]
        
        # è·å–è¯¥è¯·æ±‚çš„ KV cache
        past_key_values = self.request_caches.get(req_id)
        
        try:
            # æ‰§è¡Œæ¨¡å‹å‰å‘ä¼ æ’­
            outputs = self.model(
                input_ids=req_token_ids,
                position_ids=req_positions,
                past_key_values=past_key_values,
                use_cache=True,
                return_dict=True,
            )
            
            # æ›´æ–°è¯¥è¯·æ±‚çš„ KV cache
            self.request_caches[req_id] = outputs.past_key_values
            
            # æå–ä¸‹ä¸€ä¸ª token çš„ logits
            next_token_logits = outputs.logits[0, -1, :]  # [vocab_size]
            results[req_id] = next_token_logits
            
            print(f"    âœ… æˆåŠŸå¤„ç†ï¼Œlogits shape: {next_token_logits.shape}")
            
        except Exception as e:
            print(f"    âŒ å¤„ç†å¤±è´¥: {e}")
            # å¯ä»¥æ·»åŠ é™çº§å¤„ç†
    
    print(f"ğŸ“Š æ‰¹å¤„ç†å®Œæˆï¼Œå¤„ç†äº† {len(results)} ä¸ªè¯·æ±‚")
    return results

def free_request_cache(self, req_id: str):
    """é‡Šæ”¾è¯·æ±‚çš„ KV cache
    
    ä¸ºä»€ä¹ˆéœ€è¦æ˜¾å¼é‡Šæ”¾ï¼Ÿ
    - é˜²æ­¢å†…å­˜æ³„æ¼
    - åŠæ—¶å›æ”¶èµ„æº
    - ä¸ºæ–°è¯·æ±‚è…¾å‡ºç©ºé—´
    """
    if req_id in self.request_caches:
        del self.request_caches[req_id]
        print(f"ğŸ—‘ï¸ é‡Šæ”¾è¯·æ±‚ {req_id} çš„ KV cache")
```

---

## ğŸ”„ ç¬¬å››éƒ¨åˆ†ï¼šç±»/å‡½æ•°è°ƒç”¨æµç¨‹è¯¦è§£

### 4.1 å®Œæ•´çš„å‡½æ•°è°ƒç”¨é“¾è·¯å›¾

```mermaid
graph TD
    A[ç”¨æˆ·è°ƒç”¨ engine.generate_batch] --> B[LLMEngine.generate_batch]
    
    B --> B1[è½¬æ¢ prompts ä¸º Request å¯¹è±¡]
    B1 --> B2[processor.process_request å¾ªç¯]
    B2 --> B3[engine_core.add_request å¾ªç¯]
    
    B3 --> C[ä¸»å¾ªç¯: while æœ‰æœªå®Œæˆè¯·æ±‚]
    C --> C1[engine_core.step]
    
    %% EngineCore.step è¯¦ç»†æµç¨‹
    C1 --> D1[1. scheduler.schedule]
    D1 --> D2[2. prepare_inputs_from_scheduler_output]
    D2 --> D3[3. executor.execute_model_batch]
    D3 --> D4[4. sampler.sample å¾ªç¯]
    D4 --> D5[5. scheduler.update_from_output]
    D5 --> D6[6. processor.decode_tokens]
    D6 --> D7[7. executor.free_request_cache]
    
    D7 --> C2{æ‰€æœ‰è¯·æ±‚å®Œæˆ?}
    C2 -->|å¦| C1
    C2 -->|æ˜¯| E[è¿”å›æ‰€æœ‰è¾“å‡º]
    
    %% è°ƒåº¦å™¨è¯¦ç»†æµç¨‹
    D1 --> S1[Scheduler.schedule è¯¦ç»†æµç¨‹]
    S1 --> S2[æ£€æŸ¥ waiting é˜Ÿåˆ—]
    S2 --> S3[åº”ç”¨ FCFS + Tokené¢„ç®—ç­–ç•¥]
    S3 --> S4[æ¥çº³æ–°è¯·æ±‚åˆ° running]
    S4 --> S5[ä¸º running è¯·æ±‚è°ƒåº¦ decode]
    S5 --> S6[æ„å»º SchedulerOutput]
    
    %% æ‰§è¡Œå™¨è¯¦ç»†æµç¨‹
    D3 --> M1[GPUExecutor.execute_model_batch]
    M1 --> M2[GPUWorker.execute_model_batch]
    M2 --> M3[ModelRunner.execute_model_batch]
    M3 --> M4[input_batch.to_tensors]
    M4 --> M5[ä¸ºæ¯ä¸ªè¯·æ±‚å•ç‹¬æ‰§è¡Œå¾ªç¯]
    M5 --> M6[è·å– request_caches[req_id]]
    M6 --> M7[model.forward]
    M7 --> M8[æ›´æ–° request_caches[req_id]]
    
    style A fill:#e1f5fe
    style C1 fill:#f3e5f5
    style D1 fill:#e8f5e8
    style D3 fill:#fce4ec
    style S1 fill:#e8f5e8
    style M1 fill:#fce4ec
```

### 4.2 å…³é”®å‡½æ•°çš„è¾“å…¥è¾“å‡ºè¯¦è§£

#### LLMEngine.generate_batch() å‡½æ•°æµç¨‹

```python
def generate_batch_flow_demo():
    """å±•ç¤º generate_batch çš„å®Œæ•´è°ƒç”¨æµç¨‹"""
    
    print("ğŸš€ === LLMEngine.generate_batch å¼€å§‹ ===")
    
    # è¾“å…¥: List[str] prompts
    prompts = [
        "What is AI?",
        "Explain quantum computing.",
        "Write a haiku about coding."
    ]
    
    # æ­¥éª¤1: è½¬æ¢ä¸º Request å¯¹è±¡
    print("ğŸ“ æ­¥éª¤1: åˆ›å»º Request å¯¹è±¡")
    requests = []
    for i, prompt in enumerate(prompts):
        # è°ƒç”¨: processor.process_request()
        request = Request(
            request_id=f"req-{i}",
            prompt=prompt,
            prompt_token_ids=tokenize(prompt),  # [15, 284, 318, 9552, 30]
            sampling_params=sampling_params
        )
        requests.append(request)
        print(f"  åˆ›å»º {request.request_id}: {len(request.prompt_token_ids)} tokens")
    
    # æ­¥éª¤2: æ·»åŠ åˆ°å¼•æ“æ ¸å¿ƒ
    print("ğŸ“¥ æ­¥éª¤2: æ·»åŠ è¯·æ±‚åˆ° EngineCore")
    for request in requests:
        # è°ƒç”¨: engine_core.add_request()
        # â””â”€â”€ scheduler.add_request()
        #     â””â”€â”€ waiting_queue.add_request()
        engine_core.add_request(request)
    
    # æ­¥éª¤3: ä¸»å¾ªç¯
    print("ğŸ”„ æ­¥éª¤3: å¼€å§‹ä¸»å¾ªç¯")
    all_outputs = {}
    finished_count = 0
    iteration = 0
    
    while finished_count < len(requests):
        iteration += 1
        print(f"\n--- è¿­ä»£ {iteration} ---")
        
        # è°ƒç”¨: engine_core.step()
        step_outputs = engine_core.step()
        
        # æ”¶é›†è¾“å‡º
        for req_id, output in step_outputs.items():
            all_outputs[req_id] = output
            if output.finished:
                finished_count += 1
                print(f"âœ… {req_id} å®Œæˆ")
    
    print(f"ğŸ‰ æ‰¹å¤„ç†å®Œæˆï¼Œå…± {iteration} æ¬¡è¿­ä»£")
    return all_outputs
```

#### EngineCore.step() å‡½æ•°æµç¨‹

```python
def engine_core_step_flow():
    """å±•ç¤º EngineCore.step çš„è¯¦ç»†æµç¨‹"""
    
    print("ğŸ›ï¸ === EngineCore.step å¼€å§‹ ===")
    
    # æ­¥éª¤1: è°ƒåº¦å†³ç­–
    print("1ï¸âƒ£ è°ƒåº¦å†³ç­–: scheduler.schedule()")
    scheduler_output = scheduler.schedule()
    """
    SchedulerOutput(
        scheduled_new_reqs=[
            NewRequestData(req_id="req-0", prompt_token_ids=[15,284,318,9552,30], ...)
        ],
        scheduled_cached_reqs=CachedRequestData(
            req_ids=["req-1", "req-2"],
            new_token_ids=[[42], [17]], ...
        ),
        total_num_scheduled_tokens=37
    )
    """
    
    # æ­¥éª¤2: å‡†å¤‡æ‰¹å¤„ç†è¾“å…¥
    print("2ï¸âƒ£ å‡†å¤‡è¾“å…¥: prepare_inputs_from_scheduler_output()")
    input_batch = prepare_inputs_from_scheduler_output(scheduler_output)
    """
    InputBatch(
        req_ids=["req-0", "req-1", "req-2"],
        token_ids=[[15,284,318,9552,30], [42], [17]],
        start_positions=[0, 25, 18],
        is_prefill=[True, False, False]
    )
    """
    
    # æ­¥éª¤3: æ‰§è¡Œæ¨¡å‹
    print("3ï¸âƒ£ æ‰§è¡Œæ¨¡å‹: executor.execute_model_batch()")
    logits_dict = executor.execute_model_batch(input_batch)
    """
    {
        "req-0": tensor([0.1, 0.3, 0.6, ...]),  # [vocab_size]
        "req-1": tensor([0.2, 0.4, 0.4, ...]),
        "req-2": tensor([0.5, 0.2, 0.3, ...])
    }
    """
    
    # æ­¥éª¤4: é‡‡æ ·ç”Ÿæˆ
    print("4ï¸âƒ£ é‡‡æ ·ç”Ÿæˆ: sampler.sample()")
    sampled_tokens = {}
    for req_id, logits in logits_dict.items():
        request = scheduler.requests[req_id]
        next_tokens, _ = sampler.sample(
            logits.unsqueeze(0),  # [1, vocab_size]
            request.sampling_params
        )
        sampled_tokens[req_id] = next_tokens[0].item()
    """
    {
        "req-0": 464,  # "Paris"
        "req-1": 318,  # "is"
        "req-2": 257   # "a"
    }
    """
    
    # æ­¥éª¤5: æ›´æ–°è°ƒåº¦å™¨
    print("5ï¸âƒ£ æ›´æ–°çŠ¶æ€: scheduler.update_from_output()")
    outputs = scheduler.update_from_output(scheduler_output, sampled_tokens)
    """
    {
        "req-0": RequestOutput(req_id="req-0", outputs=[...], finished=False),
        "req-1": RequestOutput(req_id="req-1", outputs=[...], finished=True),
        "req-2": RequestOutput(req_id="req-2", outputs=[...], finished=False)
    }
    """
    
    # æ­¥éª¤6: è§£ç æ–‡æœ¬
    print("6ï¸âƒ£ è§£ç æ–‡æœ¬: processor.decode_tokens()")
    for req_id, output in outputs.items():
        for completion in output.outputs:
            completion.text = processor.decode_tokens(completion.token_ids)
    
    # æ­¥éª¤7: æ¸…ç†èµ„æº
    print("7ï¸âƒ£ æ¸…ç†èµ„æº: executor.free_request_cache()")
    for req_id in scheduler_output.finished_req_ids:
        executor.free_request_cache(req_id)
    
    return outputs
```

#### Scheduler.schedule() å‡½æ•°æµç¨‹

```python
def scheduler_schedule_flow():
    """å±•ç¤º Scheduler.schedule çš„è¯¦ç»†è°ƒåº¦é€»è¾‘"""
    
    print("ğŸ§  === Scheduler.schedule å¼€å§‹ ===")
    
    # å½“å‰çŠ¶æ€
    print(f"å½“å‰çŠ¶æ€: waiting={len(waiting)}, running={len(running)}")
    
    scheduled_new_reqs = []
    scheduled_cached_reqs = CachedRequestData.make_empty()
    total_tokens = 0
    
    # é˜¶æ®µ1: æ¥çº³æ–°è¯·æ±‚ (Prefill)
    print("ğŸ“‹ é˜¶æ®µ1: æ¥çº³æ–°è¯·æ±‚ (Prefill)")
    while (waiting and 
           len(running) < max_num_seqs and 
           total_tokens < max_num_batched_tokens):
        
        request = waiting.peek_request()
        prompt_len = len(request.prompt_token_ids)
        
        # Token é¢„ç®—æ£€æŸ¥
        if total_tokens + prompt_len > max_num_batched_tokens:
            print(f"âŒ {request.request_id} è¶…å‡ºé¢„ç®—: {total_tokens}+{prompt_len} > {max_num_batched_tokens}")
            break
        
        # æ¥çº³è¯·æ±‚
        request = waiting.pop_request()  # O(1) æ“ä½œ
        running.append(request)
        request.status = RequestStatus.RUNNING
        
        # è°ƒåº¦æ•´ä¸ª prompt
        new_req_data = NewRequestData(
            req_id=request.request_id,
            prompt_token_ids=request.prompt_token_ids,
            sampling_params=request.sampling_params,
            num_computed_tokens=0
        )
        scheduled_new_reqs.append(new_req_data)
        total_tokens += prompt_len
        
        print(f"âœ… æ¥çº³ {request.request_id}: prefill {prompt_len} tokens")
    
    # é˜¶æ®µ2: è°ƒåº¦è¿è¡Œä¸­è¯·æ±‚ (Decode)
    print("ğŸ”„ é˜¶æ®µ2: è°ƒåº¦è¿è¡Œä¸­è¯·æ±‚ (Decode)")
    cached_req_ids = []
    cached_tokens = []
    cached_computed = []
    cached_output = []
    
    for request in running:
        # è·³è¿‡åˆšæ¥çº³çš„è¯·æ±‚
        if request.request_id in [r.req_id for r in scheduled_new_reqs]:
            continue
        
        # Token é¢„ç®—æ£€æŸ¥
        if total_tokens + 1 > max_num_batched_tokens:
            print(f"âŒ {request.request_id} decode è¶…å‡ºé¢„ç®—")
            continue
        
        seq = request.get_seqs()[0]
        
        # è·å–æœ€åä¸€ä¸ª token
        if seq.get_output_len() > 0:
            last_token = seq.data.output_token_ids[-1]
        else:
            last_token = seq.data.prompt_token_ids[-1]
        
        # è°ƒåº¦ 1 ä¸ª token
        cached_req_ids.append(request.request_id)
        cached_tokens.append([last_token])
        cached_computed.append(seq.get_len())
        cached_output.append(seq.get_output_len())
        total_tokens += 1
        
        print(f"âœ… è°ƒåº¦ {request.request_id}: decode 1 token")
    
    # æ„å»ºè¾“å‡º
    scheduled_cached_reqs = CachedRequestData(
        req_ids=cached_req_ids,
        new_token_ids=cached_tokens,
        num_computed_tokens=cached_computed,
        num_output_tokens=cached_output
    )
    
    scheduler_output = SchedulerOutput(
        scheduled_new_reqs=scheduled_new_reqs,
        scheduled_cached_reqs=scheduled_cached_reqs,
        num_scheduled_tokens={},  # ä¼šå¡«å……
        total_num_scheduled_tokens=total_tokens,
        finished_req_ids=finished_req_ids.copy()
    )
    
    print(f"ğŸ“Š è°ƒåº¦å®Œæˆ: {len(scheduled_new_reqs)} æ–°è¯·æ±‚, {len(cached_req_ids)} ç»§ç»­è¯·æ±‚")
    return scheduler_output
```

#### ModelRunner.execute_model_batch() å‡½æ•°æµç¨‹

```python
def model_runner_batch_flow():
    """å±•ç¤º ModelRunner.execute_model_batch çš„æ‰§è¡Œæµç¨‹"""
    
    print("ğŸš€ === ModelRunner.execute_model_batch å¼€å§‹ ===")
    
    # è¾“å…¥: InputBatch
    input_batch = InputBatch(
        req_ids=["req-0", "req-1", "req-2"],
        token_ids=[[15,284,318,9552,30], [42], [17]],
        start_positions=[0, 25, 18],
        is_prefill=[True, False, False]
    )
    
    # æ­¥éª¤1: è½¬æ¢ä¸ºå¡«å……å¼ é‡
    print("1ï¸âƒ£ è½¬æ¢ä¸ºå¡«å……å¼ é‡: input_batch.to_tensors()")
    token_ids, attention_mask, positions = input_batch.to_tensors(device)
    """
    token_ids = [
        [15, 284, 318, 9552, 30],  # req-0: å®Œæ•´ prompt
        [42,   0,   0,    0,  0],  # req-1: 1 token + 4 padding
        [17,   0,   0,    0,  0]   # req-2: 1 token + 4 padding
    ]
    attention_mask = [
        [1, 1, 1, 1, 1],  # req-0: å…¨éƒ¨æœ‰æ•ˆ
        [1, 0, 0, 0, 0],  # req-1: åªæœ‰ç¬¬1ä¸ªæœ‰æ•ˆ
        [1, 0, 0, 0, 0]   # req-2: åªæœ‰ç¬¬1ä¸ªæœ‰æ•ˆ
    ]
    """
    
    # æ­¥éª¤2: ä¸ºæ¯ä¸ªè¯·æ±‚å•ç‹¬æ‰§è¡Œ (M2 é™åˆ¶)
    print("2ï¸âƒ£ ä¸ºæ¯ä¸ªè¯·æ±‚å•ç‹¬æ‰§è¡Œ")
    results = {}
    
    for i, req_id in enumerate(input_batch.req_ids):
        print(f"  å¤„ç†è¯·æ±‚ {req_id}")
        
        # æå–è¯¥è¯·æ±‚çš„è¾“å…¥
        req_token_ids = token_ids[i:i+1]  # [1, seq_len]
        req_attention_mask = attention_mask[i:i+1]
        
        # ç§»é™¤ padding
        seq_len = req_attention_mask[0].sum().item()
        req_token_ids = req_token_ids[:, :seq_len]  # [1, actual_len]
        
        # è·å– KV cache
        past_key_values = request_caches.get(req_id)
        print(f"    KV cache: {'å­˜åœ¨' if past_key_values else 'æ–°å»º'}")
        
        # æ‰§è¡Œæ¨¡å‹
        outputs = model(
            input_ids=req_token_ids,
            past_key_values=past_key_values,
            use_cache=True,
            return_dict=True
        )
        
        # æ›´æ–° KV cache
        request_caches[req_id] = outputs.past_key_values
        
        # æå– logits
        next_token_logits = outputs.logits[0, -1, :]  # [vocab_size]
        results[req_id] = next_token_logits
        
        print(f"    âœ… å®Œæˆï¼Œlogits shape: {next_token_logits.shape}")
    
    print(f"ğŸ“Š æ‰¹å¤„ç†å®Œæˆ: {len(results)} ä¸ªè¯·æ±‚")
    return results
```

### 4.3 æ•°æ®ç»“æ„å˜æ¢çš„è¯¦ç»†è¿‡ç¨‹

```mermaid
graph LR
    subgraph "ç”¨æˆ·è¾“å…¥"
        A1["prompts = <br/>['What is AI?',<br/>'Explain quantum',<br/>'Write haiku']"]
    end

    subgraph "Request åˆ›å»º"
        B1["requests = <br/>[Request(req-0, [15,284,318]),<br/>Request(req-1, [8495,31312]),<br/>Request(req-2, [16594,47413])]"]
    end

    subgraph "è°ƒåº¦å™¨è¾“å‡º"
        C1["SchedulerOutput<br/>new_reqs=[req-0],<br/>cached_reqs=[req-1,req-2],<br/>total_tokens=15"]
    end

    subgraph "æ‰¹å¤„ç†è¾“å…¥"
        D1["InputBatch<br/>req_ids=[req-0,req-1,req-2],<br/>token_ids=[[15,284,318],[42],[17]],<br/>is_prefill=[T,F,F]"]
    end

    subgraph "å¡«å……å¼ é‡"
        E1["Tensors:<br/>token_ids=[3,3] padded<br/>attention_mask=[3,3]<br/>positions=[3,3]"]
    end

    subgraph "æ¨¡å‹è¾“å‡º"
        F1["logits_dict = <br/>{'req-0': tensor([0.1,0.3,0.6]),<br/>'req-1': tensor([0.2,0.4,0.4]),<br/>'req-2': tensor([0.5,0.2,0.3])}"]
    end

    subgraph "é‡‡æ ·ç»“æœ"
        G1["sampled_tokens = <br/>{'req-0': 464,<br/>'req-1': 318,<br/>'req-2': 257}"]
    end

    subgraph "æœ€ç»ˆè¾“å‡º"
        H1["outputs = <br/>{'req-0': RequestOutput(...),<br/>'req-1': RequestOutput(...),<br/>'req-2': RequestOutput(...)}"]
    end

    A1 --> B1
    B1 --> C1
    C1 --> D1
    D1 --> E1
    E1 --> F1
    F1 --> G1
    G1 --> H1

    style A1 fill:#e3f2fd
    style C1 fill:#e8f5e8
    style D1 fill:#fff3e0
    style E1 fill:#fce4ec
    style F1 fill:#f3e5f5
    style H1 fill:#f1f8e9
```

---

## ğŸ›ï¸ ç¬¬äº”éƒ¨åˆ†ï¼šå¼•æ“åè°ƒ

### 5.1 EngineCore - è¿ç»­æ‰¹å¤„ç†çš„æŒ‡æŒ¥ä¸­å¿ƒ

```python
class EngineCore:
    """å¼•æ“æ ¸å¿ƒ - åè°ƒæ‰€æœ‰ç»„ä»¶
    
    è¿™æ˜¯è¿ç»­æ‰¹å¤„ç†çš„æŒ‡æŒ¥ä¸­å¿ƒï¼š
    1. åè°ƒè°ƒåº¦å™¨ã€æ‰§è¡Œå™¨ã€é‡‡æ ·å™¨
    2. å®ç°ä¸»è¦çš„ç”Ÿæˆå¾ªç¯
    3. ç®¡ç†è¯·æ±‚çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸ
    """
    
    def __init__(self, model_config, scheduler_config, executor, sampler, processor):
        """åˆå§‹åŒ–å¼•æ“æ ¸å¿ƒ
        
        ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¹ˆå¤šç»„ä»¶ï¼Ÿ
        - scheduler: å†³ç­–è°ƒåº¦
        - executor: æ‰§è¡Œæ¨¡å‹
        - sampler: ç”Ÿæˆ token
        - processor: å¤„ç†æ–‡æœ¬
        """
        self.scheduler = Scheduler(model_config, scheduler_config)
        self.executor = executor
        self.sampler = sampler
        self.processor = processor
        self.iteration = 0
        
        print("ğŸ›ï¸ EngineCore åˆå§‹åŒ–å®Œæˆ")
    
    def step(self) -> Dict[str, RequestOutput]:
        """æ‰§è¡Œä¸€æ¬¡è¿­ä»£ - è¿ç»­æ‰¹å¤„ç†çš„æ ¸å¿ƒå¾ªç¯ï¼
        
        è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿæœ€é‡è¦çš„æ–¹æ³•ï¼š
        1. è°ƒåº¦å†³ç­–
        2. æ‰§è¡Œæ¨¡å‹
        3. é‡‡æ ·ç”Ÿæˆ
        4. æ›´æ–°çŠ¶æ€
        5. æ¸…ç†èµ„æº
        """
        self.iteration += 1
        print(f"\nğŸ”„ === è¿­ä»£ {self.iteration} å¼€å§‹ ===")
        
        # æ­¥éª¤1: è°ƒåº¦å†³ç­–
        print("1ï¸âƒ£ è°ƒåº¦å†³ç­–")
        scheduler_output = self.scheduler.schedule()
        
        if scheduler_output.is_empty:
            print("   ğŸ“­ æ²¡æœ‰è¯·æ±‚éœ€è¦å¤„ç†")
            return {}
        
        # æ­¥éª¤2: å‡†å¤‡è¾“å…¥
        print("2ï¸âƒ£ å‡†å¤‡æ‰¹å¤„ç†è¾“å…¥")
        input_batch = prepare_inputs_from_scheduler_output(scheduler_output)
        
        # æ­¥éª¤3: æ‰§è¡Œæ¨¡å‹
        print("3ï¸âƒ£ æ‰§è¡Œæ¨¡å‹")
        logits_dict = self.executor.execute_model_batch(input_batch)
        
        # æ­¥éª¤4: é‡‡æ ·ç”Ÿæˆ
        print("4ï¸âƒ£ é‡‡æ ·ç”Ÿæˆ")
        sampled_tokens = {}
        
        for req_id in input_batch.req_ids:
            if req_id not in logits_dict:
                continue
            
            logits = logits_dict[req_id]  # [vocab_size]
            request = self.scheduler.requests[req_id]
            
            # é‡‡æ ·ä¸‹ä¸€ä¸ª token
            logits_batch = logits.unsqueeze(0)  # [1, vocab_size]
            next_tokens, _ = self.sampler.sample(logits_batch, request.sampling_params)
            sampled_tokens[req_id] = next_tokens[0].item()
            
            print(f"   ğŸ² è¯·æ±‚ {req_id} é‡‡æ ·åˆ° token {sampled_tokens[req_id]}")
        
        # æ­¥éª¤5: æ›´æ–°è°ƒåº¦å™¨çŠ¶æ€
        print("5ï¸âƒ£ æ›´æ–°è°ƒåº¦å™¨çŠ¶æ€")
        outputs = self.scheduler.update_from_output(scheduler_output, sampled_tokens)
        
        # æ­¥éª¤6: è§£ç æ–‡æœ¬
        print("6ï¸âƒ£ è§£ç æ–‡æœ¬")
        for req_id, output in outputs.items():
            request = self.scheduler.requests[req_id]
            for completion in output.outputs:
                if completion.token_ids:
                    completion.text = self.processor.decode_tokens(
                        completion.token_ids,
                        skip_special_tokens=request.sampling_params.skip_special_tokens,
                    )
                    print(f"   ğŸ“ è¯·æ±‚ {req_id} æ–‡æœ¬: '{completion.text}'")
        
        # æ­¥éª¤7: æ¸…ç†èµ„æº
        print("7ï¸âƒ£ æ¸…ç†èµ„æº")
        for req_id in scheduler_output.finished_req_ids:
            self.executor.free_request_cache(req_id)
            print(f"   ğŸ—‘ï¸ æ¸…ç†è¯·æ±‚ {req_id}")
        
        print(f"âœ… è¿­ä»£ {self.iteration} å®Œæˆï¼Œè¿”å› {len(outputs)} ä¸ªè¾“å‡º")
        return outputs
```

### 5.2 LLMEngine çš„æ‰¹é‡ç”Ÿæˆ API

```python
def generate_batch(
    self,
    prompts: List[str],
    sampling_params: SamplingParams,
) -> Dict[str, RequestOutput]:
    """æ‰¹é‡ç”Ÿæˆ API - ç”¨æˆ·çš„ä¸»è¦æ¥å£
    
    è¿™æ˜¯ç”¨æˆ·çœ‹åˆ°çš„ç®€å•æ¥å£ï¼ŒèƒŒåæ˜¯å¤æ‚çš„è¿ç»­æ‰¹å¤„ç†ç³»ç»Ÿ
    """
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆ: {len(prompts)} ä¸ª prompts")
    
    # æ­¥éª¤1: è½¬æ¢ä¸º Request å¯¹è±¡
    requests = []
    for i, prompt in enumerate(prompts):
        request = self.processor.process_request(
            prompt, 
            sampling_params,
            request_id=f"req-{self._request_counter}"
        )
        self._request_counter += 1
        requests.append(request)
        print(f"  ğŸ“ åˆ›å»ºè¯·æ±‚ {request.request_id}: '{prompt[:30]}...'")
    
    # æ­¥éª¤2: æ·»åŠ æ‰€æœ‰è¯·æ±‚åˆ°å¼•æ“
    for request in requests:
        self.engine_core.add_request(request)
    
    # æ­¥éª¤3: è¿è¡Œç”Ÿæˆå¾ªç¯
    all_outputs = {}
    finished_request_ids = set()
    target_request_ids = {req.request_id for req in requests}
    
    start_time = time.time()
    iteration = 0
    
    print(f"ğŸ”„ å¼€å§‹ç”Ÿæˆå¾ªç¯ï¼Œç›®æ ‡å®Œæˆ {len(requests)} ä¸ªè¯·æ±‚")
    
    while len(finished_request_ids) < len(requests):
        iteration += 1
        
        # æ‰§è¡Œä¸€æ­¥
        step_outputs = self.engine_core.step()
        
        # æ”¶é›†è¾“å‡º
        for req_id, output in step_outputs.items():
            if req_id in target_request_ids:
                all_outputs[req_id] = output
                if output.finished:
                    finished_request_ids.add(req_id)
                    print(f"  âœ… è¯·æ±‚ {req_id} å®Œæˆ")
        
        # å®‰å…¨æ£€æŸ¥
        if iteration > 10000:
            print("âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œå¼ºåˆ¶é€€å‡º")
            break
    
    # æ­¥éª¤4: ç»Ÿè®¡å’Œè¿”å›
    total_time = time.time() - start_time
    total_tokens = sum(len(output.outputs[0].token_ids) for output in all_outputs.values())
    
    print(f"\nğŸ“Š æ‰¹é‡ç”Ÿæˆå®Œæˆ:")
    print(f"  è¯·æ±‚æ•°: {len(requests)}")
    print(f"  æ€»æ—¶é—´: {total_time:.2f}s")
    print(f"  æ€» tokens: {total_tokens}")
    print(f"  ååé‡: {total_tokens/total_time:.2f} tokens/s")
    print(f"  è¿­ä»£æ¬¡æ•°: {iteration}")
    
    return all_outputs
```

---

## ğŸ“Š ç¬¬å…­éƒ¨åˆ†ï¼šç«¯åˆ°ç«¯ç¤ºä¾‹è¯¦è§£

### 6.1 å…·ä½“æ•°æ®æµè½¬ç¤ºä¾‹

è®©æˆ‘ä»¬è·Ÿè¸ª3ä¸ªå…·ä½“è¯·æ±‚åœ¨ç³»ç»Ÿä¸­çš„å®Œæ•´æµè½¬è¿‡ç¨‹ï¼š

```python
# ç¤ºä¾‹è¾“å…¥
prompts = [
    "What is the capital of France?",    # 9 tokens: [1, 2, 3, 4, 5, 6, 7, 8, 9]
    "Hello world!",                      # 3 tokens: [10, 11, 12]
    "Explain quantum computing briefly." # 5 tokens: [13, 14, 15, 16, 17]
]
```

#### è¿­ä»£ 1: æ‰€æœ‰è¯·æ±‚ Prefill

```mermaid
graph TD
    subgraph "è¾“å…¥æ•°æ®"
        A1["prompts = [<br/>'What is the capital of France?',<br/>'Hello world!',<br/>'Explain quantum computing briefly.'<br/>]"]
    end
    
    subgraph "Tokenization"
        B1["requests = [<br/>Request('req-0', [1,2,3,4,5,6,7,8,9]),<br/>Request('req-1', [10,11,12]),<br/>Request('req-2', [13,14,15,16,17])<br/>]"]
    end
    
    subgraph "è°ƒåº¦å†³ç­–"
        C1["scheduler.schedule()<br/>æ¥çº³æ‰€æœ‰è¯·æ±‚ (prefill)<br/>total_tokens = 9+3+5 = 17"]
        C2["SchedulerOutput(<br/>new_reqs=[req-0,req-1,req-2],<br/>cached_reqs=[],<br/>total_tokens=17<br/>)"]
    end
    
    subgraph "æ‰¹å¤„ç†å‡†å¤‡"
        D1["InputBatch(<br/>req_ids=['req-0','req-1','req-2'],<br/>token_ids=[[1,2,3,4,5,6,7,8,9],[10,11,12],[13,14,15,16,17]],<br/>start_positions=[0,0,0],<br/>is_prefill=[T,T,T]<br/>)"]
    end
    
    subgraph "å¼ é‡å¡«å……"
        E1["Padded Tensors:<br/>token_ids = [<br/>[1,2,3,4,5,6,7,8,9],<br/>[10,11,12,0,0,0,0,0,0],<br/>[13,14,15,16,17,0,0,0,0]<br/>]<br/>attention_mask = [<br/>[1,1,1,1,1,1,1,1,1],<br/>[1,1,1,0,0,0,0,0,0],<br/>[1,1,1,1,1,0,0,0,0]<br/>]"]
    end
    
    subgraph "æ¨¡å‹æ‰§è¡Œ"
        F1["ModelRunner.execute_model_batch()<br/>ä¸ºæ¯ä¸ªè¯·æ±‚å•ç‹¬æ‰§è¡Œ:<br/>req-0: model([1,2,3,4,5,6,7,8,9]) â†’ logits[vocab_size]<br/>req-1: model([10,11,12]) â†’ logits[vocab_size]<br/>req-2: model([13,14,15,16,17]) â†’ logits[vocab_size]"]
    end
    
    subgraph "é‡‡æ ·ç»“æœ"
        G1["Sampler.sample()<br/>req-0: logits â†’ token_id=100 ('Paris')<br/>req-1: logits â†’ token_id=200 ('How')<br/>req-2: logits â†’ token_id=300 ('Quantum')"]
    end
    
    subgraph "çŠ¶æ€æ›´æ–°"
        H1["scheduler.update_from_output()<br/>req-0: æ·»åŠ  token 100<br/>req-1: æ·»åŠ  token 200<br/>req-2: æ·»åŠ  token 300<br/>æ£€æŸ¥åœæ­¢æ¡ä»¶: å…¨éƒ¨ç»§ç»­"]
    end

    A1 --> B1 --> C1 --> C2 --> D1 --> E1 --> F1 --> G1 --> H1
    
    style C1 fill:#e8f5e8
    style D1 fill:#fff3e0
    style E1 fill:#fce4ec
    style F1 fill:#f3e5f5
    style G1 fill:#e1f5fe
```

#### è¿­ä»£ 2: æ‰€æœ‰è¯·æ±‚ Decode

```mermaid
graph TD
    subgraph "è°ƒåº¦å†³ç­–"
        A2["scheduler.schedule()<br/>æ‰€æœ‰è¯·æ±‚ç»§ç»­ decode<br/>total_tokens = 1+1+1 = 3"]
        A3["SchedulerOutput(<br/>new_reqs=[],<br/>cached_reqs=CachedRequestData(<br/>  req_ids=['req-0','req-1','req-2'],<br/>  new_token_ids=[[100],[200],[300]]<br/>)<br/>)"]
    end
    
    subgraph "æ‰¹å¤„ç†å‡†å¤‡"
        B2["InputBatch(<br/>req_ids=['req-0','req-1','req-2'],<br/>token_ids=[[100],[200],[300]],<br/>start_positions=[9,3,5],<br/>is_prefill=[F,F,F]<br/>)"]
    end
    
    subgraph "å¼ é‡å¡«å……"
        C2["Padded Tensors:<br/>token_ids = [<br/>[100],<br/>[200],<br/>[300]<br/>]<br/>positions = [<br/>[9],<br/>[3],<br/>[5]<br/>]"]
    end
    
    subgraph "æ¨¡å‹æ‰§è¡Œ"
        D2["ModelRunner.execute_model_batch()<br/>req-0: model([100], past_kv_0) â†’ logits<br/>req-1: model([200], past_kv_1) â†’ logits<br/>req-2: model([300], past_kv_2) â†’ logits<br/>æ›´æ–°å„è‡ªçš„ KV cache"]
    end
    
    subgraph "é‡‡æ ·ç»“æœ"
        E2["é‡‡æ ·ç»“æœ:<br/>req-0: token_id=101 ('is')<br/>req-1: token_id=201 ('are')<br/>req-2: token_id=301 ('computing')"]
    end
    
    subgraph "çŠ¶æ€æ›´æ–°"
        F2["æ›´æ–°åºåˆ—:<br/>req-0: [1,2,3,4,5,6,7,8,9,100,101]<br/>req-1: [10,11,12,200,201] â† å®Œæˆ!<br/>req-2: [13,14,15,16,17,300,301]<br/>req-1 ç§»å‡º running é˜Ÿåˆ—"]
    end

    A2 --> A3 --> B2 --> C2 --> D2 --> E2 --> F2
    
    style A2 fill:#e8f5e8
    style B2 fill:#fff3e0
    style D2 fill:#f3e5f5
    style F2 fill:#e1f5fe
```

#### è¿­ä»£ 3: åŠ¨æ€æ‰¹æ¬¡è°ƒæ•´

```mermaid
graph TD
    subgraph "è°ƒåº¦å†³ç­–"
        A3["scheduler.schedule()<br/>req-1 å·²å®Œæˆï¼Œä» running ç§»é™¤<br/>æ¥çº³æ–°è¯·æ±‚ req-3 (prefill)<br/>total_tokens = 1+1+12 = 14"]
    end
    
    subgraph "æ‰¹æ¬¡ç»„æˆå˜åŒ–"
        B3["æ–°æ‰¹æ¬¡ç»„æˆ:<br/>- req-0: decode (1 token)<br/>- req-2: decode (1 token)<br/>- req-3: prefill (12 tokens)<br/>åŠ¨æ€è°ƒæ•´ï¼Œä¿æŒé«˜åˆ©ç”¨ç‡"]
    end
    
    subgraph "èµ„æºåˆ©ç”¨"
        C3["GPU åˆ©ç”¨ç‡åˆ†æ:<br/>- é™æ€æ‰¹å¤„ç†: ç­‰å¾…æœ€é•¿è¯·æ±‚<br/>- è¿ç»­æ‰¹å¤„ç†: ç«‹å³å¡«å……æ–°è¯·æ±‚<br/>- åˆ©ç”¨ç‡æå‡: 40% â†’ 80%"]
    end

    A3 --> B3 --> C3
    
    style A3 fill:#e8f5e8
    style B3 fill:#fff3e0
    style C3 fill:#f1f8e9
```

---

## ğŸ¯ ç¬¬ä¸ƒéƒ¨åˆ†ï¼šå®Œæ•´æµç¨‹æ¼”ç¤º

### å®Œæ•´çš„æ¨ç†è¿‡ç¨‹æ¼”ç¤º

```python
def complete_inference_demo():
    """å®Œæ•´çš„è¿ç»­æ‰¹å¤„ç†æ¨ç†æ¼”ç¤º"""
    
    print("ğŸ¬ === è¿ç»­æ‰¹å¤„ç†å®Œæ•´æ¼”ç¤º ===\n")
    
    # 1. åˆå§‹åŒ–ç³»ç»Ÿ
    print("ğŸ”§ æ­¥éª¤1: åˆå§‹åŒ–ç³»ç»Ÿ")
    model_config = ModelConfig(model="Qwen/Qwen2.5-0.5B")
    scheduler_config = SchedulerConfig(max_num_seqs=4, max_num_batched_tokens=100)
    
    engine = LLMEngine(
        model_config=model_config,
        scheduler_config=scheduler_config,
    )
    
    # 2. å‡†å¤‡è¯·æ±‚
    print("\nğŸ“ æ­¥éª¤2: å‡†å¤‡è¯·æ±‚")
    prompts = [
        "What is the capital of France?",
        "Explain AI in simple terms.",
        "Write a haiku about coding.",
    ]
    
    sampling_params = SamplingParams(
        max_tokens=20,
        temperature=0.7,
        top_k=50,
    )
    
    # 3. æ‰§è¡Œæ‰¹é‡ç”Ÿæˆ
    print("\nğŸš€ æ­¥éª¤3: æ‰§è¡Œæ‰¹é‡ç”Ÿæˆ")
    outputs = engine.generate_batch(prompts, sampling_params)
    
    # 4. æ˜¾ç¤ºç»“æœ
    print("\nğŸ“Š æ­¥éª¤4: æ˜¾ç¤ºç»“æœ")
    for req_id, output in outputs.items():
        print(f"\nè¯·æ±‚ {req_id}:")
        print(f"  Prompt: {output.prompt}")
        print(f"  Generated: {output.outputs[0].text}")
        print(f"  Tokens: {len(output.outputs[0].token_ids)}")
        print(f"  Finish reason: {output.outputs[0].finish_reason}")

# è¿è¡Œæ¼”ç¤º
if __name__ == "__main__":
    complete_inference_demo()
```

### å®é™…è¿è¡Œç»“æœæ¼”ç¤º

```bash
$ python examples/m2_inference.py --num-prompts 3 --max-tokens 10 --device cpu

ğŸ§ª æµ‹è¯• M2 æ‰¹å¤„ç†åŠŸèƒ½...
âœ… é…ç½®åˆ›å»ºæˆåŠŸ
Loading tokenizer...
âœ… å¼•æ“åˆå§‹åŒ–æˆåŠŸ
ğŸš€ å¼€å§‹æ‰¹å¤„ç†æµ‹è¯•...

Batch generation complete:
  Requests: 3
  Total time: 7.66s
  Total tokens: 30
  Throughput: 3.92 tokens/s
  Iterations: 10

âœ… æ‰¹å¤„ç†å®Œæˆ!
å¤„ç†äº† 3 ä¸ªè¯·æ±‚:
  req-0: Hello -> , I'm trying to create a function that takes...
  req-1: Hi there -> ! I'm a 17 year old girl...
  req-2: How are you? ->  How are you doing? How are you doing?...
```

**å…³é”®è§‚å¯Ÿ**:
- âœ… æˆåŠŸå¤„ç† 3 ä¸ªè¯·æ±‚
- âœ… æ€»å…± 10 æ¬¡è¿­ä»£ï¼ˆè¿ç»­æ‰¹å¤„ç†ï¼‰
- âœ… æ¯ä¸ªè¯·æ±‚ç”Ÿæˆäº† 10 ä¸ª tokens
- âœ… ç³»ç»Ÿè‡ªåŠ¨ç®¡ç†äº†è¯·æ±‚çš„ç”Ÿå‘½å‘¨æœŸ
- âœ… ååé‡: 3.92 tokens/sï¼ˆCPUæ¨¡å¼ä¸‹çš„åŸºå‡†ï¼‰

---

## ğŸ“ ç¬¬å…«éƒ¨åˆ†ï¼šæ€»ç»“å’Œæ‰©å±•

### M2 è¿ç»­æ‰¹å¤„ç†ç³»ç»Ÿå…¨æ™¯å›¾

```mermaid
graph TB
    subgraph "ğŸŒ ç”¨æˆ·å±‚"
        U1[ç”¨æˆ·è°ƒç”¨ generate_batch]
        U2[prompts: List[str]]
        U3[sampling_params: SamplingParams]
    end

    subgraph "ğŸ›ï¸ å¼•æ“å±‚ (LLMEngine)"
        E1[è½¬æ¢ä¸º Request å¯¹è±¡]
        E2[æ·»åŠ åˆ° EngineCore]
        E3[ä¸»å¾ªç¯: while æœ‰æœªå®Œæˆè¯·æ±‚]
        E4[æ”¶é›†è¾“å‡º]
    end

    subgraph "ğŸ”„ æ ¸å¿ƒåè°ƒå±‚ (EngineCore)"
        C1[step: å•æ¬¡è¿­ä»£]
        C2[åè°ƒå„ä¸ªç»„ä»¶]
        C3[ç®¡ç†è¯·æ±‚ç”Ÿå‘½å‘¨æœŸ]
    end

    subgraph "ğŸ§  è°ƒåº¦å±‚ (Scheduler)"
        S1[waiting: FCFSRequestQueue]
        S2[running: List[Request]]
        S3[schedule: è°ƒåº¦å†³ç­–]
        S4[update_from_output: çŠ¶æ€æ›´æ–°]
    end

    subgraph "ğŸ“¦ æ‰¹å¤„ç†å±‚ (InputBatch)"
        B1[prepare_inputs_from_scheduler_output]
        B2[to_tensors: å¡«å……å’Œæ©ç ]
        B3[å¤„ç†ä¸å®šé•¿åºåˆ—]
    end

    subgraph "ğŸš€ æ‰§è¡Œå±‚ (GPUExecutor â†’ GPUWorker â†’ ModelRunner)"
        M1[execute_model_batch]
        M2[ä¸ºæ¯ä¸ªè¯·æ±‚å•ç‹¬æ‰§è¡Œ]
        M3[ç®¡ç† KV cache]
        M4[è¿”å› logits]
    end

    subgraph "ğŸ² é‡‡æ ·å±‚ (Sampler)"
        SA1[sample: ä¸ºæ¯ä¸ªè¯·æ±‚é‡‡æ ·]
        SA2[åº”ç”¨é‡‡æ ·ç­–ç•¥]
        SA3[ç”Ÿæˆ next_token]
    end

    subgraph "ğŸ’¾ æ•°æ®ç»“æ„"
        D1[Request & Sequence]
        D2[SchedulerOutput]
        D3[InputBatch]
        D4[RequestOutput]
    end

    %% æ•°æ®æµ
    U1 --> E1
    U2 --> E1
    U3 --> E1
    E1 --> E2
    E2 --> E3
    E3 --> C1
    C1 --> S3
    S3 --> B1
    B1 --> M1
    M1 --> SA1
    SA1 --> S4
    S4 --> C2
    C2 --> E4
    E4 --> U1

    %% æ•°æ®ç»“æ„å…³ç³»
    E1 -.-> D1
    S3 -.-> D2
    B1 -.-> D3
    S4 -.-> D4

    %% æ ·å¼
    style U1 fill:#e3f2fd,stroke:#01579b,stroke-width:3px
    style C1 fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    style S3 fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    style M1 fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    style SA1 fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
```

### M2 æ ¸å¿ƒåˆ›æ–°ç‚¹æ€»ç»“

```mermaid
mindmap
  root((M2 è¿ç»­æ‰¹å¤„ç†))
    è¿­ä»£çº§è°ƒåº¦
      åŠ¨æ€æ‰¹æ¬¡ç»´æŠ¤
      å®Œæˆè¯·æ±‚ç«‹å³ç§»é™¤
      æ–°è¯·æ±‚ç«‹å³åŠ å…¥
      GPU åˆ©ç”¨ç‡ 60-80%
    
    æ··åˆ Prefill/Decode
      æ–°è¯·æ±‚: å¤„ç†å®Œæ•´ prompt
      è¿è¡Œä¸­è¯·æ±‚: æ¯æ¬¡ 1 token
      åŒä¸€æ‰¹æ¬¡æ··åˆå¤„ç†
      èµ„æºéœ€æ±‚å¹³è¡¡
    
    Token é¢„ç®—ç®¡ç†
      max_num_seqs: å¹¶å‘é™åˆ¶
      max_num_batched_tokens: è®¡ç®—é™åˆ¶
      è´ªå¿ƒè°ƒåº¦ç­–ç•¥
      é˜²æ­¢ OOM
    
    KV Cache ç®¡ç†
      æ¯è¯·æ±‚ç‹¬ç«‹ cache
      past_key_values æœºåˆ¶
      æ˜¾å¼èµ„æºé‡Šæ”¾
      M3+ PagedAttention é¢„ç•™
    
    æ€§èƒ½æå‡
      ååé‡ 3-5x
      GPU åˆ©ç”¨ç‡ 2-3x
      å»¶è¿Ÿç•¥æœ‰å¢åŠ 
      æ•´ä½“æ•ˆç‡æ˜¾è‘—æ”¹å–„
```

### å…³é”®è®¾è®¡å†³ç­–å›é¡¾

1. **ä¸ºä»€ä¹ˆä½¿ç”¨ FCFS é˜Ÿåˆ—ï¼Ÿ**
   - ç®€å•å…¬å¹³
   - é¿å…é¥¥é¥¿
   - æ˜“äºå®ç°

2. **ä¸ºä»€ä¹ˆåˆ†ç¦» NewRequestData å’Œ CachedRequestDataï¼Ÿ**
   - å‡å°‘é€šä¿¡å¼€é”€
   - æ”¯æŒå¢é‡æ›´æ–°
   - æé«˜æ•ˆç‡

3. **ä¸ºä»€ä¹ˆ M2 æ¯ä¸ªè¯·æ±‚å•ç‹¬æ‰§è¡Œï¼Ÿ**
   - HuggingFace æ¨¡å‹é™åˆ¶
   - å®ç°ç®€å•
   - ä¸º M3 é“ºè·¯

4. **ä¸ºä»€ä¹ˆéœ€è¦ Token é¢„ç®—ç®¡ç†ï¼Ÿ**
   - æ§åˆ¶è®¡ç®—é‡
   - é¿å… OOM
   - ä¿è¯å“åº”æ€§

### M3+ æ‰©å±•æ–¹å‘

```python
# M3: PagedAttention çœŸæ­£æ‰¹å¤„ç†
def execute_model_batch_paged(self, input_batch):
    """M3+ å°†å®ç°çš„çœŸæ­£æ‰¹å¤„ç†"""
    
    # 1. åˆ†é… KV cache å—
    block_tables = allocate_kv_blocks(input_batch)
    
    # 2. çœŸæ­£çš„æ‰¹å¤„ç†å‰å‘ä¼ æ’­
    logits = paged_attention_forward(
        input_batch.token_ids,
        block_tables,
        self.kv_cache_blocks,
    )
    
    # 3. è¿”å›æ‰€æœ‰è¯·æ±‚çš„ logits
    return split_logits_by_request(logits, input_batch)

# M3+: æŠ¢å å’Œäº¤æ¢
def handle_memory_pressure(self):
    """å¤„ç†å†…å­˜å‹åŠ›"""
    
    if memory_usage > threshold:
        # é€‰æ‹©æŠ¢å è¯·æ±‚
        victim_requests = select_preemption_victims()
        
        # äº¤æ¢åˆ° CPU
        for request in victim_requests:
            swap_out_request(request)
```

### å­¦ä¹ è¦ç‚¹

1. **ç³»ç»Ÿæ€ç»´**: ç†è§£å„ç»„ä»¶å¦‚ä½•åä½œ
2. **æ¥å£è®¾è®¡**: æ¸…æ™°çš„æŠ½è±¡å’ŒèŒè´£åˆ†ç¦»
3. **èµ„æºç®¡ç†**: Token é¢„ç®—å’Œå†…å­˜ç®¡ç†
4. **æ‰©å±•æ€§**: ä¸ºæœªæ¥åŠŸèƒ½é¢„ç•™æ¥å£

---

## ğŸ¤ ç»“æŸè¯­

ä»Šå¤©æˆ‘ä»¬ä»é›¶å¼€å§‹å®ç°äº†ä¸€ä¸ªè¿ç»­æ‰¹å¤„ç†ç³»ç»Ÿï¼Œçœ‹åˆ°äº†ï¼š

1. **é—®é¢˜åˆ†æ**: ä¼ ç»Ÿæ‰¹å¤„ç†çš„æ•ˆç‡é—®é¢˜
2. **æ¶æ„è®¾è®¡**: è°ƒåº¦å™¨ã€æ‰§è¡Œå™¨ã€åè°ƒå™¨çš„åˆ†å·¥
3. **æ ¸å¿ƒç®—æ³•**: åŠ¨æ€è°ƒåº¦å’Œèµ„æºç®¡ç†
4. **å®Œæ•´æµç¨‹**: ä»è¯·æ±‚åˆ°è¾“å‡ºçš„å…¨è¿‡ç¨‹
5. **æ‰©å±•æ–¹å‘**: M3+ çš„æ”¹è¿›ç©ºé—´

è¿ç»­æ‰¹å¤„ç†æ˜¯ç°ä»£ LLM æ¨ç†ç³»ç»Ÿçš„åŸºç¡€ï¼ŒæŒæ¡äº†è¿™ä¸ªæŠ€æœ¯ï¼Œä½ å°±ç†è§£äº†é«˜æ€§èƒ½æ¨ç†æ¡†æ¶çš„æ ¸å¿ƒåŸç†ã€‚

**ä¸‹ä¸€æ­¥**: æˆ‘ä»¬å°†å®ç° PagedAttentionï¼Œè¿›ä¸€æ­¥æå‡å†…å­˜æ•ˆç‡å’Œæ‰¹å¤„ç†èƒ½åŠ›ï¼

è°¢è°¢å¤§å®¶ï¼æœ‰ä»€ä¹ˆé—®é¢˜å—ï¼Ÿ ğŸ™‹â€â™‚ï¸
