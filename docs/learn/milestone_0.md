# Milestone 0 å­¦ä¹ ç¬”è®°ï¼šé¡¹ç›®åŸºç¡€æ¶æ„

> æœ¬æ–‡æ¡£ä»‹ç» M0 é˜¶æ®µæ¶‰åŠçš„æ ¸å¿ƒæŠ€æœ¯åŸç†å’Œè®¾è®¡æ€æƒ³

---

## ğŸ“š ç›®å½•

1. [é…ç½®ç³»ç»Ÿè®¾è®¡](#1-é…ç½®ç³»ç»Ÿè®¾è®¡)
2. [é‡‡æ ·å‚æ•°åŸç†](#2-é‡‡æ ·å‚æ•°åŸç†)
3. [è¯·æ±‚å’Œåºåˆ—æŠ½è±¡](#3-è¯·æ±‚å’Œåºåˆ—æŠ½è±¡)
4. [æ¨¡å‹åŠ è½½æœºåˆ¶](#4-æ¨¡å‹åŠ è½½æœºåˆ¶)
5. [æ•°æ®æµè®¾è®¡](#5-æ•°æ®æµè®¾è®¡)
6. [å…³é”®è®¾è®¡æ¨¡å¼](#6-å…³é”®è®¾è®¡æ¨¡å¼)

---

## 1. é…ç½®ç³»ç»Ÿè®¾è®¡

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦é…ç½®ç³»ç»Ÿï¼Ÿ

åœ¨å¤§è¯­è¨€æ¨¡å‹æ¨ç†ç³»ç»Ÿä¸­ï¼Œé…ç½®ç³»ç»Ÿæ˜¯æ ¸å¿ƒåŸºç¡€è®¾æ–½ï¼š

1. **æ¨¡å‹é…ç½®**ï¼šä¸åŒæ¨¡å‹æœ‰ä¸åŒçš„å‚æ•°ï¼ˆdtypeã€max_length ç­‰ï¼‰
2. **èµ„æºç®¡ç†**ï¼šéœ€è¦æ§åˆ¶ GPU æ˜¾å­˜ä½¿ç”¨ã€CPU swap ç©ºé—´ç­‰
3. **è°ƒåº¦ç­–ç•¥**ï¼šæ‰¹å¤„ç†å¤§å°ã€åºåˆ—æ•°é‡ç­‰å½±å“æ€§èƒ½
4. **å¯æ‰©å±•æ€§**ï¼šä¸ºæœªæ¥åŠŸèƒ½é¢„ç•™é…ç½®æ¥å£

### 1.2 é…ç½®ç³»ç»Ÿå±‚æ¬¡ç»“æ„

```
EngineConfig (å¼•æ“é…ç½®)
    â”œâ”€â”€ ModelConfig (æ¨¡å‹é…ç½®)
    â”‚   â”œâ”€â”€ model: æ¨¡å‹è·¯å¾„
    â”‚   â”œâ”€â”€ dtype: æ•°æ®ç±»å‹
    â”‚   â”œâ”€â”€ tokenizer: åˆ†è¯å™¨è·¯å¾„
    â”‚   â””â”€â”€ max_model_len: æœ€å¤§åºåˆ—é•¿åº¦
    â”‚
    â”œâ”€â”€ CacheConfig (ç¼“å­˜é…ç½®)
    â”‚   â”œâ”€â”€ block_size: KV Cache å—å¤§å°
    â”‚   â”œâ”€â”€ gpu_memory_utilization: GPU æ˜¾å­˜åˆ©ç”¨ç‡
    â”‚   â””â”€â”€ enable_prefix_caching: å‰ç¼€ç¼“å­˜å¼€å…³
    â”‚
    â””â”€â”€ SchedulerConfig (è°ƒåº¦é…ç½®)
        â”œâ”€â”€ max_num_batched_tokens: æœ€å¤§æ‰¹å¤„ç† token æ•°
        â”œâ”€â”€ max_num_seqs: æœ€å¤§åºåˆ—æ•°
        â””â”€â”€ enable_chunked_prefill: åˆ†å—é¢„å¡«å……å¼€å…³
```

### 1.3 å…³é”®è®¾è®¡å†³ç­–

#### a) ä½¿ç”¨ dataclass

```python
from dataclasses import dataclass

@dataclass
class ModelConfig:
    model: str
    dtype: str = "auto"
    # ...
```

**ä¼˜ç‚¹**ï¼š
- è‡ªåŠ¨ç”Ÿæˆ `__init__`ã€`__repr__` ç­‰æ–¹æ³•
- ç±»å‹æç¤ºæ¸…æ™°
- æ”¯æŒé»˜è®¤å€¼
- å¯ä»¥ä½¿ç”¨ `__post_init__` è¿›è¡ŒéªŒè¯

#### b) ç±»å‹çº¦æŸ

```python
from typing import Literal

ModelDType = Literal["auto", "half", "float16", "bfloat16", "float", "float32"]
```

**ä¼˜ç‚¹**ï¼š
- ç¼–è¯‘æ—¶ç±»å‹æ£€æŸ¥
- IDE è‡ªåŠ¨è¡¥å…¨
- é¿å…æ— æ•ˆå€¼

#### c) å‚æ•°éªŒè¯

```python
def __post_init__(self):
    if self.block_size <= 0:
        raise ValueError(f"block_size must be positive")
```

**ä¼˜ç‚¹**ï¼š
- æå‰å‘ç°é…ç½®é”™è¯¯
- æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯
- é¿å…è¿è¡Œæ—¶é”™è¯¯

### 1.4 ä¸ vLLM çš„å¯¹é½

FoloVLLM çš„é…ç½®ç³»ç»Ÿå®Œå…¨å‚è€ƒ vLLM v1 è®¾è®¡ï¼š

| vLLM              | FoloVLLM          | è¯´æ˜                     |
| ----------------- | ----------------- | ------------------------ |
| `ModelConfig`     | `ModelConfig`     | æ¨¡å‹é…ç½®ï¼Œå‚æ•°åŸºæœ¬ä¸€è‡´   |
| `CacheConfig`     | `CacheConfig`     | ç¼“å­˜é…ç½®ï¼Œç®€åŒ–äº†éƒ¨åˆ†å‚æ•° |
| `SchedulerConfig` | `SchedulerConfig` | è°ƒåº¦é…ç½®ï¼Œé¢„ç•™äº†æ‰©å±•æ¥å£ |

---

## 2. é‡‡æ ·å‚æ•°åŸç†

### 2.1 ä»€ä¹ˆæ˜¯é‡‡æ ·ï¼Ÿ

é‡‡æ ·æ˜¯ä»æ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©ä¸‹ä¸€ä¸ª token çš„è¿‡ç¨‹ï¼š

```
æ¨¡å‹è¾“å‡º logits: [vocab_size]
    â†“ softmax
æ¦‚ç‡åˆ†å¸ƒ probs: [vocab_size]
    â†“ é‡‡æ ·ç­–ç•¥
ä¸‹ä¸€ä¸ª token: int
```

### 2.2 é‡‡æ ·ç­–ç•¥è¯¦è§£

#### a) Greedy Sampling (è´ªå¿ƒé‡‡æ ·)

```python
next_token = argmax(probs)
```

**åŸç†**ï¼šæ€»æ˜¯é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ token

**ç‰¹ç‚¹**ï¼š
- ç¡®å®šæ€§è¾“å‡ºï¼ˆç›¸åŒè¾“å…¥æ€»æ˜¯å¾—åˆ°ç›¸åŒè¾“å‡ºï¼‰
- å¯èƒ½å¯¼è‡´é‡å¤å’Œå•è°ƒçš„æ–‡æœ¬
- é€‚åˆéœ€è¦ç¡®å®šæ€§çš„ä»»åŠ¡ï¼ˆå¦‚ç¿»è¯‘ï¼‰

**å®ç°**ï¼š`temperature = 0.0`

#### b) Temperature Scaling (æ¸©åº¦ç¼©æ”¾)

```python
logits_scaled = logits / temperature
probs = softmax(logits_scaled)
```

**åŸç†**ï¼šè°ƒæ•´æ¦‚ç‡åˆ†å¸ƒçš„"é™¡å³­"ç¨‹åº¦

**æ•ˆæœ**ï¼š
- `temperature < 1.0`: åˆ†å¸ƒæ›´é™¡å³­ï¼Œé«˜æ¦‚ç‡ token æ›´å®¹æ˜“è¢«é€‰ä¸­ï¼ˆæ›´ç¡®å®šï¼‰
- `temperature = 1.0`: ä¸æ”¹å˜åˆ†å¸ƒï¼ˆåŸå§‹æ¦‚ç‡ï¼‰
- `temperature > 1.0`: åˆ†å¸ƒæ›´å¹³ç¼“ï¼Œä½æ¦‚ç‡ token ä¹Ÿæœ‰æœºä¼šï¼ˆæ›´éšæœºï¼‰

**å¯è§†åŒ–**ï¼š
```
temperature = 0.5      temperature = 1.0      temperature = 2.0
    â–â–â–ˆâ–â–                  â–‚â–…â–ˆâ–…â–‚                  â–„â–†â–ˆâ–†â–„
   æ›´ç¡®å®š                   å¹³è¡¡                   æ›´éšæœº
```

#### c) Top-k Sampling

```python
top_k_probs, top_k_indices = torch.topk(probs, k)
top_k_probs = top_k_probs / top_k_probs.sum()
next_token = sample(top_k_indices, top_k_probs)
```

**åŸç†**ï¼šåªä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ª token ä¸­é‡‡æ ·

**ä¼˜ç‚¹**ï¼š
- è¿‡æ»¤æ‰ä½æ¦‚ç‡çš„"å™ªéŸ³" token
- ä¿æŒè¾“å‡ºè´¨é‡
- å¢åŠ å¤šæ ·æ€§

**å…¸å‹å€¼**ï¼š`k = 50`

#### d) Top-p (Nucleus) Sampling

```python
sorted_probs, sorted_indices = torch.sort(probs, descending=True)
cumsum_probs = torch.cumsum(sorted_probs, dim=-1)
mask = cumsum_probs <= p
nucleus_probs = sorted_probs[mask]
```

**åŸç†**ï¼šé€‰æ‹©ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° p çš„æœ€å° token é›†åˆ

**ä¼˜ç‚¹**ï¼š
- åŠ¨æ€è°ƒæ•´å€™é€‰é›†å¤§å°
- åœ¨æ¦‚ç‡åˆ†å¸ƒå¹³ç¼“æ—¶åŒ…å«æ›´å¤šå€™é€‰
- åœ¨æ¦‚ç‡åˆ†å¸ƒé™¡å³­æ—¶å‡å°‘å€™é€‰

**å…¸å‹å€¼**ï¼š`p = 0.9` æˆ– `p = 0.95`

**ç¤ºä¾‹**ï¼š
```
Tokenæ¦‚ç‡: [0.4, 0.3, 0.15, 0.1, 0.05]
p = 0.9:   [âœ“   âœ“   âœ“    âœ—   âœ—  ]  ç´¯ç§¯åˆ° 0.85 < 0.9
```

#### e) Min-p Sampling

```python
threshold = p * max(probs)
mask = probs >= threshold
```

**åŸç†**ï¼šè¿‡æ»¤æ‰æ¦‚ç‡ä½äº `p * max_prob` çš„ token

**ä¼˜ç‚¹**ï¼š
- ç›¸å¯¹é˜ˆå€¼ï¼Œé€‚åº”ä¸åŒçš„æ¦‚ç‡åˆ†å¸ƒ
- é¿å…é€‰æ‹©"ä¸å¤ªå¯èƒ½"çš„ token

### 2.3 é‡‡æ ·ç­–ç•¥ç»„åˆ

å®é™…ä½¿ç”¨ä¸­ï¼Œé€šå¸¸ç»„åˆå¤šç§ç­–ç•¥ï¼š

```python
SamplingParams(
    temperature=0.8,  # å¢åŠ éšæœºæ€§
    top_p=0.9,        # Nucleus sampling
    top_k=50,         # è¿‡æ»¤ä½æ¦‚ç‡ token
)
```

**æ‰§è¡Œé¡ºåº**ï¼š
1. Temperature scaling
2. Top-k filtering
3. Top-p filtering
4. Min-p filtering
5. Random sampling

### 2.4 åœæ­¢æ¡ä»¶

#### a) Stop Strings

```python
stop = ["</s>", "\n\n", "Human:"]
```

ç”Ÿæˆçš„æ–‡æœ¬åŒ…å«ä»»ä¸€åœæ­¢å­—ç¬¦ä¸²æ—¶åœæ­¢ã€‚

#### b) Stop Token IDs

```python
stop_token_ids = [2, 50256]  # EOS tokens
```

ç”Ÿæˆçš„ token ID åœ¨åœæ­¢åˆ—è¡¨ä¸­æ—¶åœæ­¢ã€‚

#### c) Max Tokens

```python
max_tokens = 100
```

ç”ŸæˆæŒ‡å®šæ•°é‡çš„ token ååœæ­¢ã€‚

---

## 3. è¯·æ±‚å’Œåºåˆ—æŠ½è±¡

### 3.1 ä¸ºä»€ä¹ˆéœ€è¦åºåˆ—æŠ½è±¡ï¼Ÿ

åœ¨ LLM æ¨ç†ä¸­ï¼Œéœ€è¦ç®¡ç†å¤æ‚çš„çŠ¶æ€ï¼š

1. **å¤šåºåˆ—ç”Ÿæˆ**ï¼šä¸€ä¸ªè¯·æ±‚å¯èƒ½ç”Ÿæˆå¤šä¸ªå€™é€‰åºåˆ—ï¼ˆn > 1ï¼‰
2. **çŠ¶æ€è¿½è¸ª**ï¼šæ¯ä¸ªåºåˆ—æœ‰è‡ªå·±çš„ç”ŸæˆçŠ¶æ€
3. **èµ„æºç®¡ç†**ï¼šKV Cache éœ€è¦æŒ‰åºåˆ—åˆ†é…
4. **è°ƒåº¦å†³ç­–**ï¼šè°ƒåº¦å™¨éœ€è¦çŸ¥é“å“ªäº›åºåˆ—åœ¨è¿è¡Œ

### 3.2 ä¸‰å±‚æŠ½è±¡ç»“æ„

```
Request (è¯·æ±‚)
    â”œâ”€â”€ åŒ…å«å¤šä¸ª Sequence
    â”œâ”€â”€ å…±äº« prompt å’Œ sampling_params
    â””â”€â”€ ç®¡ç†è¯·æ±‚çº§åˆ«çš„çŠ¶æ€

Sequence (åºåˆ—)
    â”œâ”€â”€ æœ‰ç‹¬ç«‹çš„ seq_id
    â”œâ”€â”€ åŒ…å«ä¸€ä¸ª SequenceData
    â”œâ”€â”€ æœ‰è‡ªå·±çš„çŠ¶æ€ (WAITING/RUNNING/FINISHED)
    â””â”€â”€ ç®¡ç†åºåˆ—çº§åˆ«çš„èµ„æº (KV Cache blocks)

SequenceData (åºåˆ—æ•°æ®)
    â”œâ”€â”€ prompt_token_ids (è¾“å…¥)
    â”œâ”€â”€ output_token_ids (è¾“å‡º)
    â””â”€â”€ æä¾› token æ“ä½œæ¥å£
```

### 3.3 çŠ¶æ€æœºè®¾è®¡

#### è¯·æ±‚çŠ¶æ€æœº

```
WAITING â†’ RUNNING â†’ FINISHED_*
   â†“         â†“
   â””â”€ SWAPPED â”€â”˜
```

**çŠ¶æ€è¯´æ˜**ï¼š
- `WAITING`: åœ¨ç­‰å¾…é˜Ÿåˆ—ä¸­
- `RUNNING`: æ­£åœ¨å¤„ç†
- `SWAPPED`: è¢«æ¢å‡ºåˆ° CPUï¼ˆå†…å­˜ä¸è¶³æ—¶ï¼‰
- `FINISHED_STOPPED`: é‡åˆ°åœæ­¢æ¡ä»¶
- `FINISHED_LENGTH_CAPPED`: è¾¾åˆ°æœ€å¤§é•¿åº¦
- `FINISHED_ABORTED`: è¢«ç”¨æˆ·ä¸­æ­¢

#### åºåˆ—çŠ¶æ€æœº

ä¸è¯·æ±‚çŠ¶æ€æœºç›¸åŒï¼Œä½†å¢åŠ äº†ï¼š
- `FINISHED_IGNORED`: åœ¨ best_of > n æ—¶è¢«å¿½ç•¥çš„åºåˆ—

### 3.4 Sequence Fork æœºåˆ¶

```python
def fork(self, new_seq_id: str) -> "Sequence":
    """Fork ä¸€ä¸ªæ–°åºåˆ—ï¼ˆç”¨äº beam search æˆ– parallel samplingï¼‰"""
    new_data = SequenceData(
        prompt_token_ids=self.data.prompt_token_ids.copy(),
        output_token_ids=self.data.output_token_ids.copy(),
    )
    return Sequence(
        seq_id=new_seq_id,
        request_id=self.request_id,
        data=new_data,
        sampling_params=self.sampling_params,
    )
```

**ç”¨é€”**ï¼š
- **Beam Search**ï¼šæ¯æ¬¡æ‰©å±•æ—¶ fork å¤šä¸ªå€™é€‰
- **Parallel Sampling**ï¼šç”Ÿæˆ n > 1 ä¸ªç‹¬ç«‹åºåˆ—
- **Speculative Decoding**ï¼šéªŒè¯æ¨æµ‹çš„ token

**å…³é”®**ï¼šæ·±æ‹·è´æ•°æ®ï¼Œé¿å…å…±äº«çŠ¶æ€

### 3.5 n vs best_of

```python
SamplingParams(n=3, best_of=5)
```

- `best_of=5`: ç”Ÿæˆ 5 ä¸ªå€™é€‰åºåˆ—
- `n=3`: æœ€ç»ˆè¿”å› 3 ä¸ªæœ€å¥½çš„åºåˆ—

**æµç¨‹**ï¼š
1. åˆ›å»º 5 ä¸ª Sequence
2. å¹¶è¡Œç”Ÿæˆï¼ˆå…±äº« prompt KV Cacheï¼‰
3. æ ¹æ®ç´¯ç§¯ log æ¦‚ç‡æ’åº
4. è¿”å›å‰ 3 ä¸ª

---

## 4. æ¨¡å‹åŠ è½½æœºåˆ¶

### 4.1 HuggingFace æ¨¡å‹åŠ è½½

```python
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=dtype,
    trust_remote_code=True,
    low_cpu_mem_usage=True,
)
```

#### å…³é”®å‚æ•°

**torch_dtype**ï¼š
- `torch.float16`: åŠç²¾åº¦ï¼Œæ˜¾å­˜å ç”¨å‡åŠ
- `torch.bfloat16`: Brain Float16ï¼Œæ•°å€¼èŒƒå›´æ›´å¤§
- `torch.float32`: å…¨ç²¾åº¦ï¼Œæœ€å‡†ç¡®ä½†æœ€å æ˜¾å­˜

**trust_remote_code**ï¼š
- å…è®¸æ‰§è¡Œæ¨¡å‹ä»“åº“ä¸­çš„è‡ªå®šä¹‰ä»£ç 
- Qwen ç­‰æ¨¡å‹éœ€è¦æ­¤é€‰é¡¹

**low_cpu_mem_usage**ï¼š
- ä½¿ç”¨ accelerate åº“çš„ä¼˜åŒ–åŠ è½½
- å‡å°‘ CPU å†…å­˜å³°å€¼

### 4.2 Tokenizer é…ç½®

```python
tokenizer = AutoTokenizer.from_pretrained(
    tokenizer_path,
    use_fast=True,
    padding_side="left",
)
```

#### Padding Side

**Left Padding (æ¨èç”¨äºç”Ÿæˆ)**ï¼š
```
Sequence 1: [PAD][PAD]token1 token2 token3
Sequence 2: [PAD]token1 token2 token3 token4
                              â†‘
                      ç”Ÿæˆä»è¿™é‡Œå¼€å§‹
```

**Right Padding (ç”¨äºåˆ†ç±»)**ï¼š
```
Sequence 1: token1 token2 token3[PAD][PAD]
Sequence 2: token1 token2 token3 token4[PAD]
                  â†‘
         åˆ†ç±»å™¨çœ‹æœ€åä¸€ä¸ªçœŸå® token
```

### 4.3 Dtype é€‰æ‹©ç­–ç•¥

```python
def _get_dtype(self) -> torch.dtype:
    if self.model_config.torch_dtype is not None:
        return self.model_config.torch_dtype
    
    # é»˜è®¤ï¼šGPU ç”¨ FP16ï¼ŒCPU ç”¨ FP32
    if torch.cuda.is_available():
        return torch.float16
    else:
        return torch.float32
```

**å†³ç­–å› ç´ **ï¼š
1. **ç²¾åº¦éœ€æ±‚**ï¼šç§‘å­¦è®¡ç®—ç”¨ FP32ï¼Œæ¨ç†ç”¨ FP16
2. **æ˜¾å­˜é™åˆ¶**ï¼šFP16 å‡åŠæ˜¾å­˜
3. **ç¡¬ä»¶æ”¯æŒ**ï¼šA100/H100 æ”¯æŒ BF16
4. **æ¨¡å‹è®­ç»ƒ dtype**ï¼šæœ€å¥½ä¸è®­ç»ƒæ—¶ä¸€è‡´

### 4.4 Max Model Length æ¨æ–­

```python
if self.model_config.max_model_len is None:
    if hasattr(hf_config, "max_position_embeddings"):
        self.model_config.max_model_len = hf_config.max_position_embeddings
    else:
        self.model_config.max_model_len = 2048  # é»˜è®¤å€¼
```

**æ¥æº**ï¼š
1. ç”¨æˆ·æ˜¾å¼æŒ‡å®šï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
2. æ¨¡å‹é…ç½®æ–‡ä»¶ä¸­çš„ `max_position_embeddings`
3. é»˜è®¤å€¼ï¼ˆ2048ï¼‰

---

## 5. æ•°æ®æµè®¾è®¡

### 5.1 ç«¯åˆ°ç«¯æ•°æ®æµ

```
ç”¨æˆ·è¾“å…¥æ–‡æœ¬
    â†“ Tokenizer.encode()
prompt_token_ids: List[int]
    â†“ åˆ›å»º Request
Request + Sequence
    â†“ è°ƒåº¦å™¨è°ƒåº¦ (M2)
Batch of Sequences
    â†“ æ¨¡å‹å‰å‘ä¼ æ’­ (M1)
Logits: [batch_size, vocab_size]
    â†“ é‡‡æ · (M1)
next_tokens: [batch_size]
    â†“ æ·»åŠ åˆ° Sequence
output_token_ids: List[int]
    â†“ Tokenizer.decode()
è¾“å‡ºæ–‡æœ¬
```

### 5.2 é…ç½®ä¼ é€’

```
EngineConfig
    â†“ åˆ†å‘
ModelConfig â†’ ModelLoader â†’ Model
CacheConfig â†’ KVCacheManager (M3)
SchedulerConfig â†’ Scheduler (M2)
```

### 5.3 çŠ¶æ€æ›´æ–°æµ

```
Sequence.status = WAITING
    â†“ Scheduler.schedule()
Sequence.status = RUNNING
    â†“ Worker.execute()
Sequence.add_token_id(new_token)
    â†“ æ£€æŸ¥åœæ­¢æ¡ä»¶
Sequence.status = FINISHED_*
```

---

## 6. å…³é”®è®¾è®¡æ¨¡å¼

### 6.1 Builder Pattern (é…ç½®æ„å»º)

```python
# åˆ†æ­¥æ„å»ºé…ç½®
model_config = ModelConfig(model="Qwen/Qwen3-0.6B")
cache_config = CacheConfig(block_size=16)
scheduler_config = SchedulerConfig(max_num_seqs=256)

# ç»„è£…æˆå¼•æ“é…ç½®
engine_config = EngineConfig(
    model_config=model_config,
    cache_config=cache_config,
    scheduler_config=scheduler_config,
)
```

### 6.2 Strategy Pattern (é‡‡æ ·ç­–ç•¥)

```python
class SamplingParams:
    @property
    def sampling_type(self) -> SamplingType:
        if self.temperature == 0.0:
            return SamplingType.GREEDY
        else:
            return SamplingType.RANDOM
```

ä¸åŒçš„å‚æ•°ç»„åˆå¯¹åº”ä¸åŒçš„é‡‡æ ·ç­–ç•¥ï¼Œè¿è¡Œæ—¶åŠ¨æ€é€‰æ‹©ã€‚

### 6.3 Factory Pattern (æ¨¡å‹åŠ è½½)

```python
def get_model_and_tokenizer(config, device):
    loader = ModelLoader(config)
    return loader.load_model_and_tokenizer(device)
```

å°è£…å¤æ‚çš„æ¨¡å‹åŠ è½½é€»è¾‘ã€‚

### 6.4 State Pattern (åºåˆ—çŠ¶æ€)

```python
class SequenceStatus(Enum):
    WAITING = "waiting"
    RUNNING = "running"
    FINISHED_STOPPED = "finished_stopped"
    
    def is_finished(self) -> bool:
        return self in [
            SequenceStatus.FINISHED_STOPPED,
            SequenceStatus.FINISHED_LENGTH_CAPPED,
            # ...
        ]
```

å°è£…çŠ¶æ€è½¬æ¢é€»è¾‘ã€‚

---

## 7. æ€§èƒ½è€ƒè™‘

### 7.1 å†…å­˜ä¼˜åŒ–

1. **Dataclass vs Dict**ï¼š
   - Dataclass æœ‰ç±»å‹æ£€æŸ¥å’Œæ›´å¥½çš„æ€§èƒ½
   - Dict æ›´çµæ´»ä½†å®¹æ˜“å‡ºé”™

2. **Deep Copy vs Shallow Copy**ï¼š
   - Sequence fork ä½¿ç”¨æ·±æ‹·è´é¿å…å…±äº«çŠ¶æ€
   - ä½†ä¼šå¢åŠ å†…å­˜å¼€é”€

3. **List vs Numpy Array**ï¼š
   - token_ids ä½¿ç”¨ List æ–¹ä¾¿åŠ¨æ€å¢é•¿
   - æ‰¹å¤„ç†æ—¶è½¬æ¢ä¸º Tensor

### 7.2 ç±»å‹æç¤ºçš„ä»·å€¼

```python
def get_seqs(self, status: Optional[SequenceStatus] = None) -> List[Sequence]:
    """ç±»å‹æç¤ºå¸®åŠ© IDE å’Œ mypy æ£€æŸ¥"""
```

**ä¼˜ç‚¹**ï¼š
- ç¼–è¯‘æ—¶å‘ç°ç±»å‹é”™è¯¯
- IDE è‡ªåŠ¨è¡¥å…¨
- ä»£ç æ›´æ˜“è¯»

---

## 8. ä¸åç»­ Milestone çš„è¿æ¥

### M0 ä¸ºåç»­é˜¶æ®µé¢„ç•™çš„æ¥å£

1. **Sequence.block_ids**: ç”¨äº M3 PagedAttention
2. **CacheConfig.enable_prefix_caching**: ç”¨äº M6 å‰ç¼€ç¼“å­˜
3. **SchedulerConfig.enable_chunked_prefill**: ç”¨äº M5 åˆ†å—é¢„å¡«å……
4. **SamplingParams.logprobs**: ç”¨äº M1+ æ—¥å¿—æ¦‚ç‡è¿”å›

### æ‰©å±•ç‚¹

- æ–°çš„é…ç½®ç±»å¯ä»¥è½»æ¾æ·»åŠ 
- æ–°çš„é‡‡æ ·ç­–ç•¥åªéœ€æ‰©å±• SamplingParams
- æ–°çš„çŠ¶æ€å¯ä»¥åŠ å…¥æšä¸¾ç±»
- æ–°çš„æ¨¡å‹åªéœ€å®ç°åŠ è½½é€»è¾‘

---

## 9. æ€»ç»“

### æ ¸å¿ƒæŠ€æœ¯ç‚¹

1. **é…ç½®ç³»ç»Ÿ**ï¼šåˆ†å±‚è®¾è®¡ï¼Œç±»å‹å®‰å…¨ï¼Œå‚æ•°éªŒè¯
2. **é‡‡æ ·ç­–ç•¥**ï¼šTemperatureã€Top-kã€Top-pã€åœæ­¢æ¡ä»¶
3. **åºåˆ—æŠ½è±¡**ï¼šä¸‰å±‚ç»“æ„ï¼ŒçŠ¶æ€æœºï¼Œfork æœºåˆ¶
4. **æ¨¡å‹åŠ è½½**ï¼šHuggingFace é›†æˆï¼Œdtype é€‰æ‹©ï¼Œè‡ªåŠ¨é…ç½®

### è®¾è®¡åŸåˆ™

1. **å¯¹é½ vLLM**ï¼šå­¦ä¹ æˆç†Ÿæ¡†æ¶çš„è®¾è®¡
2. **æ¸è¿›å¼**ï¼šé¢„ç•™æ‰©å±•æ¥å£
3. **ç±»å‹å®‰å…¨**ï¼šä½¿ç”¨ç±»å‹æç¤ºå’ŒéªŒè¯
4. **æ¨¡å—åŒ–**ï¼šæ¸…æ™°çš„èŒè´£åˆ†ç¦»

### å­¦ä¹ å»ºè®®

1. ç†è§£æ¯ä¸ªç±»çš„èŒè´£å’Œå…³ç³»
2. å¯¹æ¯” vLLM æºç ç†è§£è®¾è®¡æ€è·¯
3. åŠ¨æ‰‹å®éªŒä¸åŒçš„é…ç½®ç»„åˆ
4. é˜…è¯»åç»­ milestone äº†è§£æ¼”è¿›

---

## å‚è€ƒèµ„æ–™

### è®ºæ–‡
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer åŸç†
- [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751) - Top-p Sampling

### ä»£ç 
- [vLLM Official Repo](https://github.com/vllm-project/vllm) - å‚è€ƒå®ç°
- [HuggingFace Transformers](https://github.com/huggingface/transformers) - æ¨¡å‹åº“

### åšå®¢
- [How to generate text: using different decoding methods](https://huggingface.co/blog/how-to-generate)
- [Nucleus Sampling explained](https://towardsdatascience.com/the-curious-case-of-neural-text-degeneration-374f79c5c9a4)

