# Milestone 2: è¿ç»­æ‰¹å¤„ç† - æŠ€æœ¯åŸç†ç¬”è®°

**å­¦ä¹ ç›®æ ‡**: æ·±å…¥ç†è§£è¿ç»­æ‰¹å¤„ç†ï¼ˆContinuous Batchingï¼‰çš„æ ¸å¿ƒåŸç†å’Œå®ç°æŠ€æœ¯

---

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### 1. è¿ç»­æ‰¹å¤„ç† (Continuous Batching)

#### 1.1 ä¼ ç»Ÿæ‰¹å¤„ç†çš„é—®é¢˜

**é™æ€æ‰¹å¤„ç† (Static Batching)**:
```
Batch 1: [Req1, Req2, Req3, Req4]
         â†“
    æ‰€æœ‰è¯·æ±‚åŒæ—¶å¼€å§‹
         â†“
    ç­‰å¾…æœ€é•¿è¯·æ±‚å®Œæˆ
         â†“
    æ‰€æœ‰è¯·æ±‚åŒæ—¶ç»“æŸ
```

**é—®é¢˜**:
- çŸ­è¯·æ±‚å®Œæˆåä»éœ€ç­‰å¾…é•¿è¯·æ±‚
- GPU å¤§é‡æ—¶é—´å¤„äºç©ºé—²çŠ¶æ€
- ååé‡å—æœ€é•¿è¯·æ±‚é™åˆ¶
- èµ„æºåˆ©ç”¨ç‡ä½ï¼ˆé€šå¸¸ 20-40%ï¼‰

#### 1.2 è¿ç»­æ‰¹å¤„ç†çš„è§£å†³æ–¹æ¡ˆ

**è¿­ä»£çº§è°ƒåº¦ (Iteration-level Scheduling)**:
```
Iteration 1: [Req1, Req2, Req3, Req4] â†’ å¤„ç†
Iteration 2: [Req1, Req2, Req5, Req6] â†’ Req3,Req4å®Œæˆï¼ŒåŠ å…¥Req5,Req6
Iteration 3: [Req1, Req7, Req8, Req9] â†’ Req2,Req5,Req6å®Œæˆï¼ŒåŠ å…¥æ–°è¯·æ±‚
...
```

**ä¼˜åŠ¿**:
- å®Œæˆçš„è¯·æ±‚ç«‹å³ç§»é™¤
- æ–°è¯·æ±‚ç«‹å³åŠ å…¥
- åŠ¨æ€ç»´æŠ¤æ»¡è½½æ‰¹æ¬¡
- GPU åˆ©ç”¨ç‡æå‡è‡³ 60-80%
- ååé‡æå‡ 3-5x

### 2. Prefill vs Decode é˜¶æ®µ

#### 2.1 Prefill é˜¶æ®µï¼ˆé¢„å¡«å……ï¼‰

**å®šä¹‰**: å¤„ç†è¾“å…¥ prompt çš„æ‰€æœ‰ tokenï¼Œç”Ÿæˆå¯¹åº”çš„ KV cache

**ç‰¹å¾**:
```python
# è¾“å…¥: å®Œæ•´çš„ prompt tokens
input_tokens = [1, 15, 284, 318, 262, 3139, 286, 4881, 30]  # "What is the capital of France?"

# è¾“å‡º: æ‰€æœ‰ä½ç½®çš„ KV cache + ä¸‹ä¸€ä¸ª token çš„ logits
kv_cache = generate_kv_for_all_positions(input_tokens)
next_token_logits = model_forward(input_tokens, kv_cache)
```

**è®¡ç®—ç‰¹æ€§**:
- **å¹¶è¡Œåº¦é«˜**: æ‰€æœ‰ token ä½ç½®å¯ä»¥å¹¶è¡Œè®¡ç®— attention
- **è®¡ç®—å¯†é›†**: å¤§é‡çŸ©é˜µä¹˜æ³•è¿ç®—
- **å†…å­˜è®¿é—®**: ä¸»è¦æ˜¯æƒé‡è¯»å–
- **æ—¶é—´å¤æ‚åº¦**: O(nÂ²) å…¶ä¸­ n æ˜¯ prompt é•¿åº¦

#### 2.2 Decode é˜¶æ®µï¼ˆè§£ç ï¼‰

**å®šä¹‰**: åŸºäºå·²æœ‰çš„ KV cacheï¼Œé€ä¸ªç”Ÿæˆæ–° token

**ç‰¹å¾**:
```python
# è¾“å…¥: å•ä¸ªæ–° token + å·²æœ‰ KV cache
new_token = 464  # "Paris"
existing_kv_cache = [...] # ä¹‹å‰æ­¥éª¤çš„ KV cache

# è¾“å‡º: æ›´æ–°çš„ KV cache + ä¸‹ä¸€ä¸ª token çš„ logits
updated_kv_cache = append_to_kv_cache(existing_kv_cache, new_token)
next_token_logits = model_forward([new_token], updated_kv_cache)
```

**è®¡ç®—ç‰¹æ€§**:
- **å¹¶è¡Œåº¦ä½**: åªå¤„ç†ä¸€ä¸ª token ä½ç½®
- **å†…å­˜å¯†é›†**: å¤§é‡ KV cache è¯»å†™
- **è®¡ç®—é‡å°**: ç›¸å¯¹è¾ƒå°‘çš„çŸ©é˜µè¿ç®—
- **æ—¶é—´å¤æ‚åº¦**: O(n) å…¶ä¸­ n æ˜¯åºåˆ—é•¿åº¦

#### 2.3 æ··åˆæ‰¹æ¬¡çš„æŒ‘æˆ˜

**é—®é¢˜**: åŒä¸€æ‰¹æ¬¡ä¸­åŒæ—¶å­˜åœ¨ prefill å’Œ decode è¯·æ±‚

```python
# ç¤ºä¾‹æ‰¹æ¬¡
batch = {
    "req-1": {"type": "prefill", "tokens": [1,2,3,4,5]},    # æ–°è¯·æ±‚
    "req-2": {"type": "decode", "tokens": [42]},            # ç»§ç»­ç”Ÿæˆ
    "req-3": {"type": "decode", "tokens": [17]},            # ç»§ç»­ç”Ÿæˆ
    "req-4": {"type": "prefill", "tokens": [10,11,12]},    # æ–°è¯·æ±‚
}
```

**æŒ‘æˆ˜**:
- Prefill éœ€è¦å¤§é‡è®¡ç®—èµ„æº
- Decode éœ€è¦å¤§é‡å†…å­˜å¸¦å®½
- èµ„æºéœ€æ±‚ä¸åŒ¹é…å¯¼è‡´åˆ©ç”¨ç‡ä¸å‡
- éœ€è¦ç²¾å¿ƒè®¾è®¡è°ƒåº¦ç­–ç•¥

### 3. Token é¢„ç®—ç®¡ç†

#### 3.1 è°ƒåº¦çº¦æŸ

**ä¸¤ä¸ªå…³é”®é™åˆ¶**:
```python
class SchedulerConfig:
    max_num_seqs: int = 256           # æœ€å¤§å¹¶å‘åºåˆ—æ•°
    max_num_batched_tokens: int = 2048  # å•æ¬¡è¿­ä»£æœ€å¤§ token æ•°
```

**çº¦æŸåŸå› **:
- `max_num_seqs`: GPU å†…å­˜é™åˆ¶ï¼ˆæ¯ä¸ªåºåˆ—éœ€è¦ KV cacheï¼‰
- `max_num_batched_tokens`: è®¡ç®—èµ„æºé™åˆ¶ï¼ˆå•æ¬¡å‰å‘ä¼ æ’­çš„å·¥ä½œé‡ï¼‰

#### 3.2 è°ƒåº¦ç®—æ³•

**Token é¢„ç®—åˆ†é…**:
```python
def schedule_requests():
    total_tokens = 0
    
    # 1. æ¥çº³æ–°è¯·æ±‚ï¼ˆPrefillï¼‰
    for request in waiting_queue:
        prompt_len = len(request.prompt_tokens)
        if total_tokens + prompt_len > max_num_batched_tokens:
            break  # é¢„ç®—ä¸è¶³
        
        admit_request(request)
        total_tokens += prompt_len
    
    # 2. è°ƒåº¦è¿è¡Œä¸­è¯·æ±‚ï¼ˆDecodeï¼‰
    for request in running_requests:
        if total_tokens + 1 > max_num_batched_tokens:
            break  # é¢„ç®—ä¸è¶³
        
        schedule_decode(request)
        total_tokens += 1
```

**ä¼˜å…ˆçº§ç­–ç•¥**:
1. æ–°è¯·æ±‚ä¼˜å…ˆï¼ˆé¿å…é¥¥é¥¿ï¼‰
2. è¿è¡Œä¸­è¯·æ±‚ä¿è¯ï¼ˆé¿å…ä¸­æ–­ï¼‰
3. FCFS å…¬å¹³è°ƒåº¦

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### 1. è°ƒåº¦å™¨æ¶æ„

#### 1.1 è¯·æ±‚é˜Ÿåˆ—ç®¡ç†

**é˜Ÿåˆ—çŠ¶æ€è½¬æ¢**:
```
   add_request()
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    schedule()    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    finish
â”‚   WAITING   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚   RUNNING   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ FINISHED
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†‘                                â”‚
       â”‚ preempt()                      â”‚ update_from_output()
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**é˜Ÿåˆ—å®ç°**:
```python
class Scheduler:
    def __init__(self):
        self.waiting: FCFSRequestQueue = FCFSRequestQueue()  # ç­‰å¾…é˜Ÿåˆ—
        self.running: List[Request] = []                     # è¿è¡Œé˜Ÿåˆ—
        self.requests: Dict[str, Request] = {}               # æ‰€æœ‰è¯·æ±‚
        self.finished_req_ids: Set[str] = set()              # å·²å®Œæˆè¯·æ±‚
```

#### 1.2 è°ƒåº¦å†³ç­–æµç¨‹

**æ ¸å¿ƒè°ƒåº¦å¾ªç¯**:
```python
def schedule() -> SchedulerOutput:
    # æ­¥éª¤1: æ¥çº³æ–°è¯·æ±‚
    while (len(running) < max_num_seqs and 
           waiting and 
           budget_available()):
        request = waiting.pop()
        running.append(request)
        schedule_prefill(request)
    
    # æ­¥éª¤2: è°ƒåº¦è¿è¡Œä¸­è¯·æ±‚
    for request in running:
        if budget_available():
            schedule_decode(request)
    
    # æ­¥éª¤3: æ„å»ºè°ƒåº¦è¾“å‡º
    return SchedulerOutput(...)
```

### 2. æ‰¹å¤„ç†æ‰§è¡Œæ¶æ„

#### 2.1 è¾“å…¥æ‰¹æ¬¡å‡†å¤‡

**ä¸å®šé•¿åºåˆ—å¤„ç†**:
```python
# åŸå§‹è¾“å…¥ï¼ˆä¸å®šé•¿ï¼‰
raw_inputs = {
    "req-1": [1, 2, 3, 4, 5],      # 5 tokens
    "req-2": [10, 11],             # 2 tokens  
    "req-3": [20, 21, 22],         # 3 tokens
}

# å¡«å……åçš„æ‰¹æ¬¡
padded_batch = {
    "token_ids": [
        [1, 2, 3, 4, 5],           # req-1: æ— éœ€å¡«å……
        [10, 11, 0, 0, 0],         # req-2: å¡«å……3ä¸ª0
        [20, 21, 22, 0, 0],        # req-3: å¡«å……2ä¸ª0
    ],
    "attention_mask": [
        [1, 1, 1, 1, 1],           # req-1: å…¨éƒ¨æœ‰æ•ˆ
        [1, 1, 0, 0, 0],           # req-2: å‰2ä¸ªæœ‰æ•ˆ
        [1, 1, 1, 0, 0],           # req-3: å‰3ä¸ªæœ‰æ•ˆ
    ]
}
```

#### 2.2 KV Cache ç®¡ç†

**M2 å®ç°ç­–ç•¥**:
```python
class ModelRunner:
    def __init__(self):
        # æ¯ä¸ªè¯·æ±‚ç‹¬ç«‹çš„ KV cache
        self.request_caches: Dict[str, Any] = {}
    
    def execute_model_batch(self, input_batch):
        results = {}
        
        # ä¸ºæ¯ä¸ªè¯·æ±‚å•ç‹¬æ‰§è¡Œï¼ˆM2 é™åˆ¶ï¼‰
        for req_id in input_batch.req_ids:
            # è·å–è¯¥è¯·æ±‚çš„ cache
            past_key_values = self.request_caches.get(req_id)
            
            # å•ç‹¬æ‰§è¡Œæ¨¡å‹
            outputs = self.model(
                input_ids=req_tokens,
                past_key_values=past_key_values,
                use_cache=True
            )
            
            # æ›´æ–°è¯¥è¯·æ±‚çš„ cache
            self.request_caches[req_id] = outputs.past_key_values
            results[req_id] = outputs.logits[:, -1, :]
        
        return results
```

**M3+ æ”¹è¿›æ–¹å‘**:
- PagedAttention: çœŸæ­£çš„æ‰¹å¤„ç† KV cache
- å—çº§ç®¡ç†: ç²¾ç¡®çš„å†…å­˜æ§åˆ¶
- å…±äº«æœºåˆ¶: å‰ç¼€ç¼“å­˜æ”¯æŒ

### 3. å¼•æ“åè°ƒæ¶æ„

#### 3.1 EngineCore ä¸»å¾ªç¯

**è¿ç»­æ‰¹å¤„ç†æ ¸å¿ƒæµç¨‹**:
```python
def step() -> Dict[str, RequestOutput]:
    # 1. è°ƒåº¦å†³ç­–
    scheduler_output = self.scheduler.schedule()
    
    # 2. å‡†å¤‡è¾“å…¥
    input_batch = prepare_inputs_from_scheduler_output(scheduler_output)
    
    # 3. æ‰§è¡Œæ¨¡å‹
    logits_dict = self.executor.execute_model_batch(input_batch)
    
    # 4. é‡‡æ ·ç”Ÿæˆ
    sampled_tokens = {}
    for req_id, logits in logits_dict.items():
        request = self.scheduler.requests[req_id]
        next_token = self.sampler.sample(logits, request.sampling_params)
        sampled_tokens[req_id] = next_token
    
    # 5. æ›´æ–°çŠ¶æ€
    outputs = self.scheduler.update_from_output(
        scheduler_output, sampled_tokens
    )
    
    # 6. æ¸…ç†èµ„æº
    for req_id in scheduler_output.finished_req_ids:
        self.executor.free_request_cache(req_id)
    
    return outputs
```

---

## ğŸ”§ å…³é”®ç®—æ³•

### 1. åŠ¨æ€æ‰¹æ¬¡ç»„è£…ç®—æ³•

#### 1.1 è´ªå¿ƒè°ƒåº¦ç®—æ³•

```python
def greedy_schedule(waiting_queue, running_requests, constraints):
    """è´ªå¿ƒè°ƒåº¦ç®—æ³•ï¼šåœ¨çº¦æŸä¸‹æœ€å¤§åŒ–èµ„æºåˆ©ç”¨"""
    
    scheduled_new = []
    scheduled_cached = []
    total_tokens = 0
    
    # é˜¶æ®µ1: æ¥çº³æ–°è¯·æ±‚ï¼ˆæŒ‰ FCFS é¡ºåºï¼‰
    while waiting_queue:
        request = waiting_queue.peek()
        required_tokens = len(request.prompt_tokens)
        
        # æ£€æŸ¥çº¦æŸ
        if (len(running_requests) + len(scheduled_new) >= max_num_seqs or
            total_tokens + required_tokens > max_num_batched_tokens):
            break
        
        # æ¥çº³è¯·æ±‚
        request = waiting_queue.pop()
        scheduled_new.append(request)
        total_tokens += required_tokens
    
    # é˜¶æ®µ2: è°ƒåº¦è¿è¡Œä¸­è¯·æ±‚ï¼ˆæ¯ä¸ª1 tokenï¼‰
    for request in running_requests:
        if total_tokens + 1 <= max_num_batched_tokens:
            scheduled_cached.append(request)
            total_tokens += 1
    
    return scheduled_new, scheduled_cached, total_tokens
```

#### 1.2 è´Ÿè½½å‡è¡¡è€ƒè™‘

**Prefill/Decode æ¯”ä¾‹å¹³è¡¡**:
```python
def balanced_schedule(waiting_queue, running_requests):
    """å¹³è¡¡ prefill å’Œ decode çš„èµ„æºéœ€æ±‚"""
    
    # è®¡ç®—å½“å‰ decode è´Ÿè½½
    decode_load = len(running_requests)
    
    # åŠ¨æ€è°ƒæ•´ prefill æ¥çº³æ•°é‡
    max_prefill_tokens = max_num_batched_tokens - decode_load
    
    # ä¼˜å…ˆä¿è¯ decodeï¼ˆé¿å…ä¸­æ–­ï¼‰
    # å‰©ä½™é¢„ç®—åˆ†é…ç»™ prefill
    return schedule_with_budget(max_prefill_tokens)
```

### 2. åœæ­¢æ¡ä»¶æ£€æµ‹ç®—æ³•

#### 2.1 å¤šç§åœæ­¢æ¡ä»¶

```python
def check_stop_conditions(sequence, sampling_params):
    """æ£€æŸ¥åºåˆ—æ˜¯å¦åº”è¯¥åœæ­¢ç”Ÿæˆ"""
    
    # 1. é•¿åº¦é™åˆ¶
    if (sampling_params.max_tokens and 
        sequence.get_output_len() >= sampling_params.max_tokens):
        return True, "length"
    
    # 2. EOS token
    if (not sampling_params.ignore_eos and 
        sequence.get_last_token_id() == eos_token_id):
        return True, "stop"
    
    # 3. è‡ªå®šä¹‰åœæ­¢ token
    if (sampling_params.stop_token_ids and 
        sequence.get_last_token_id() in sampling_params.stop_token_ids):
        return True, "stop"
    
    # 4. åœæ­¢å­—ç¬¦ä¸²ï¼ˆéœ€è¦è§£ç åæ£€æŸ¥ï¼‰
    if sampling_params.stop_strings:
        decoded_text = decode_tokens(sequence.get_output_tokens())
        for stop_str in sampling_params.stop_strings:
            if stop_str in decoded_text:
                return True, "stop"
    
    return False, None
```

### 3. å†…å­˜ç®¡ç†ç®—æ³•

#### 3.1 KV Cache ç”Ÿå‘½å‘¨æœŸç®¡ç†

```python
class KVCacheManager:
    """M2 ç®€åŒ–ç‰ˆ KV Cache ç®¡ç†"""
    
    def __init__(self):
        self.request_caches = {}
    
    def allocate_cache(self, req_id):
        """ä¸ºæ–°è¯·æ±‚åˆ†é… cache"""
        self.request_caches[req_id] = None
    
    def update_cache(self, req_id, new_cache):
        """æ›´æ–°è¯·æ±‚çš„ cache"""
        self.request_caches[req_id] = new_cache
    
    def free_cache(self, req_id):
        """é‡Šæ”¾å®Œæˆè¯·æ±‚çš„ cache"""
        if req_id in self.request_caches:
            del self.request_caches[req_id]
    
    def get_memory_usage(self):
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆM3+ éœ€è¦ï¼‰"""
        # M2: æ— æ³•ç²¾ç¡®è®¡ç®—
        # M3+: åŸºäºå—çš„ç²¾ç¡®ç»Ÿè®¡
        return len(self.request_caches)
```

---

## ğŸ“Š æ€§èƒ½åˆ†æ

### 1. ç†è®ºæ€§èƒ½æ¨¡å‹

#### 1.1 ååé‡åˆ†æ

**é™æ€æ‰¹å¤„ç†ååé‡**:
```
T_static = B / max(L_1, L_2, ..., L_B)
```
å…¶ä¸­ï¼š
- B: æ‰¹æ¬¡å¤§å°
- L_i: ç¬¬ i ä¸ªè¯·æ±‚çš„é•¿åº¦

**è¿ç»­æ‰¹å¤„ç†ååé‡**:
```
T_continuous = Î£(tokens_per_iteration) / Î£(time_per_iteration)
```

**ç†è®ºæå‡**:
- æœ€ä¼˜æƒ…å†µ: 5-10xï¼ˆå½“è¯·æ±‚é•¿åº¦å·®å¼‚å¾ˆå¤§æ—¶ï¼‰
- å…¸å‹æƒ…å†µ: 3-5xï¼ˆå®é™…å·¥ä½œè´Ÿè½½ï¼‰
- æœ€å·®æƒ…å†µ: 1xï¼ˆæ‰€æœ‰è¯·æ±‚é•¿åº¦ç›¸åŒï¼‰

#### 1.2 å»¶è¿Ÿåˆ†æ

**é¦– Token å»¶è¿Ÿ (TTFT)**:
```
TTFT_continuous = TTFT_single + queue_wait_time + batch_overhead
```

**Token é—´å»¶è¿Ÿ (TPOT)**:
```
TPOT_continuous â‰ˆ TPOT_single  # ç†æƒ³æƒ…å†µä¸‹ç›¸è¿‘
```

**æƒè¡¡**:
- ååé‡å¤§å¹…æå‡
- å»¶è¿Ÿç•¥æœ‰å¢åŠ ï¼ˆå¯æ¥å—ï¼‰
- æ•´ä½“æ•ˆç‡æ˜¾è‘—æ”¹å–„

### 2. èµ„æºåˆ©ç”¨ç‡åˆ†æ

#### 2.1 GPU åˆ©ç”¨ç‡

**è®¡ç®—åˆ©ç”¨ç‡**:
```python
def compute_utilization():
    prefill_ops = sum(seq_len^2 for prefill_requests)
    decode_ops = sum(seq_len for decode_requests)
    total_ops = prefill_ops + decode_ops
    
    # GPU å³°å€¼ç®—åŠ›
    peak_ops = gpu_flops * time_per_iteration
    
    return total_ops / peak_ops
```

**å†…å­˜å¸¦å®½åˆ©ç”¨ç‡**:
```python
def memory_utilization():
    kv_cache_reads = sum(seq_len * hidden_dim for decode_requests)
    weight_reads = model_size * num_requests
    total_memory_ops = kv_cache_reads + weight_reads
    
    # GPU å³°å€¼å¸¦å®½
    peak_bandwidth = gpu_bandwidth * time_per_iteration
    
    return total_memory_ops / peak_bandwidth
```

#### 2.2 ç“¶é¢ˆåˆ†æ

**è®¡ç®—ç“¶é¢ˆåœºæ™¯**:
- å¤§é‡ prefill è¯·æ±‚
- é•¿ prompt å¤„ç†
- æ¨¡å‹å‚æ•°é‡å¤§

**å†…å­˜ç“¶é¢ˆåœºæ™¯**:
- å¤§é‡ decode è¯·æ±‚
- é•¿åºåˆ—ç”Ÿæˆ
- KV cache è®¿é—®å¯†é›†

**ä¼˜åŒ–ç­–ç•¥**:
- åŠ¨æ€è°ƒæ•´ prefill/decode æ¯”ä¾‹
- æ™ºèƒ½æ‰¹æ¬¡ç»„è£…
- å¼‚æ­¥å¤„ç†æµæ°´çº¿

---

## ğŸ”® M3+ æ‰©å±•æ–¹å‘

### 1. PagedAttention åŸç†

#### 1.1 è™šæ‹Ÿå†…å­˜æ€æƒ³

**ä¼ ç»Ÿ KV Cache**:
```
Request 1: [K1, V1, K2, V2, K3, V3, ...]  # è¿ç»­å­˜å‚¨
Request 2: [K1, V1, K2, V2, ...]          # è¿ç»­å­˜å‚¨
```

**PagedAttention**:
```
Block Pool: [Block0, Block1, Block2, Block3, ...]

Request 1 Block Table: [0, 2, 5, ...]  # æŒ‡å‘ç‰©ç†å—
Request 2 Block Table: [1, 3, ...]     # æŒ‡å‘ç‰©ç†å—
```

#### 1.2 æ ¸å¿ƒä¼˜åŠ¿

**å†…å­˜æ•ˆç‡**:
- æ¶ˆé™¤å†…éƒ¨ç¢ç‰‡
- æ”¯æŒåŠ¨æ€åˆ†é…
- å†…å­˜åˆ©ç”¨ç‡æ¥è¿‘ 100%

**å…±äº«èƒ½åŠ›**:
- å‰ç¼€å…±äº«ï¼ˆå¤šä¸ªè¯·æ±‚å…±äº«ç›¸åŒå‰ç¼€ï¼‰
- Copy-on-Write æœºåˆ¶
- æ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨

### 2. æŠ¢å å’Œäº¤æ¢æœºåˆ¶

#### 2.1 æŠ¢å ç­–ç•¥

```python
def preemption_policy():
    """å†…å­˜ä¸è¶³æ—¶çš„æŠ¢å ç­–ç•¥"""
    
    # å€™é€‰æŠ¢å è¯·æ±‚
    candidates = [req for req in running_requests 
                 if req.can_be_preempted()]
    
    # æŠ¢å ç­–ç•¥ï¼ˆå¤šç§é€‰æ‹©ï¼‰
    if policy == "LRU":
        return min(candidates, key=lambda r: r.last_access_time)
    elif policy == "shortest_remaining":
        return min(candidates, key=lambda r: r.remaining_tokens)
    elif policy == "lowest_priority":
        return min(candidates, key=lambda r: r.priority)
```

#### 2.2 äº¤æ¢æœºåˆ¶

```python
def swap_out_request(request):
    """å°†è¯·æ±‚çš„ KV cache äº¤æ¢åˆ° CPU"""
    
    # 1. å¤åˆ¶ KV cache åˆ° CPU
    cpu_cache = copy_to_cpu(request.kv_cache_blocks)
    
    # 2. é‡Šæ”¾ GPU å†…å­˜
    free_gpu_blocks(request.kv_cache_blocks)
    
    # 3. æ›´æ–°è¯·æ±‚çŠ¶æ€
    request.status = RequestStatus.SWAPPED
    request.cpu_cache = cpu_cache

def swap_in_request(request):
    """å°†è¯·æ±‚çš„ KV cache ä» CPU æ¢å¤åˆ° GPU"""
    
    # 1. åˆ†é… GPU å—
    gpu_blocks = allocate_gpu_blocks(request.num_blocks)
    
    # 2. ä» CPU å¤åˆ¶æ•°æ®
    copy_from_cpu(request.cpu_cache, gpu_blocks)
    
    # 3. æ›´æ–°è¯·æ±‚çŠ¶æ€
    request.status = RequestStatus.RUNNING
    request.kv_cache_blocks = gpu_blocks
```

---

## ğŸ“ å­¦ä¹ è¦ç‚¹æ€»ç»“

### æ ¸å¿ƒæ¦‚å¿µæŒæ¡

1. **è¿ç»­æ‰¹å¤„ç† vs é™æ€æ‰¹å¤„ç†**
   - è¿­ä»£çº§è°ƒåº¦çš„ä¼˜åŠ¿
   - åŠ¨æ€æ‰¹æ¬¡ç»´æŠ¤æœºåˆ¶
   - æ€§èƒ½æå‡åŸç†

2. **Prefill vs Decode**
   - è®¡ç®—ç‰¹æ€§å·®å¼‚
   - èµ„æºéœ€æ±‚ä¸åŒ
   - æ··åˆæ‰¹æ¬¡æŒ‘æˆ˜

3. **è°ƒåº¦ç®—æ³•**
   - Token é¢„ç®—ç®¡ç†
   - è´ªå¿ƒè°ƒåº¦ç­–ç•¥
   - è´Ÿè½½å‡è¡¡è€ƒè™‘

4. **ç³»ç»Ÿæ¶æ„**
   - è°ƒåº¦å™¨è®¾è®¡
   - æ‰¹å¤„ç†æ‰§è¡Œ
   - å¼•æ“åè°ƒ

### å®ç°æŠ€å·§

1. **é˜Ÿåˆ—ç®¡ç†**
   - FCFS å…¬å¹³è°ƒåº¦
   - çŠ¶æ€è½¬æ¢ç®¡ç†
   - æŠ¢å æ¢å¤æœºåˆ¶

2. **æ‰¹æ¬¡å¤„ç†**
   - ä¸å®šé•¿åºåˆ—å¡«å……
   - Attention mask ç”Ÿæˆ
   - KV cache ç‹¬ç«‹ç®¡ç†

3. **èµ„æºä¼˜åŒ–**
   - å†…å­˜ç”Ÿå‘½å‘¨æœŸç®¡ç†
   - è®¡ç®—èµ„æºè°ƒåº¦
   - ç“¶é¢ˆè¯†åˆ«å’Œä¼˜åŒ–

### æ‰©å±•æ–¹å‘

1. **PagedAttention**
   - è™šæ‹Ÿå†…å­˜æ€æƒ³
   - å—çº§ç®¡ç†æœºåˆ¶
   - å‰ç¼€å…±äº«èƒ½åŠ›

2. **é«˜çº§è°ƒåº¦**
   - æŠ¢å å’Œäº¤æ¢
   - ä¼˜å…ˆçº§è°ƒåº¦
   - SLA æ„ŸçŸ¥è°ƒåº¦

3. **æ€§èƒ½ä¼˜åŒ–**
   - å¼‚æ­¥å¤„ç†
   - æµæ°´çº¿å¹¶è¡Œ
   - ç¡¬ä»¶åŠ é€Ÿ

---

**å­¦ä¹ å»ºè®®**:
1. å…ˆç†è§£è¿ç»­æ‰¹å¤„ç†çš„æ ¸å¿ƒæ€æƒ³
2. æ·±å…¥åˆ†æ Prefill/Decode çš„å·®å¼‚
3. æŒæ¡è°ƒåº¦ç®—æ³•çš„è®¾è®¡åŸç†
4. ç†è§£ç³»ç»Ÿæ¶æ„çš„åè°ƒæœºåˆ¶
5. æ€è€ƒ M3+ çš„æ‰©å±•æ–¹å‘

è¿™äº›æ¦‚å¿µå’ŒæŠ€æœ¯æ˜¯ç°ä»£ LLM æ¨ç†ç³»ç»Ÿçš„åŸºç¡€ï¼ŒæŒæ¡å®ƒä»¬å¯¹äºç†è§£å’Œå¼€å‘é«˜æ€§èƒ½æ¨ç†æ¡†æ¶è‡³å…³é‡è¦ã€‚
