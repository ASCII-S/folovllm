# Milestone 1 é¢è¯•æŒ‡å—

> æœ¬æ–‡æ¡£æ•´ç† M1 åŸºç¡€ç¦»çº¿æ¨ç†é˜¶æ®µå¯èƒ½é‡åˆ°çš„é¢è¯•é—®é¢˜åŠå›ç­”è¦ç‚¹

---

## ğŸ“‹ ç›®å½•

1. [KV Cache ç›¸å…³](#1-kv-cache-ç›¸å…³)
2. [Attention æœºåˆ¶ç›¸å…³](#2-attention-æœºåˆ¶ç›¸å…³)
3. [ä½ç½®ç¼–ç ç›¸å…³](#3-ä½ç½®ç¼–ç ç›¸å…³)
4. [é‡‡æ ·ç­–ç•¥ç›¸å…³](#4-é‡‡æ ·ç­–ç•¥ç›¸å…³)
5. [æ¨¡å‹æ¶æ„ç›¸å…³](#5-æ¨¡å‹æ¶æ„ç›¸å…³)
6. [æ¨ç†ä¼˜åŒ–ç›¸å…³](#6-æ¨ç†ä¼˜åŒ–ç›¸å…³)
7. [ç³»ç»Ÿè®¾è®¡ç›¸å…³](#7-ç³»ç»Ÿè®¾è®¡ç›¸å…³)
8. [æ•°å€¼ç¨³å®šæ€§ç›¸å…³](#8-æ•°å€¼ç¨³å®šæ€§ç›¸å…³)

---

## 1. KV Cache ç›¸å…³

### Q1.1: ä»€ä¹ˆæ˜¯ KV Cacheï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**é—®é¢˜èƒŒæ™¯**ï¼š
- Transformer ç”Ÿæˆç¬¬ N ä¸ª token æ—¶ï¼Œéœ€è¦è®¡ç®—å®ƒä¸å‰ N-1 ä¸ª token çš„ attention
- å‰ N-1 ä¸ª token çš„ K å’Œ V åœ¨æ¯ä¸€æ­¥éƒ½é‡æ–°è®¡ç®—æ˜¯æµªè´¹çš„

**KV Cache åŸç†**ï¼š
- ç¼“å­˜å·²è®¡ç®—çš„ K å’Œ V çŸ©é˜µ
- æ¯æ¬¡åªè®¡ç®—æ–° token çš„ K å’Œ Vï¼Œè¿½åŠ åˆ°ç¼“å­˜

**æ•°å­¦è¡¨è¾¾**ï¼š
```
Without cache:
  Step 1: Qâ‚ @ [Kâ‚]áµ€
  Step 2: Qâ‚‚ @ [Kâ‚, Kâ‚‚]áµ€  â† Kâ‚ é‡å¤è®¡ç®—
  Step 3: Qâ‚ƒ @ [Kâ‚, Kâ‚‚, Kâ‚ƒ]áµ€  â† Kâ‚, Kâ‚‚ é‡å¤è®¡ç®—

With cache:
  Step 1: Qâ‚ @ [Kâ‚]áµ€, cache=[Kâ‚]
  Step 2: Qâ‚‚ @ [Kâ‚, Kâ‚‚]áµ€, cache=[Kâ‚, Kâ‚‚]  â† Kâ‚ ä» cache è¯»å–
  Step 3: Qâ‚ƒ @ [Kâ‚, Kâ‚‚, Kâ‚ƒ]áµ€, cache=[Kâ‚, Kâ‚‚, Kâ‚ƒ]  â† Kâ‚, Kâ‚‚ ä» cache è¯»å–
```

**æ€§èƒ½æå‡**ï¼š
- æ—¶é—´å¤æ‚åº¦ï¼š$O(n^2)$ â†’ $O(n)$ï¼ˆn ä¸ºåºåˆ—é•¿åº¦ï¼‰
- å®é™…åŠ é€Ÿï¼š10-50xï¼ˆå–å†³äºåºåˆ—é•¿åº¦ï¼‰

---

### Q1.2: KV Cache çš„å†…å­˜å¼€é”€å¦‚ä½•è®¡ç®—ï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**å•å±‚ Cache å¤§å°**ï¼š
```python
# æ¯å±‚çš„ KV cache
memory_per_layer = 2 * batch_size * num_kv_heads * seq_len * head_dim * bytes_per_element

# ç¤ºä¾‹ï¼šQwen3-0.6B (float16)
# - num_kv_heads = 2
# - head_dim = 64
# - seq_len = 2048
# - batch_size = 1
# - bytes_per_element = 2 (float16)

memory_per_layer = 2 * 1 * 2 * 2048 * 64 * 2 = 1,048,576 bytes = 1 MB
```

**å…¨æ¨¡å‹ Cache**ï¼š
```python
# Qwen3-0.6B æœ‰ 28 å±‚
total_memory = 28 * 1 MB = 28 MB (per request, seq_len=2048)
```

**æ‰¹å¤„ç†åœºæ™¯**ï¼š
```python
# batch_size = 16, seq_len = 2048
total_memory = 28 * 16 MB = 448 MB
```

**è¿½é—®ï¼šå¦‚ä½•å‡å°‘ KV Cache å†…å­˜ï¼Ÿ**

**å›ç­”**ï¼š
1. **GQA (Grouped Query Attention)**ï¼šå‡å°‘ KV heads
   - MHA: 16 Q heads, 16 KV heads
   - GQA: 16 Q heads, 2 KV heads â†’ 8x å†…å­˜å‡å°‘
2. **MQA (Multi-Query Attention)**ï¼šæ‰€æœ‰ Q å…±äº«ä¸€ä¸ª KV
   - 16 Q heads, 1 KV head â†’ 16x å†…å­˜å‡å°‘
3. **Paged Attention** (M3)ï¼šåˆ†é¡µç®¡ç†ï¼Œé¿å…è¿ç»­å†…å­˜
4. **Quantization**ï¼šint8 æˆ– int4 KV cache

---

### Q1.3: Prefill å’Œ Decode é˜¶æ®µçš„ KV Cache å¦‚ä½•ä½¿ç”¨ï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**Prefill é˜¶æ®µ**ï¼ˆå¤„ç† promptï¼‰ï¼š
```python
# è¾“å…¥: "Hello, how are you?" â†’ [token1, token2, ..., token6]
input_ids = [9906, 11, 1268, 527, 499, 30]  # shape: [1, 6]
positions = [0, 1, 2, 3, 4, 5]

# ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ token çš„ Q, K, V
Q = [Qâ‚€, Qâ‚, Qâ‚‚, Qâ‚ƒ, Qâ‚„, Qâ‚…]  # shape: [1, 16, 6, 64]
K = [Kâ‚€, Kâ‚, Kâ‚‚, Kâ‚ƒ, Kâ‚„, Kâ‚…]  # shape: [1, 2, 6, 64]
V = [Vâ‚€, Vâ‚, Vâ‚‚, Vâ‚ƒ, Vâ‚„, Vâ‚…]  # shape: [1, 2, 6, 64]

# Attention è®¡ç®—ï¼ˆéœ€è¦ causal maskï¼‰
attn = softmax(Q @ Káµ€ + mask) @ V

# åˆå§‹åŒ– cache
kv_cache = (K, V)  # shape: [1, 2, 6, 64]
```

**Decode é˜¶æ®µ**ï¼ˆç”Ÿæˆæ¯ä¸ª tokenï¼‰ï¼š
```python
# Step 1: ç”Ÿæˆç¬¬ 7 ä¸ª token
input_ids = [358]  # shape: [1, 1]
positions = [6]

# åªè®¡ç®—æ–° token çš„ Q, K, V
Qâ‚† = ...  # shape: [1, 16, 1, 64]
Kâ‚† = ...  # shape: [1, 2, 64] (3D!)
Vâ‚† = ...  # shape: [1, 2, 64]

# è¿½åŠ åˆ° cache
K_cached = [Kâ‚€, Kâ‚, Kâ‚‚, Kâ‚ƒ, Kâ‚„, Kâ‚…, Kâ‚†]  # shape: [1, 2, 7, 64]
V_cached = [Vâ‚€, Vâ‚, Vâ‚‚, Vâ‚ƒ, Vâ‚„, Vâ‚…, Vâ‚†]  # shape: [1, 2, 7, 64]

# Attentionï¼ˆä¸éœ€è¦ maskï¼Œå¯ä»¥çœ‹æ‰€æœ‰å†å²ï¼‰
attn = softmax(Qâ‚† @ K_cachedáµ€) @ V_cached

# Step 2: ç”Ÿæˆç¬¬ 8 ä¸ª token
Qâ‚‡ @ [Kâ‚€, ..., Kâ‚†, Kâ‚‡]áµ€ @ [Vâ‚€, ..., Vâ‚†, Vâ‚‡]
...
```

**å…³é”®åŒºåˆ«**ï¼š

| é˜¶æ®µ    | è¾“å…¥å½¢çŠ¶         | KV å½¢çŠ¶                                | Mask             | è®¡ç®—é‡   |
| ------- | ---------------- | -------------------------------------- | ---------------- | -------- |
| Prefill | [batch, seq_len] | [batch, heads, seq_len, dim] (4D)      | éœ€è¦ causal mask | $O(n^2)$ |
| Decode  | [batch, 1]       | [batch, heads, dim] (3D) â†’ è¿½åŠ åå˜ 4D | ä¸éœ€è¦           | $O(n)$   |

---

### Q1.4: ä¸ºä»€ä¹ˆ M1 ä½¿ç”¨è¿ç»­å†…å­˜ KV Cacheï¼Œè€Œ M3 è¦æ”¹ç”¨ Paged Attentionï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**M1 æ–¹æ¡ˆï¼ˆè¿ç»­å†…å­˜ï¼‰**ï¼š
```python
# æ¯æ¬¡è¿½åŠ éƒ½åˆ›å»ºæ–° tensor
key_cache = torch.cat([key_cache, new_key], dim=2)

é—®é¢˜ï¼š
1. éœ€è¦è¿ç»­å†…å­˜å—
2. torch.cat ä¼šå¤åˆ¶æ‰€æœ‰æ•°æ®
3. å†…å­˜ç¢ç‰‡åŒ–
4. æœ€å¤§åºåˆ—é•¿åº¦å—é™
```

**å†…å­˜ç¤ºä¾‹**ï¼š
```
Request 1: [Kâ‚€, Kâ‚, Kâ‚‚, Kâ‚ƒ, ...] (2048 tokens)
Request 2: [Kâ‚€, Kâ‚, Kâ‚‚, ...] (512 tokens, ä½†é¢„ç•™ 2048)
Request 3: [Kâ‚€, Kâ‚, ...] (256 tokens, ä½†é¢„ç•™ 2048)

æµªè´¹çš„å†…å­˜ = (2048-512) + (2048-256) = 3328 tokens
```

**M3 æ–¹æ¡ˆï¼ˆPaged Attentionï¼‰**ï¼š
```python
# åˆ†é¡µç®¡ç†ï¼Œç±»ä¼¼æ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜
Page 0: [Kâ‚€, Kâ‚, ..., Kâ‚â‚…]  # 16 tokens per page
Page 1: [Kâ‚â‚†, Kâ‚â‚‡, ..., Kâ‚ƒâ‚]
...

# æŒ‰éœ€åˆ†é…ï¼Œæ— æµªè´¹
Request 1: [Page0, Page1, ..., Page127]  # 2048 tokens = 128 pages
Request 2: [Page128, ..., Page159]        # 512 tokens = 32 pages
Request 3: [Page160, ..., Page175]        # 256 tokens = 16 pages
```

**ä¼˜åŠ¿**ï¼š
- âœ… å†…å­˜åˆ©ç”¨ç‡æ¥è¿‘ 100%
- âœ… æ”¯æŒå˜é•¿åºåˆ—
- âœ… ä¾¿äºå¤šè¯·æ±‚å…±äº«ï¼ˆshared prefixï¼‰
- âœ… é¿å…å†…å­˜ç¢ç‰‡

---

## 2. Attention æœºåˆ¶ç›¸å…³

### Q2.1: ä¸ºä»€ä¹ˆ Attention è¦é™¤ä»¥ $\sqrt{d_k}$ï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**æ•°å­¦æ¨å¯¼**ï¼š

å‡è®¾ $Q$ å’Œ $K$ çš„å…ƒç´ æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„éšæœºå˜é‡ï¼Œå‡å€¼ 0ï¼Œæ–¹å·® 1ï¼š
- $Q, K \sim \mathcal{N}(0, 1)$

ç‚¹ç§¯ç»“æœï¼š
$$\text{score} = Q \cdot K = \sum_{i=1}^{d_k} Q_i K_i$$

æ ¹æ®ä¸­å¿ƒæé™å®šç†ï¼š
- $\mathbb{E}[\text{score}] = 0$
- $\text{Var}[\text{score}] = d_k$ï¼ˆå› ä¸º $d_k$ ä¸ªç‹¬ç«‹å˜é‡ç›¸åŠ ï¼‰

**é—®é¢˜**ï¼š
- å½“ $d_k$ å¾ˆå¤§æ—¶ï¼ˆå¦‚ 64ã€128ï¼‰ï¼Œæ–¹å·®ä¼šå¾ˆå¤§
- Softmax è¾“å…¥æ–¹å·®å¤§ â†’ è¾“å‡ºæ¥è¿‘ one-hot â†’ æ¢¯åº¦æ¥è¿‘ 0

**è§£å†³**ï¼š
$$\text{score}_{\text{scaled}} = \frac{Q \cdot K}{\sqrt{d_k}}$$

æ­¤æ—¶ï¼š
$$\text{Var}[\text{score}_{\text{scaled}}] = \frac{d_k}{d_k} = 1$$

**å®éªŒéªŒè¯**ï¼š
```python
# ä¸ scale
d_k = 64
scores = torch.randn(1, 64) @ torch.randn(64, 100)
print(scores.var())  # ~64

# Scale
scores_scaled = scores / (64 ** 0.5)
print(scores_scaled.var())  # ~1
```

**è¿½é—®ï¼šæœ‰æ²¡æœ‰å…¶ä»– scale æ–¹æ³•ï¼Ÿ**

**å›ç­”**ï¼š
1. **å¯å­¦ä¹  scale**ï¼š$\text{score} = Q \cdot K / \alpha$ï¼Œå…¶ä¸­ $\alpha$ æ˜¯å¯å­¦ä¹ å‚æ•°
2. **å›ºå®š scale**ï¼šæŸäº›æ¨¡å‹ç”¨å›ºå®šå€¼ï¼ˆå¦‚ 8ï¼‰
3. **QK Norm**ï¼šå¯¹ Q å’Œ K åˆ†åˆ«åš LayerNormï¼ˆQwen3 ä½¿ç”¨ï¼‰

---

### Q2.2: ä»€ä¹ˆæ˜¯ Grouped Query Attention (GQA)ï¼Ÿä¸ºä»€ä¹ˆè¦ç”¨å®ƒï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**ä¸‰ç§ Attention æ¨¡å¼**ï¼š

1. **MHA (Multi-Head Attention)**ï¼š
```python
num_heads = 16
num_kv_heads = 16  # æ¯ä¸ª head ç‹¬ç«‹çš„ K, V

Q: [batch, 16, seq, 64]
K: [batch, 16, seq, 64]  # 16 ä»½ç‹¬ç«‹çš„ K
V: [batch, 16, seq, 64]  # 16 ä»½ç‹¬ç«‹çš„ V
```

2. **MQA (Multi-Query Attention)**ï¼š
```python
num_heads = 16
num_kv_heads = 1  # æ‰€æœ‰ head å…±äº« K, V

Q: [batch, 16, seq, 64]
K: [batch, 1, seq, 64]  # åªæœ‰ 1 ä»½ K
V: [batch, 1, seq, 64]  # åªæœ‰ 1 ä»½ V
```

3. **GQA (Grouped Query Attention)**ï¼š
```python
num_heads = 16
num_kv_heads = 2  # 8 ä¸ª Q heads å…±äº« 1 ä¸ª KV head

Q: [batch, 16, seq, 64]
K: [batch, 2, seq, 64]  # 2 ä»½ K
V: [batch, 2, seq, 64]  # 2 ä»½ V

# Q heads åˆ†ç»„
Group 0: Q[0:8] ä½¿ç”¨ K[0], V[0]
Group 1: Q[8:16] ä½¿ç”¨ K[1], V[1]
```

**æ€§èƒ½å¯¹æ¯”**ï¼š

| æ¨¡å¼ | KV Cache å†…å­˜ | è´¨é‡ | æ¨ç†é€Ÿåº¦ |
| ---- | ------------- | ---- | -------- |
| MHA  | 100%          | æœ€å¥½ | æ…¢       |
| GQA  | 12.5% (2/16)  | å¾ˆå¥½ | å¿«       |
| MQA  | 6.25% (1/16)  | å¥½   | æœ€å¿«     |

**å®ç°ç»†èŠ‚**ï¼š
```python
# GQA æ—¶éœ€è¦é‡å¤ KV heads
batch_size, num_heads, seq_len, head_dim = query.shape
_, num_kv_heads, _, _ = key.shape

if num_heads > num_kv_heads:
    # é‡å¤ KV
    num_repeats = num_heads // num_kv_heads
    key = key.unsqueeze(2).expand(
        batch_size, num_kv_heads, num_repeats, seq_len, head_dim
    ).reshape(batch_size, num_heads, seq_len, head_dim)
    # value åŒç†
```

**ä¸ºä»€ä¹ˆ GQA æœ‰æ•ˆ**ï¼š
- Q ä¸»è¦è´Ÿè´£"æŸ¥è¯¢"ï¼ˆå¤šæ ·æ€§é‡è¦ï¼‰
- K, V ä¸»è¦è´Ÿè´£"å†…å®¹"ï¼ˆå¯ä»¥å…±äº«ï¼‰
- å®éªŒè¡¨æ˜ GQA è´¨é‡æ¥è¿‘ MHAï¼Œä½†å†…å­˜å’Œé€Ÿåº¦å¤§å¹…æå‡

---

### Q2.3: Causal Mask æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•å®ç°ï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**å®šä¹‰**ï¼š
- Causal Mask ç¡®ä¿æ¯ä¸ª token åªèƒ½çœ‹åˆ°**è‡ªå·±å’Œä¹‹å‰**çš„ token
- é˜²æ­¢ä¿¡æ¯æ³„éœ²ï¼ˆç”Ÿæˆæ—¶ä¸èƒ½çœ‹æœªæ¥ï¼‰

**å®ç°**ï¼š
```python
def create_causal_mask(seq_len_q, seq_len_k):
    # åˆ›å»ºå…¨ 1 çŸ©é˜µ
    mask = torch.ones(seq_len_q, seq_len_k)
    
    # ä¿ç•™ä¸Šä¸‰è§’ï¼ˆä¸åŒ…æ‹¬å¯¹è§’çº¿ï¼‰
    mask = torch.triu(mask, diagonal=seq_len_k - seq_len_q + 1)
    
    # å°† 1 æ›¿æ¢ä¸º -infï¼ˆç¦æ­¢ attendï¼‰
    mask = mask.masked_fill(mask == 1, float('-inf'))
    
    return mask
```

**ç¤ºä¾‹**ï¼š
```python
# Prefill: seq_len_q = seq_len_k = 4
mask = create_causal_mask(4, 4)
"""
[[  0, -âˆ, -âˆ, -âˆ],
 [  0,   0, -âˆ, -âˆ],
 [  0,   0,   0, -âˆ],
 [  0,   0,   0,   0]]

è§£é‡Šï¼š
- Token 0 åªèƒ½çœ‹ Token 0
- Token 1 å¯ä»¥çœ‹ Token 0, 1
- Token 2 å¯ä»¥çœ‹ Token 0, 1, 2
- Token 3 å¯ä»¥çœ‹ Token 0, 1, 2, 3
"""

# Decode: seq_len_q = 1, seq_len_k = 5
mask = create_causal_mask(1, 5)
"""
[[0, 0, 0, 0, 0]]

è§£é‡Šï¼šæ–° token å¯ä»¥çœ‹æ‰€æœ‰å†å² token
"""
```

**åº”ç”¨**ï¼š
```python
# Attention è®¡ç®—
attn_weights = Q @ K.transpose(-2, -1)
attn_weights = attn_weights + mask  # åŠ  mask
attn_weights = F.softmax(attn_weights, dim=-1)
```

**ä¸ºä»€ä¹ˆåŠ  -inf**ï¼š
$$\text{softmax}(x_i + (-\infty)) = \frac{e^{x_i} \cdot e^{-\infty}}{\sum_j e^{x_j}} = \frac{e^{x_i} \cdot 0}{\sum_j e^{x_j}} = 0$$

**è¿½é—®ï¼šEncoder-Decoder æ¨¡å‹çš„ mask æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ**

**å›ç­”**ï¼š
- **Encoder**: åŒå‘ attentionï¼Œä¸éœ€è¦ causal mask
- **Decoder Self-Attention**: Causal maskï¼ˆåŒä¸Šï¼‰
- **Decoder Cross-Attention**: ä¸éœ€è¦ causal maskï¼ˆå¯ä»¥çœ‹å®Œæ•´ encoder è¾“å‡ºï¼‰

---

## 3. ä½ç½®ç¼–ç ç›¸å…³

### Q3.1: RoPE æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆæ¯”ä¼ ç»Ÿä½ç½®ç¼–ç æ›´å¥½ï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**ä¼ ç»Ÿä½ç½®ç¼–ç ï¼ˆSinusoidal PEï¼‰**ï¼š
```python
# åœ¨è¾“å…¥åŠ ä½ç½®ç¼–ç 
x = embedding(tokens) + positional_encoding(positions)
```

**é—®é¢˜**ï¼š
- ä½ç½®ä¿¡æ¯åœ¨æ·±å±‚ç½‘ç»œä¸­ä¼šè¢«ç¨€é‡Š
- å¤–æ¨æ€§å·®ï¼ˆè®­ç»ƒ 512ï¼Œæ¨ç† 2048 ä¼šå¤±æ•ˆï¼‰

**RoPE (Rotary Position Embedding)**ï¼š
```python
# åœ¨ Attention ä¸­æ—‹è½¬ Q å’Œ K
Q_rot = rotate(Q, position)
K_rot = rotate(K, position)
attn = softmax(Q_rot @ K_rotáµ€) @ V
```

**æ ¸å¿ƒæ€æƒ³**ï¼š
- å°†ä½ç½®ä¿¡æ¯ç¼–ç ä¸º**æ—‹è½¬**
- åœ¨å¤å¹³é¢ä¸Šæ—‹è½¬å‘é‡

**æ•°å­¦åŸç†**ï¼š

å¯¹äºä½ç½® $m$ å’Œ $n$ï¼š
$$\text{score}(m, n) = q_m^T k_n = (R_m q)^T (R_n k) = q^T R_m^T R_n k = q^T R_{m-n} k$$

å…¶ä¸­ $R_\theta$ æ˜¯æ—‹è½¬çŸ©é˜µï¼š
$$R_\theta = \begin{bmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{bmatrix}$$

**ç»“è®º**ï¼šAttention score åªä¾èµ–**ç›¸å¯¹ä½ç½®** $m-n$ï¼

**å®ç°**ï¼š
```python
# é¢„è®¡ç®—æ—‹è½¬é¢‘ç‡
inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
# base = 10000, dim = 64
# inv_freq = [1.0, 0.1, 0.01, 0.001, ...]

# è®¡ç®— cos å’Œ sin
t = torch.arange(seq_len)  # [0, 1, 2, ...]
freqs = torch.outer(t, inv_freq)  # [seq_len, dim//2]
cos = freqs.cos()
sin = freqs.sin()

# åº”ç”¨æ—‹è½¬
x1, x2 = x.chunk(2, dim=-1)
x_rot = torch.cat([
    x1 * cos - x2 * sin,
    x1 * sin + x2 * cos,
], dim=-1)
```

**ä¼˜åŠ¿**ï¼š
1. âœ… **ç›¸å¯¹ä½ç½®**ï¼šè‡ªç„¶ç¼–ç ç›¸å¯¹ä½ç½®ä¿¡æ¯
2. âœ… **å¤–æ¨æ€§**ï¼šå¯ä»¥æ¨ç†æ›´é•¿åºåˆ—
3. âœ… **æ— å‚æ•°**ï¼šä¸å¢åŠ æ¨¡å‹å‚æ•°
4. âœ… **ä¿æŒå†…ç§¯**ï¼šä¸ç ´å QÂ·K çš„è¯­ä¹‰

**è¿½é—®ï¼šRoPE å¦‚ä½•æ”¯æŒé•¿åºåˆ—å¤–æ¨ï¼Ÿ**

**å›ç­”**ï¼š
```python
# æ–¹æ³•1ï¼šçº¿æ€§æ’å€¼
rope_scaling = {"type": "linear", "factor": 2.0}
# åŸæœ¬ max_len=2048ï¼Œç°åœ¨æ”¯æŒ 4096

# æ–¹æ³•2ï¼šNTK-aware scaling
rope_scaling = {"type": "ntk", "factor": 2.0}
# è°ƒæ•´ base å€¼ï¼Œæ›´å¥½çš„å¤–æ¨æ€§

# æ–¹æ³•3ï¼šYaRN (Yet another RoPE extensioN)
rope_scaling = {"type": "yarn", "factor": 4.0}
# æ··åˆå¤šç§ç­–ç•¥
```

---

### Q3.2: ä¸ºä»€ä¹ˆ RoPE åªåº”ç”¨åœ¨ Q å’Œ Kï¼Œä¸åº”ç”¨åœ¨ Vï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**Attention æœºåˆ¶åˆ†è§£**ï¼š
$$\text{Attention}(Q, K, V) = \underbrace{\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)}_{\text{æƒé‡è®¡ç®—}} \underbrace{V}_{\text{å†…å®¹èšåˆ}}$$

**ä½ç½®ä¿¡æ¯çš„ä½œç”¨**ï¼š
- **QK è®¡ç®—**ï¼šå†³å®š"å“ªäº›ä½ç½®é‡è¦"ï¼ˆéœ€è¦ä½ç½®ä¿¡æ¯ï¼‰
- **V è®¡ç®—**ï¼šèšåˆ"å†…å®¹"ï¼ˆä¸éœ€è¦ä½ç½®ä¿¡æ¯ï¼‰

**æ•°å­¦éªŒè¯**ï¼š

å¦‚æœå¯¹ V ä¹Ÿåº”ç”¨æ—‹è½¬ï¼š
$$\text{output} = \text{softmax}(Q_{\text{rot}} K_{\text{rot}}^T) V_{\text{rot}}$$

é—®é¢˜ï¼š
- V çš„æ—‹è½¬ä¸å‚ä¸æƒé‡è®¡ç®—
- åªæ˜¯å¯¹è¾“å‡ºåšé¢å¤–å˜æ¢
- ä¸å¢åŠ ä»»ä½•ä½ç½®ä¿¡æ¯ï¼Œåè€Œå¼•å…¥å¹²æ‰°

**å®éªŒè¯æ˜**ï¼š
- åªæ—‹è½¬ QKï¼šæ€§èƒ½æœ€å¥½
- æ—‹è½¬ QKVï¼šæ€§èƒ½ä¸‹é™
- åªæ—‹è½¬ Vï¼šå®Œå…¨å¤±æ•ˆ

**ç›´è§‰ç†è§£**ï¼š
```
Q: "æˆ‘æƒ³æ‰¾ä»€ä¹ˆä½ç½®çš„ä¿¡æ¯ï¼Ÿ" ï¼ˆéœ€è¦ä½ç½®ï¼‰
K: "æˆ‘åœ¨å“ªä¸ªä½ç½®ï¼Ÿ" ï¼ˆéœ€è¦ä½ç½®ï¼‰
V: "æˆ‘çš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ" ï¼ˆåªéœ€è¦å†…å®¹ï¼Œä¸éœ€è¦ä½ç½®ï¼‰
```

---

## 4. é‡‡æ ·ç­–ç•¥ç›¸å…³

### Q4.1: Greedyã€Top-kã€Top-pã€Temperature æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿå¦‚ä½•é€‰æ‹©ï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**1. Greedy Samplingï¼ˆè´ªå¿ƒï¼‰**ï¼š
```python
next_token = torch.argmax(logits, dim=-1)
```
- **ä¼˜ç‚¹**ï¼šç¡®å®šæ€§ï¼Œå¯å¤ç°
- **ç¼ºç‚¹**ï¼šé‡å¤ã€æ— èŠã€é™·å…¥å¾ªç¯
- **é€‚ç”¨åœºæ™¯**ï¼šç¿»è¯‘ã€æ‘˜è¦ç­‰éœ€è¦ç¡®å®šæ€§çš„ä»»åŠ¡

**2. Temperature Scalingï¼ˆæ¸©åº¦ï¼‰**ï¼š
```python
logits = logits / temperature
probs = F.softmax(logits, dim=-1)
next_token = torch.multinomial(probs, num_samples=1)
```
- **temperature < 1.0**ï¼šåˆ†å¸ƒæ›´å°–é”ï¼ˆæ›´ç¡®å®šï¼‰
- **temperature = 1.0**ï¼šåŸå§‹åˆ†å¸ƒ
- **temperature > 1.0**ï¼šåˆ†å¸ƒæ›´å¹³æ»‘ï¼ˆæ›´éšæœºï¼‰

**ç›´è§‰**ï¼š
```
åŸå§‹ logits: [5.0, 3.0, 1.0]
åŸå§‹ probs:  [0.84, 0.16, 0.00]

T = 0.5 (å†·): [0.98, 0.02, 0.00]  â† æ›´ä¿å®ˆ
T = 1.0:      [0.84, 0.16, 0.00]  â† æ ‡å‡†
T = 2.0 (çƒ­): [0.62, 0.32, 0.06]  â† æ›´å¤šæ ·
```

**3. Top-k Sampling**ï¼š
```python
# åªä¿ç•™æ¦‚ç‡æœ€é«˜çš„ k ä¸ª token
top_k_values, top_k_indices = torch.topk(logits, k)
filtered_logits = torch.full_like(logits, float('-inf'))
filtered_logits.scatter_(-1, top_k_indices, top_k_values)
probs = F.softmax(filtered_logits, dim=-1)
```
- **k=1**ï¼šç­‰ä»·äº Greedy
- **k=50**ï¼šå¸¸ç”¨å€¼ï¼Œå¹³è¡¡å¤šæ ·æ€§å’Œè´¨é‡
- **é—®é¢˜**ï¼šå›ºå®š k ä¸å¤Ÿçµæ´»ï¼ˆæœ‰æ—¶éœ€è¦æ›´å¤š/æ›´å°‘é€‰æ‹©ï¼‰

**4. Top-p (Nucleus) Sampling**ï¼š
```python
# åŠ¨æ€é€‰æ‹©ï¼Œä¿ç•™ç´¯ç§¯æ¦‚ç‡ >= p çš„æœ€å°é›†åˆ
sorted_probs, sorted_indices = torch.sort(probs, descending=True)
cumsum_probs = torch.cumsum(sorted_probs, dim=-1)
mask = cumsum_probs > p
# ç§»é™¤ mask ä¸º True çš„ token
```
- **p=0.9**ï¼šå¸¸ç”¨å€¼
- **ä¼˜åŠ¿**ï¼šåŠ¨æ€è°ƒæ•´å€™é€‰é›†å¤§å°
- **ä¾‹å­**ï¼š
  ```
  probs = [0.6, 0.25, 0.1, 0.03, 0.02]
  p = 0.9
  ç´¯ç§¯: [0.6, 0.85, 0.95, ...]
  ä¿ç•™: [0.6, 0.25, 0.1]  # 3 ä¸ª token
  ```

**5. Min-p Sampling**ï¼š
```python
# è¿‡æ»¤æ¦‚ç‡ < min_p * max_prob çš„ token
threshold = min_p * max(probs)
mask = probs < threshold
```
- **min_p=0.05**ï¼šå¸¸ç”¨å€¼
- **ä¼˜åŠ¿**ï¼šè¿‡æ»¤é•¿å°¾ï¼Œä½†ä¿ç•™ç›¸å¯¹é‡è¦çš„é€‰æ‹©

**ç»„åˆä½¿ç”¨**ï¼ˆæ¨èï¼‰ï¼š
```python
# 1. Temperature: è°ƒæ•´åˆ†å¸ƒ
logits = logits / 0.7

# 2. Min-p: ç²—è¿‡æ»¤
logits = apply_min_p(logits, 0.05)

# 3. Top-k: å›ºå®šä¸Šé™
logits = apply_top_k(logits, 50)

# 4. Top-p: åŠ¨æ€è°ƒæ•´
logits = apply_top_p(logits, 0.95)

# 5. é‡‡æ ·
probs = F.softmax(logits, dim=-1)
next_token = torch.multinomial(probs, num_samples=1)
```

**é€‰æ‹©å»ºè®®**ï¼š

| ä»»åŠ¡     | æ¨èé…ç½®                        |
| -------- | ------------------------------- |
| åˆ›æ„å†™ä½œ | T=0.8-1.0, top_p=0.95, top_k=50 |
| å¯¹è¯     | T=0.7, top_p=0.9, top_k=40      |
| ç¿»è¯‘     | Greedy æˆ– T=0.3                 |
| ä»£ç ç”Ÿæˆ | T=0.2-0.5, top_p=0.95           |
| æ‘˜è¦     | T=0.3-0.5, top_p=0.9            |

---

### Q4.2: ä¸ºä»€ä¹ˆé‡‡æ ·é¡ºåºæ˜¯ Temperature â†’ Min-p â†’ Top-k â†’ Top-pï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**é¡ºåºé€»è¾‘**ï¼š

1. **Temperatureï¼ˆå…¨å±€è°ƒæ•´ï¼‰**ï¼š
   - ä½œç”¨ï¼šè°ƒæ•´æ•´ä¸ªåˆ†å¸ƒçš„é™¡å³­åº¦
   - åŸå› ï¼šå½±å“åç»­æ‰€æœ‰è¿‡æ»¤æ­¥éª¤
   - å¿…é¡»æœ€å…ˆåº”ç”¨

2. **Min-pï¼ˆç²—è¿‡æ»¤ï¼‰**ï¼š
   - ä½œç”¨ï¼šå»é™¤é•¿å°¾ï¼ˆæ¦‚ç‡è¿‡ä½çš„ tokenï¼‰
   - åŸå› ï¼šå¿«é€Ÿå‡å°‘å€™é€‰é›†ï¼Œæå‡åç»­æ•ˆç‡
   - ç›¸å¯¹å®½æ¾çš„è¿‡æ»¤

3. **Top-kï¼ˆå›ºå®šè¿‡æ»¤ï¼‰**ï¼š
   - ä½œç”¨ï¼šä¿ç•™å›ºå®šæ•°é‡çš„å€™é€‰
   - åŸå› ï¼šè®¾ç½®ç¡¬ä¸Šé™ï¼Œé˜²æ­¢å€™é€‰è¿‡å¤š
   - å›ºå®šæ•°é‡ï¼Œä¸ä¾èµ–æ¦‚ç‡åˆ†å¸ƒ

4. **Top-pï¼ˆç²¾ç»†è¿‡æ»¤ï¼‰**ï¼š
   - ä½œç”¨ï¼šåŸºäºç´¯ç§¯æ¦‚ç‡åŠ¨æ€è°ƒæ•´
   - åŸå› ï¼šåœ¨ top-k åŸºç¡€ä¸Šè¿›ä¸€æ­¥ç²¾ç»†åŒ–
   - åŠ¨æ€æ•°é‡ï¼Œé€‚åº”åˆ†å¸ƒç‰¹ç‚¹

**ç¤ºä¾‹**ï¼š
```python
åŸå§‹ logits: [5.0, 4.5, 4.0, 3.0, 2.0, 1.0, 0.5, 0.1, ...]

# Step 1: Temperature (T=0.7)
scaled: [7.14, 6.43, 5.71, 4.29, 2.86, 1.43, 0.71, 0.14, ...]
probs: [0.45, 0.25, 0.15, 0.08, 0.04, 0.02, 0.01, 0.00, ...]

# Step 2: Min-p (0.05 * 0.45 = 0.0225)
è¿‡æ»¤: probs < 0.0225
ä¿ç•™: [0.45, 0.25, 0.15, 0.08, 0.04, 0.02]  # 6 ä¸ª

# Step 3: Top-k (k=5)
ä¿ç•™: [0.45, 0.25, 0.15, 0.08, 0.04]  # 5 ä¸ª

# Step 4: Top-p (p=0.9)
ç´¯ç§¯: [0.45, 0.70, 0.85, 0.93, ...]
ä¿ç•™: [0.45, 0.25, 0.15, 0.08]  # 4 ä¸ªï¼ˆç´¯ç§¯åˆ° 0.93 > 0.9ï¼‰
```

**å¦‚æœé¡ºåºé”™è¯¯ä¼šæ€æ ·ï¼Ÿ**

é”™è¯¯é¡ºåºï¼šTop-p â†’ Temperature
```python
# Step 1: Top-p (p=0.9) on åŸå§‹åˆ†å¸ƒ
ä¿ç•™: [0.6, 0.3, 0.1]

# Step 2: Temperature (T=0.5)
ç»“æœ: [0.8, 0.15, 0.05]
# é—®é¢˜ï¼šTemperature å¤±å»ä½œç”¨ï¼ˆå·²ç»è¿‡æ»¤äº†ï¼‰
```

æ­£ç¡®é¡ºåºï¼šTemperature â†’ Top-p
```python
# Step 1: Temperature (T=0.5)
åˆ†å¸ƒ: [0.8, 0.15, 0.03, 0.01, 0.01]

# Step 2: Top-p (p=0.9)
ä¿ç•™: [0.8, 0.15]
# æ­£ç¡®ï¼šåŸºäºè°ƒæ•´åçš„åˆ†å¸ƒè¿‡æ»¤
```

---

### Q4.3: å¦‚ä½•å®ç°å¯å¤ç°çš„éšæœºé‡‡æ ·ï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**é—®é¢˜**ï¼š
- `torch.multinomial` æ˜¯éšæœºçš„
- ç›¸åŒè¾“å…¥ï¼Œæ¯æ¬¡è¾“å‡ºä¸åŒ

**è§£å†³æ–¹æ¡ˆ1ï¼šè®¾ç½®å…¨å±€éšæœºç§å­**
```python
torch.manual_seed(42)
next_token = torch.multinomial(probs, num_samples=1)
```
**é—®é¢˜**ï¼šå½±å“æ‰€æœ‰éšæœºæ“ä½œ

**è§£å†³æ–¹æ¡ˆ2ï¼šä½¿ç”¨ Generatorï¼ˆæ¨èï¼‰**
```python
generator = torch.Generator(device='cuda')
generator.manual_seed(42)
next_token = torch.multinomial(probs, num_samples=1, generator=generator)
```
**ä¼˜åŠ¿**ï¼š
- ç‹¬ç«‹çš„éšæœºæµ
- ä¸å½±å“å…¶ä»–éšæœºæ“ä½œ
- æ¯ä¸ªè¯·æ±‚å¯ä»¥æœ‰ç‹¬ç«‹çš„ seed

**å®ç°ç»†èŠ‚**ï¼š
```python
class Sampler:
    def __init__(self):
        self._generator = None
    
    def sample(self, logits, sampling_params):
        # è®¾ç½®éšæœºç§å­
        if sampling_params.seed is not None:
            if self._generator is None:
                self._generator = torch.Generator(device=logits.device)
            self._generator.manual_seed(sampling_params.seed)
        
        # é‡‡æ ·
        if sampling_params.sampling_type == SamplingType.GREEDY:
            sampled_tokens = torch.argmax(logits, dim=-1)
        else:
            probs = F.softmax(logits, dim=-1)
            sampled_tokens = torch.multinomial(
                probs,
                num_samples=1,
                generator=self._generator,  # ä½¿ç”¨ç‹¬ç«‹ generator
            ).squeeze(-1)
        
        return sampled_tokens
```

**éªŒè¯å¯å¤ç°æ€§**ï¼š
```python
# æµ‹è¯•
engine = LLMEngine(...)
params1 = SamplingParams(seed=42, temperature=0.8)
params2 = SamplingParams(seed=42, temperature=0.8)

output1 = engine.generate("Hello", params1)
output2 = engine.generate("Hello", params2)

assert output1.outputs[0].text == output2.outputs[0].text  # ç›¸åŒï¼
```

**æ³¨æ„äº‹é¡¹**ï¼š
- Greedy é‡‡æ ·å¤©ç„¶å¯å¤ç°ï¼ˆæ— éœ€ seedï¼‰
- ä¸åŒ PyTorch ç‰ˆæœ¬å¯èƒ½æœ‰å·®å¼‚
- GPU å’Œ CPU å¯èƒ½äº§ç”Ÿä¸åŒç»“æœï¼ˆæµ®ç‚¹ç²¾åº¦ï¼‰

---

## 5. æ¨¡å‹æ¶æ„ç›¸å…³

### Q5.1: ä¸ºä»€ä¹ˆ Qwen3 ä½¿ç”¨ RMSNorm è€Œä¸æ˜¯ LayerNormï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**LayerNorm**ï¼š
$$\text{LayerNorm}(x) = \frac{x - \mu}{\sigma} \cdot \gamma + \beta$$

å…¶ä¸­ï¼š
- $\mu = \frac{1}{d}\sum_i x_i$ï¼ˆå‡å€¼ï¼‰
- $\sigma = \sqrt{\frac{1}{d}\sum_i (x_i - \mu)^2}$ï¼ˆæ ‡å‡†å·®ï¼‰

**RMSNorm**ï¼š
$$\text{RMSNorm}(x) = \frac{x}{\text{RMS}(x)} \cdot \gamma$$

å…¶ä¸­ï¼š
- $\text{RMS}(x) = \sqrt{\frac{1}{d}\sum_i x_i^2}$ï¼ˆå‡æ–¹æ ¹ï¼‰

**åŒºåˆ«**ï¼š
1. **Re-centeringï¼ˆå‡å‡å€¼ï¼‰**ï¼š
   - LayerNorm: æœ‰
   - RMSNorm: æ— 
2. **è®¡ç®—é‡**ï¼š
   - LayerNorm: 2 æ¬¡éå†ï¼ˆè®¡ç®—å‡å€¼ã€æ ‡å‡†å·®ï¼‰
   - RMSNorm: 1 æ¬¡éå†ï¼ˆåªè®¡ç®— RMSï¼‰

**ä¸ºä»€ä¹ˆ RMSNorm æ›´å¥½**ï¼š

**ç†è®ºåˆ†æ**ï¼š
- è®ºæ–‡ã€ŠRoot Mean Square Layer Normalizationã€‹å‘ç°ï¼š
  - Re-centering å¯¹æ€§èƒ½æå‡æœ‰é™
  - Scalingï¼ˆé™¤ä»¥æ ‡å‡†å·®ï¼‰æ‰æ˜¯å…³é”®
- RMSNorm ä¿ç•™ scalingï¼Œå»æ‰ re-centering

**æ€§èƒ½æå‡**ï¼š
- è®¡ç®—é‡ï¼šå‡å°‘ ~10-15%
- é€Ÿåº¦ï¼šåŠ é€Ÿ ~5-10%
- ç²¾åº¦ï¼šå‡ ä¹æ— æŸ

**å®éªŒç»“æœ**ï¼š
```python
# åœ¨ç›¸åŒè®­ç»ƒè®¾ç½®ä¸‹
LayerNorm: PPL = 12.3, Time = 100s
RMSNorm:   PPL = 12.4, Time = 93s  # ç•¥å¿«ï¼Œè´¨é‡ç›¸å½“
```

**ä»£ç å¯¹æ¯”**ï¼š
```python
# LayerNorm
mean = x.mean(-1, keepdim=True)
var = ((x - mean) ** 2).mean(-1, keepdim=True)
x_norm = (x - mean) / sqrt(var + eps)  # éœ€è¦å‡å‡å€¼

# RMSNorm
rms = sqrt(x.pow(2).mean(-1, keepdim=True))
x_norm = x / (rms + eps)  # ä¸éœ€è¦å‡å‡å€¼ï¼Œæ›´ç®€å•
```

**è¿½é—®ï¼šä¸ºä»€ä¹ˆ RMSNorm æœ‰ Fused Residual ç‰ˆæœ¬ï¼Ÿ**

**å›ç­”**ï¼š
```python
# ä¼ ç»Ÿæ–¹å¼ï¼ˆ2 æ¬¡å†…å­˜è®¿é—®ï¼‰
residual = x + residual  # ç¬¬ 1 æ¬¡
x_norm = RMSNorm(residual)  # ç¬¬ 2 æ¬¡

# Fused æ–¹å¼ï¼ˆ1 æ¬¡å†…å­˜è®¿é—®ï¼‰
x_norm, new_residual = RMSNorm(x, residual)
# å†…éƒ¨ä¸€æ¬¡æ€§å®Œæˆï¼šnew_residual = x + residual, x_norm = norm(new_residual)
```

**å¥½å¤„**ï¼š
- å‡å°‘å†…å­˜å¸¦å®½
- å‡å°‘ kernel launch
- æå‡ 5-10% æ€§èƒ½

---

### Q5.2: ä¸ºä»€ä¹ˆ Qwen3 çš„ MLP ä½¿ç”¨ SiLU è€Œä¸æ˜¯ ReLUï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**æ¿€æ´»å‡½æ•°å¯¹æ¯”**ï¼š

1. **ReLU**ï¼š
$$\text{ReLU}(x) = \max(0, x)$$
- ä¼˜ç‚¹ï¼šç®€å•ï¼Œè®¡ç®—å¿«
- ç¼ºç‚¹ï¼šéå…‰æ»‘ï¼Œæ¢¯åº¦æ¶ˆå¤±ï¼ˆx<0ï¼‰

2. **GELU**ï¼š
$$\text{GELU}(x) = x \cdot \Phi(x)$$
- ä¼˜ç‚¹ï¼šå…‰æ»‘ï¼Œæ€§èƒ½å¥½
- ç¼ºç‚¹ï¼šè®¡ç®—å¤æ‚ï¼ˆæ¶‰åŠè¯¯å·®å‡½æ•°ï¼‰

3. **SiLU (Swish)**ï¼š
$$\text{SiLU}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}$$
- ä¼˜ç‚¹ï¼šå…‰æ»‘ï¼Œæ¥è¿‘ GELU æ€§èƒ½ï¼Œè®¡ç®—ç®€å•
- ç¼ºç‚¹ï¼šæ¯” ReLU ç¨æ…¢

**ä¸ºä»€ä¹ˆé€‰ SiLU**ï¼š

**æ€§èƒ½**ï¼š
```python
# å®éªŒç»“æœï¼ˆç›¸åŒè®­ç»ƒ setupï¼‰
ReLU: PPL = 15.2
GELU: PPL = 12.8
SiLU: PPL = 12.9  # æ¥è¿‘ GELU
```

**è®¡ç®—æ•ˆç‡**ï¼š
```python
# ç›¸å¯¹é€Ÿåº¦ï¼ˆReLU = 1.0ï¼‰
ReLU: 1.0
GELU: 0.85  # æ…¢ 15%
SiLU: 0.95  # æ…¢ 5%
```

**å…‰æ»‘æ€§**ï¼š
```python
# å¯¼æ•°
ReLU':  {0, 1}  # ä¸è¿ç»­
SiLU':  è¿ç»­ä¸”å…‰æ»‘
```

**Gated MLP**ï¼š
```python
# Qwen3 ä½¿ç”¨ Gated MLP
gate = linear_gate(x)
up = linear_up(x)
output = silu(gate) * up  # SiLU ä½œä¸º gate æ¿€æ´»

# ä¸ºä»€ä¹ˆï¼š
# - SiLU è¾“å‡º [0, +âˆ)ï¼Œé€‚åˆåš gate
# - å…‰æ»‘æ€§æœ‰åŠ©äºæ¢¯åº¦æµåŠ¨
```

**Fused å®ç°**ï¼š
```python
# åˆå¹¶ gate å’Œ up æŠ•å½±
gate_up = linear_gate_up(x)  # [hidden, 2*intermediate]
gate, up = gate_up.chunk(2, dim=-1)
output = F.silu(gate) * up  # ä¸€æ¬¡ kernel

# å¥½å¤„ï¼š
# - å•ä¸ª linear layer æ›´é«˜æ•ˆ
# - å‡å°‘å†…å­˜è®¿é—®
```

---

### Q5.3: ä¸ºä»€ä¹ˆè¦åˆå¹¶ QKV æŠ•å½±ä¸ºä¸€ä¸ª Linear Layerï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**ä¼ ç»Ÿæ–¹å¼ï¼ˆ3 ä¸ª Linearï¼‰**ï¼š
```python
Q = linear_q(x)  # [hidden, q_size]
K = linear_k(x)  # [hidden, kv_size]
V = linear_v(x)  # [hidden, kv_size]
```

**åˆå¹¶æ–¹å¼ï¼ˆ1 ä¸ª Linearï¼‰**ï¼š
```python
QKV = linear_qkv(x)  # [hidden, q_size + 2*kv_size]
Q, K, V = QKV.split([q_size, kv_size, kv_size], dim=-1)
```

**ä¼˜åŠ¿**ï¼š

**1. å‡å°‘ Kernel Launch**ï¼š
```python
# 3 ä¸ª Linear
kernel_launch Ã— 3  # æ¯æ¬¡æœ‰å¯åŠ¨å¼€é”€

# 1 ä¸ª Linear
kernel_launch Ã— 1  # å¼€é”€å‡å°‘ 2/3
```

**2. å†…å­˜è®¿é—®ä¼˜åŒ–**ï¼š
```python
# 3 ä¸ª Linearï¼ˆ3 æ¬¡è¯»å– xï¼‰
è¯» x â†’ è®¡ç®— Q â†’ å†™ Q
è¯» x â†’ è®¡ç®— K â†’ å†™ K
è¯» x â†’ è®¡ç®— V â†’ å†™ V

# 1 ä¸ª Linearï¼ˆ1 æ¬¡è¯»å– xï¼‰
è¯» x â†’ è®¡ç®— QKV â†’ å†™ QKV
```

**3. è®¡ç®—æ•ˆç‡**ï¼š
```python
# å•ä¸ªå¤§çŸ©é˜µä¹˜æ³•æ›´é«˜æ•ˆ
[batch*seq, hidden] @ [hidden, q_size+2*kv_size]
# GPU å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨ tensor cores
```

**æ€§èƒ½æå‡**ï¼š
```python
# å®æµ‹ï¼ˆQwen3-0.6B, batch=1, seq=1ï¼‰
3 ä¸ª Linear: 0.064 ms/token
1 ä¸ª Linear: 0.058 ms/token  # å¿« ~10%
```

**æƒé‡åŠ è½½**ï¼š
```python
# HuggingFace checkpoint é€šå¸¸æ˜¯åˆ†ç¦»çš„
state_dict = {
    'q_proj.weight': ...,
    'k_proj.weight': ...,
    'v_proj.weight': ...,
}

# éœ€è¦åˆå¹¶
qkv_weight = torch.cat([
    state_dict['q_proj.weight'],
    state_dict['k_proj.weight'],
    state_dict['v_proj.weight'],
], dim=0)
```

**æ³¨æ„äº‹é¡¹**ï¼š
- å¹¶éæ‰€æœ‰æ¨¡å‹éƒ½åˆå¹¶ï¼ˆå¦‚ LLaMAï¼‰
- M1 ä½¿ç”¨ HF åŸç”Ÿæ¨¡å‹ï¼ˆæœªåˆå¹¶ï¼‰
- M2 å¯ä»¥è€ƒè™‘åˆå¹¶ä¼˜åŒ–

---

## 6. æ¨ç†ä¼˜åŒ–ç›¸å…³

### Q6.1: Prefill å’Œ Decode çš„æ€§èƒ½ç“¶é¢ˆåˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**Prefill é˜¶æ®µ**ï¼ˆå¤„ç† promptï¼‰ï¼š

**ç‰¹ç‚¹**ï¼š
- è¾“å…¥ï¼šå¤šä¸ª tokenï¼ˆå¦‚ 100 ä¸ªï¼‰
- è®¡ç®—ï¼š$O(n^2)$ çš„ attention
- å¹¶è¡Œåº¦é«˜ï¼šæ‰€æœ‰ token åŒæ—¶è®¡ç®—

**ç“¶é¢ˆ**ï¼š
```
è®¡ç®—å¯†é›†å‹ (Compute-bound)

åŸå› ï¼š
- å¤§é‡çŸ©é˜µä¹˜æ³•ï¼ˆQ@K^T, attention@Vï¼‰
- GPU è®¡ç®—å•å…ƒåˆ©ç”¨ç‡é«˜
- å†…å­˜å¸¦å®½å‹åŠ›ç›¸å¯¹è¾ƒå°
```

**ä¼˜åŒ–æ–¹å‘**ï¼š
- Flash Attentionï¼ˆå‡å°‘å†…å­˜è®¿é—®ï¼Œæå‡è®¡ç®—æ•ˆç‡ï¼‰
- Tensor Parallelismï¼ˆåˆ†å¸ƒå¼è®¡ç®—ï¼‰
- æ··åˆç²¾åº¦ï¼ˆFP16/BF16ï¼‰

**Decode é˜¶æ®µ**ï¼ˆç”Ÿæˆ tokenï¼‰ï¼š

**ç‰¹ç‚¹**ï¼š
- è¾“å…¥ï¼š1 ä¸ª token
- è®¡ç®—ï¼š$O(n)$ çš„ attention
- å¹¶è¡Œåº¦ä½ï¼šå•ä¸ª token

**ç“¶é¢ˆ**ï¼š
```
å†…å­˜å¸¦å®½å¯†é›†å‹ (Memory-bound)

åŸå› ï¼š
- éœ€è¦è¯»å–æ•´ä¸ª KV cache
- è®¡ç®—é‡å°ï¼ˆå•ä¸ª tokenï¼‰
- GPU è®¡ç®—å•å…ƒåˆ©ç”¨ç‡ä½ï¼ˆ<30%ï¼‰
```

**æ•°æ®**ï¼š
```python
# Qwen3-0.6B, seq_len=2048
Prefill: 
  - FLOPS: ~1.2 TFLOPs
  - Memory: ~100 MB
  - Time: 262 ms
  - GPU Util: ~90%

Decode:
  - FLOPS: ~0.6 GFLOPs (æ¯ token)
  - Memory: ~50 MB (è¯»å– KV cache)
  - Time: 64 ms (æ¯ token)
  - GPU Util: ~25%
```

**ä¼˜åŒ–æ–¹å‘**ï¼š
- Continuous Batchingï¼ˆå¢åŠ å¹¶è¡Œåº¦ï¼‰
- Paged Attentionï¼ˆå‡å°‘å†…å­˜è®¿é—®ï¼‰
- Speculative Decodingï¼ˆå‡å°‘ decode æ­¥æ•°ï¼‰

**å¯¹æ¯”å›¾**ï¼š
```
Prefill:
GPU: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 90% (è®¡ç®—é¥±å’Œ)
MEM: [â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 15% (å†…å­˜ç©ºé—²)

Decode:
GPU: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 25% (è®¡ç®—ç©ºé—²)
MEM: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘] 80% (å†…å­˜é¥±å’Œ)
```

---

### Q6.2: ä»€ä¹ˆæ˜¯ Continuous Batchingï¼Ÿä¸ºä»€ä¹ˆèƒ½æå‡ååé‡ï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**ä¼ ç»Ÿ Batchingï¼ˆStatic Batchingï¼‰**ï¼š
```python
# æ‰€æœ‰è¯·æ±‚åŒæ—¶å¼€å§‹å’Œç»“æŸ
requests = [req1, req2, req3, req4]  # batch_size=4
while not all_finished:
    logits = model(requests)
    sample(logits)

# é—®é¢˜ï¼š
req1: [=============================] (30 tokens)
req2: [===============] (15 tokens) Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·  â† ç­‰å¾…
req3: [====================] (20 tokens) Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·  â† ç­‰å¾…
req4: [========================] (24 tokens) Â·Â·Â·Â·Â·Â·  â† ç­‰å¾…
                                  â†‘
                            æµªè´¹çš„ GPU æ—¶é—´
```

**Continuous Batchingï¼ˆDynamic Batchingï¼‰**ï¼š
```python
# è¯·æ±‚å¯ä»¥åŠ¨æ€åŠ å…¥/ç¦»å¼€ batch
active_requests = []

while True:
    # æ·»åŠ æ–°è¯·æ±‚
    active_requests += new_requests()
    
    # ç§»é™¤å®Œæˆçš„è¯·æ±‚
    active_requests = [r for r in active_requests if not r.finished]
    
    # æ‰§è¡Œä¸€æ­¥
    logits = model(active_requests)
    sample(logits)

# æ•ˆæœï¼š
Batch 1: [req1, req2, req3, req4]
Batch 2: [req1, req2, req3, req4]
Batch 3: [req1, req3, req4, req5]  â† req2 å®Œæˆï¼Œreq5 åŠ å…¥
Batch 4: [req1, req3, req4, req5, req6]
Batch 5: [req1, req4, req5, req6]  â† req3 å®Œæˆ
...
```

**æ€§èƒ½æå‡**ï¼š

**ååé‡**ï¼š
```python
# Static Batching
ååé‡ = batch_size / max_completion_time
       = 4 / 30 tokens = 0.13 req/token

# Continuous Batching
ååé‡ = æ€»è¯·æ±‚æ•° / æ€»æ—¶é—´
       â‰ˆ 2-3x æå‡
```

**GPU åˆ©ç”¨ç‡**ï¼š
```python
# Static Batching
å¹³å‡ batch size = (4+4+4+3+2+1) / 6 = 3.0

# Continuous Batching
å¹³å‡ batch size = (4+4+4+5+5+4+...) / N â‰ˆ 4.5
                â†‘ æŒç»­ä¿æŒé«˜ batch size
```

**å®ç°æŒ‘æˆ˜**ï¼š

1. **KV Cache ç®¡ç†**ï¼š
   - æ¯ä¸ªè¯·æ±‚æœ‰ä¸åŒçš„ cache å¤§å°
   - éœ€è¦åŠ¨æ€åˆ†é…å†…å­˜

2. **Attention è®¡ç®—**ï¼š
   - ä¸åŒè¯·æ±‚çš„ seq_len ä¸åŒ
   - éœ€è¦æ”¯æŒ variable-length attention

3. **è°ƒåº¦ç­–ç•¥**ï¼š
   - å¦‚ä½•é€‰æ‹©ä¸‹ä¸€ä¸ª batchï¼Ÿ
   - FCFSã€Priorityã€Fairnessï¼Ÿ

**M2 å®ç°æ–¹å¼**ï¼š
```python
class Scheduler:
    def __init__(self):
        self.waiting = []
        self.running = []
    
    def schedule(self) -> List[Request]:
        # æ·»åŠ ç­‰å¾…çš„è¯·æ±‚åˆ° running
        while len(self.running) < MAX_BATCH_SIZE and self.waiting:
            req = self.waiting.pop(0)
            self.running.append(req)
        
        # ç§»é™¤å®Œæˆçš„è¯·æ±‚
        self.running = [r for r in self.running if not r.is_finished()]
        
        return self.running
```

---

### Q6.3: å¦‚ä½•ä¼˜åŒ– Transformer æ¨ç†çš„å†…å­˜ä½¿ç”¨ï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**å†…å­˜ç»„æˆ**ï¼š
```python
Total Memory = Model Weights + KV Cache + Activations + Temporary Buffers

# Qwen3-0.6B ç¤ºä¾‹ï¼ˆbatch=1, seq=2048, fp16ï¼‰
Model Weights: 1.2 GB
KV Cache:      28 MB (per request)
Activations:   50 MB (per layer)
Temporary:     100 MB
Total:         ~1.4 GB (å•è¯·æ±‚)
```

**ä¼˜åŒ–ç­–ç•¥**ï¼š

**1. æ¨¡å‹æƒé‡ä¼˜åŒ–**ï¼š
```python
# é‡åŒ–
FP16:  1.2 GB
INT8:  600 MB (-50%)
INT4:  300 MB (-75%)

# ç¨€ç–åŒ–
å‰ªæ: å‡å°‘ 10-30% æƒé‡

# LoRAï¼ˆæ¨ç†æ—¶åˆå¹¶ï¼‰
åªåŠ è½½ base model + LoRA adapters
```

**2. KV Cache ä¼˜åŒ–**ï¼š

**GQA (Grouped Query Attention)**ï¼š
```python
# MHA: 16 Q heads, 16 KV heads
KV Cache = 28 MB

# GQA: 16 Q heads, 2 KV heads
KV Cache = 3.5 MB (-87.5%)
```

**Paged Attention (M3)**ï¼š
```python
# ä¼ ç»Ÿï¼šè¿ç»­å†…å­˜ï¼Œé¢„ç•™æœ€å¤§é•¿åº¦
æ¯è¯·æ±‚ = max_seq_len * kv_size = æµªè´¹å¤š

# Pagedï¼šæŒ‰éœ€åˆ†é…ï¼Œåˆ©ç”¨ç‡ 100%
æ¯è¯·æ±‚ = actual_seq_len * kv_size
```

**KV Cache Quantization**ï¼š
```python
# FP16 â†’ INT8
KV Cache å†…å­˜ -50%
ç²¾åº¦æŸå¤± <1%
```

**3. Activation ä¼˜åŒ–**ï¼š

**Activation Checkpointing (Gradient Checkpointing)**ï¼š
```python
# æ¨ç†æ—¶ä¸éœ€è¦ï¼ˆåªåœ¨è®­ç»ƒæ—¶ï¼‰
```

**Fused Kernels**ï¼š
```python
# å‡å°‘ä¸­é—´ç»“æœå­˜å‚¨
# ä¾‹å¦‚ï¼šRMSNorm + Residual ä¸€æ¬¡æ€§å®Œæˆ
```

**4. Batching ä¼˜åŒ–**ï¼š

**Continuous Batching**ï¼š
```python
# åŠ¨æ€ç®¡ç†ï¼Œé¿å… padding
Static: [req1(2048), req2(512)] â†’ 2*2048 = 4096 tokens
        æµªè´¹: 2048 - 512 = 1536 tokens

Continuous: [req1(2048), req2(512)] â†’ 2048+512 = 2560 tokens
            èŠ‚çœ: 1536 tokens (-38%)
```

**5. FlashAttention (M4)**ï¼š
```python
# å‡å°‘ attention çš„å†…å­˜å ç”¨
# Naive: O(nÂ²) å†…å­˜
# Flash: O(n) å†…å­˜

# Qwen3-0.6B, seq=2048
Naive: 200 MB attention ä¸­é—´ç»“æœ
Flash: 20 MB (-90%)
```

**ç»¼åˆä¼˜åŒ–æ•ˆæœ**ï¼š
```python
# åŸºç¡€é…ç½®ï¼ˆFP16ï¼‰
å•è¯·æ±‚å†…å­˜: 1.4 GB
æœ€å¤§ batch: 16 (24GB GPU)

# ä¼˜åŒ–åï¼ˆGQA + Paged + Flash + INT8 KVï¼‰
å•è¯·æ±‚å†…å­˜: 0.3 GB
æœ€å¤§ batch: 70 (24GB GPU)
ååé‡: 4-5x æå‡
```

---

## 7. ç³»ç»Ÿè®¾è®¡ç›¸å…³

### Q7.1: ä¸ºä»€ä¹ˆéœ€è¦åˆ†å±‚è®¾è®¡ï¼ˆEngine â†’ Executor â†’ Worker â†’ ModelRunnerï¼‰ï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**æ¶æ„å±‚æ¬¡**ï¼š
```
User API:       LLMEngine
                   â†“
Logic Layer:    Scheduler, Sampler, Processor
                   â†“
Execution:      GPUExecutor (æ¥å£å±‚)
                   â†“
Device:         GPUWorker (è®¾å¤‡ç®¡ç†)
                   â†“
Model:          ModelRunner (æ¨¡å‹è¿è¡Œ)
                   â†“
Hardware:       GPU/CUDA
```

**æ¯å±‚èŒè´£**ï¼š

**1. LLMEngineï¼ˆä¸šåŠ¡é€»è¾‘ï¼‰**ï¼š
```python
èŒè´£ï¼š
- æä¾›ç”¨æˆ· API (generate, add_request)
- åè°ƒå„ç»„ä»¶ï¼ˆscheduler, sampler, executorï¼‰
- ç®¡ç†è¯·æ±‚ç”Ÿå‘½å‘¨æœŸ
- æ„é€ è¾“å‡º

ä¸å…³å¿ƒï¼š
- GPU å¦‚ä½•æ‰§è¡Œ
- æ¨¡å‹å¦‚ä½•åŠ è½½
- å†…å­˜å¦‚ä½•åˆ†é…
```

**2. GPUExecutorï¼ˆæ‰§è¡Œæ¥å£ï¼‰**ï¼š
```python
èŒè´£ï¼š
- ç»Ÿä¸€çš„æ‰§è¡Œæ¥å£
- éšè—åˆ†å¸ƒå¼ç»†èŠ‚ï¼ˆå• GPU vs å¤š GPUï¼‰
- æä¾› get_next_token_logits() ç­‰é«˜å±‚æ¥å£

ä¸å…³å¿ƒï¼š
- è¯·æ±‚å¦‚ä½•è°ƒåº¦
- é‡‡æ ·å¦‚ä½•è¿›è¡Œ
- å…·ä½“è®¾å¤‡ç»†èŠ‚
```

**3. GPUWorkerï¼ˆè®¾å¤‡ç®¡ç†ï¼‰**ï¼š
```python
èŒè´£ï¼š
- ç®¡ç† GPU è®¾å¤‡
- åŠ è½½æ¨¡å‹åˆ°è®¾å¤‡
- ç®¡ç†è®¾å¤‡å†…å­˜
- åˆ›å»º ModelRunner

ä¸å…³å¿ƒï¼š
- æ¨¡å‹å†…éƒ¨ç»“æ„
- å‰å‘ä¼ æ’­ç»†èŠ‚
- åˆ†å¸ƒå¼é€šä¿¡
```

**4. ModelRunnerï¼ˆæ¨¡å‹è¿è¡Œï¼‰**ï¼š
```python
èŒè´£ï¼š
- å‡†å¤‡æ¨¡å‹è¾“å…¥
- æ‰§è¡Œå‰å‘ä¼ æ’­
- ç®¡ç† KV cache
- è¿”å› logits

ä¸å…³å¿ƒï¼š
- è¯·æ±‚ä»å“ªæ¥
- ç»“æœå¦‚ä½•å¤„ç†
- è®¾å¤‡å¦‚ä½•ç®¡ç†
```

**å¥½å¤„**ï¼š

**1. å…³æ³¨ç‚¹åˆ†ç¦»**ï¼š
```python
# Engine å…³å¿ƒä¸šåŠ¡é€»è¾‘
engine.generate(prompt, params)

# Executor å…³å¿ƒæ‰§è¡Œ
executor.get_next_token_logits(tokens, pos)

# Worker å…³å¿ƒè®¾å¤‡
worker = GPUWorker(model_config, device="cuda:0")

# Runner å…³å¿ƒæ¨¡å‹
runner.execute_model(input_ids, positions)
```

**2. ä¾¿äºæ‰©å±•**ï¼š
```python
# M1: å• GPU
executor = GPUExecutor(config, device="cuda")

# M6: å¤š GPU Tensor Parallelism
executor = GPUExecutor(config, tensor_parallel_size=4)
# å†…éƒ¨åˆ›å»º 4 ä¸ª GPUWorkerï¼Œç”¨æˆ·æ— æ„ŸçŸ¥

# M7: Pipeline Parallelism
executor = PipelineExecutor(config, pipeline_stages=4)
```

**3. ä¾¿äºæµ‹è¯•**ï¼š
```python
# å•å…ƒæµ‹è¯•ï¼šåªæµ‹ ModelRunner
runner = ModelRunner(model, config, device)
logits = runner.execute_model(input_ids, positions)
assert logits.shape == expected_shape

# é›†æˆæµ‹è¯•ï¼šæµ‹ Engine
engine = LLMEngine(config)
output = engine.generate(prompt, params)
assert output.outputs[0].text != ""
```

**4. ä¾¿äºç»´æŠ¤**ï¼š
```python
# ä¿®æ”¹æ¨¡å‹è¿è¡Œé€»è¾‘ï¼šåªæ”¹ ModelRunner
# ä¿®æ”¹è°ƒåº¦é€»è¾‘ï¼šåªæ”¹ Scheduler
# ä¿®æ”¹é‡‡æ ·é€»è¾‘ï¼šåªæ”¹ Sampler

# ä¸ä¼šç›¸äº’å½±å“
```

**å¯¹æ¯” Monolithic è®¾è®¡**ï¼š
```python
# ä¸åˆ†å±‚ï¼ˆæ‰€æœ‰é€»è¾‘åœ¨ä¸€èµ·ï¼‰
class LLMEngine:
    def generate(self, prompt):
        # è®¾å¤‡ç®¡ç†
        model = load_model_to_gpu()
        # æ¨¡å‹è¿è¡Œ
        logits = model(input_ids)
        # é‡‡æ ·
        next_token = sample(logits)
        # ...
        
# é—®é¢˜ï¼š
# - æ— æ³•æµ‹è¯•å•ä¸ªç»„ä»¶
# - æ— æ³•æ‰©å±•ï¼ˆå¦‚æ·»åŠ å¤š GPUï¼‰
# - ä»£ç éš¾ä»¥ç†è§£
```

---

### Q7.2: HuggingFace æ¨¡å‹å’Œè‡ªå®šä¹‰æ¨¡å‹æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿä¸ºä»€ä¹ˆ M1 é€‰æ‹© HFï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**è‡ªå®šä¹‰æ¨¡å‹ï¼ˆåŸè®¡åˆ’ï¼‰**ï¼š
```python
# folovllm/model_executor/models/qwen.py
class Qwen3ForCausalLM(nn.Module):
    def __init__(self, config):
        self.model = Qwen3Model(config)
        self.lm_head = nn.Linear(...)
    
    def forward(self, input_ids, positions, kv_caches):
        hidden = self.model(input_ids, positions, kv_caches)
        logits = self.lm_head(hidden)
        return logits

ä¼˜ç‚¹ï¼š
- âœ… å®Œå…¨å¯æ§
- âœ… å¯ä»¥ä¼˜åŒ–ï¼ˆmerged QKV, fused opsï¼‰
- âœ… å­¦ä¹ ä»·å€¼é«˜

ç¼ºç‚¹ï¼š
- âŒ æƒé‡åŠ è½½å¤æ‚
- âŒ æ¶æ„å¿…é¡»å®Œå…¨åŒ¹é… HF
- âŒ è°ƒè¯•å›°éš¾
- âŒ ç»´æŠ¤æˆæœ¬é«˜
```

**HuggingFace æ¨¡å‹ï¼ˆM1 å®é™…ï¼‰**ï¼š
```python
# ç›´æ¥ä½¿ç”¨ HF
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-0.6B",
    torch_dtype=torch.float16,
    device_map="cuda",
)

ä¼˜ç‚¹ï¼š
- âœ… æƒé‡ç›´æ¥åŠ è½½
- âœ… ç¨³å®šå¯é 
- âœ… å¿«é€Ÿå¼€å‘
- âœ… ä¸ HF è¾“å‡ºä¸€è‡´ï¼ˆä¾¿äºéªŒè¯ï¼‰

ç¼ºç‚¹ï¼š
- âŒ é»‘ç›’ï¼ˆéš¾ä»¥æ·±å…¥ä¼˜åŒ–ï¼‰
- âŒ å­¦ä¹ ä»·å€¼è¾ƒä½
```

**ä¸ºä»€ä¹ˆ M1 é€‰æ‹© HF**ï¼š

**1. æ¶æ„å·®å¼‚**ï¼š
```python
# è‡ªå®šä¹‰æ¨¡å‹
class Qwen3Attention:
    self.qkv_proj = nn.Linear(hidden, q_size + 2*kv_size)  # åˆå¹¶

# HF æ¨¡å‹
class Qwen2Attention:
    self.q_proj = nn.Linear(hidden, q_size)
    self.k_proj = nn.Linear(hidden, kv_size)
    self.v_proj = nn.Linear(hidden, kv_size)
    self.q_norm = nn.LayerNorm(...)  # é¢å¤–çš„ norm
    self.k_norm = nn.LayerNorm(...)
```

**2. æƒé‡æ˜ å°„å¤æ‚**ï¼š
```python
# éœ€è¦è¿™æ ·æ˜ å°„
hf_state_dict = torch.load("model.safetensors")
custom_state_dict = {}

# QKV åˆå¹¶
q_weight = hf_state_dict['q_proj.weight']
k_weight = hf_state_dict['k_proj.weight']
v_weight = hf_state_dict['v_proj.weight']
custom_state_dict['qkv_proj.weight'] = torch.cat([q, k, v], dim=0)

# å¤„ç† q_norm, k_normï¼ˆè‡ªå®šä¹‰æ¨¡å‹æ²¡æœ‰ï¼‰
# ... å¤æ‚çš„é€»è¾‘

model.load_state_dict(custom_state_dict)  # å®¹æ˜“å‡ºé”™
```

**3. ç¨³å®šæ€§ä¼˜å…ˆ**ï¼š
```python
# M1 ç›®æ ‡ï¼šå¿«é€ŸéªŒè¯æ•´ä½“æµç¨‹
# è‡ªå®šä¹‰æ¨¡å‹ï¼šè°ƒè¯•æƒé‡åŠ è½½å¯èƒ½èŠ±è´¹å¤§é‡æ—¶é—´
# HF æ¨¡å‹ï¼šå¼€ç®±å³ç”¨ï¼Œä¸“æ³¨æ ¸å¿ƒé€»è¾‘
```

**æƒè¡¡**ï¼š
```python
M1: ä½¿ç”¨ HFï¼ˆç¨³å®šæ€§ï¼‰
M2: ç»§ç»­ HFï¼ˆä¸“æ³¨ schedulingï¼‰
M3: è€ƒè™‘è‡ªå®šä¹‰ï¼ˆä¼˜åŒ– KV cacheï¼‰
M4: è‡ªå®šä¹‰ï¼ˆFlash Attentionï¼‰
M5: è‡ªå®šä¹‰ + ä¼˜åŒ–ï¼ˆæ€§èƒ½å…³é”®æœŸï¼‰
```

**å¦‚ä½•å…¼å®¹ HF æ¨¡å‹**ï¼š
```python
# folovllm/model_loader.py
def _wrap_model_for_folovllm(self, model):
    # æ·»åŠ  folovllm éœ€è¦çš„æ¥å£
    if not hasattr(model, 'compute_logits'):
        def compute_logits(hidden_states):
            return model.lm_head(hidden_states)
        
        import types
        model.compute_logits = types.MethodType(compute_logits, model)
    
    return model

# folovllm/worker/model_runner.py
def execute_model(self, token_ids, start_pos):
    # æ£€æµ‹ HF æ¨¡å‹
    if 'position_ids' in str(self.model.forward.__code__.co_varnames):
        # HF æ¨¡å‹
        outputs = self.model(
            input_ids=input_ids,
            position_ids=positions,
            past_key_values=self.past_key_values,  # HF cache
            use_cache=True,
        )
        logits = outputs.logits
        self.past_key_values = outputs.past_key_values
    else:
        # è‡ªå®šä¹‰æ¨¡å‹
        hidden = self.model(input_ids, positions, self.kv_caches)
        logits = self.model.compute_logits(hidden)
    
    return logits
```

---

## 8. æ•°å€¼ç¨³å®šæ€§ç›¸å…³

### Q8.1: ä¸ºä»€ä¹ˆ Softmax è¦åœ¨ FP32 ä¸‹è®¡ç®—ï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**é—®é¢˜èƒŒæ™¯**ï¼š
```python
# FP16 çš„æ•°å€¼èŒƒå›´
Max: 65504
Min: 6e-8
```

**Softmax å…¬å¼**ï¼š
$$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$$

**FP16 çš„é—®é¢˜**ï¼š

**1. ä¸Šæº¢ï¼ˆOverflowï¼‰**ï¼š
```python
x = torch.tensor([10.0, 20.0, 30.0], dtype=torch.float16)
exp_x = torch.exp(x)
# exp(30) â‰ˆ 1e13 > 65504 â†’ Overflow â†’ inf

softmax = exp_x / exp_x.sum()
# inf / inf = nan
```

**2. ä¸‹æº¢ï¼ˆUnderflowï¼‰**ï¼š
```python
x = torch.tensor([-100.0, -90.0, -80.0], dtype=torch.float16)
exp_x = torch.exp(x)
# exp(-100) â‰ˆ 3e-44 < 6e-8 â†’ Underflow â†’ 0

softmax = exp_x / exp_x.sum()
# 0 / 0 = nan
```

**è§£å†³æ–¹æ¡ˆï¼šFP32 + æ•°å€¼ç¨³å®šæŠ€å·§**ï¼š
```python
def stable_softmax(x):
    # 1. è½¬ FP32
    x = x.float()
    
    # 2. å‡å»æœ€å¤§å€¼ï¼ˆé˜²æ­¢ä¸Šæº¢ï¼‰
    x_max = x.max(dim=-1, keepdim=True)
    x_shifted = x - x_max  # æœ€å¤§å€¼å˜ä¸º 0
    
    # 3. è®¡ç®— exp
    exp_x = torch.exp(x_shifted)  # æœ€å¤§å€¼çš„ exp ä¸º 1ï¼Œä¸ä¼šæº¢å‡º
    
    # 4. å½’ä¸€åŒ–
    softmax = exp_x / exp_x.sum(dim=-1, keepdim=True)
    
    return softmax
```

**æ•°å­¦è¯æ˜**ï¼š
$$\frac{e^{x_i - c}}{\sum_j e^{x_j - c}} = \frac{e^{x_i}}{e^c} \cdot \frac{e^c}{\sum_j e^{x_j}} = \frac{e^{x_i}}{\sum_j e^{x_j}}$$

**å®ç°**ï¼š
```python
# PyTorch çš„ F.softmax å†…ç½®äº†æ•°å€¼ç¨³å®šæ€§
attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32)

# ç„¶åè½¬å›åŸ dtype
attn_weights = attn_weights.to(query.dtype)
```

**æ€§èƒ½å½±å“**ï¼š
```python
# FP16 softmax: å¿«ä½†ä¸ç¨³å®š
# FP32 softmax: æ…¢ ~10% ä½†ç¨³å®š

# æƒè¡¡ï¼šç¨³å®šæ€§ > æ€§èƒ½
```

---

### Q8.2: RMSNorm ä¸ºä»€ä¹ˆè¦åŠ  epsilonï¼Ÿå¦‚ä½•é€‰æ‹© epsilon å€¼ï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**RMSNorm å…¬å¼**ï¼š
$$\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{d}\sum_i x_i^2 + \epsilon}}$$

**ä¸ºä»€ä¹ˆéœ€è¦ epsilon**ï¼š

**é—®é¢˜ï¼šé™¤é›¶**ï¼š
```python
# æç«¯æƒ…å†µï¼šx å…¨ä¸º 0
x = torch.zeros(128)
rms = torch.sqrt(x.pow(2).mean())  # rms = 0
x_norm = x / rms  # 0 / 0 = nan
```

**è§£å†³ï¼šåŠ  epsilon**ï¼š
```python
rms = torch.sqrt(x.pow(2).mean() + eps)  # rms = sqrt(eps) â‰  0
x_norm = x / rms  # 0 / sqrt(eps) = 0ï¼ˆæ­£å¸¸ï¼‰
```

**å¦‚ä½•é€‰æ‹© epsilon**ï¼š

**å¸¸è§å€¼**ï¼š
```python
LayerNorm: eps = 1e-5
RMSNorm:   eps = 1e-6  # Qwen3 ä½¿ç”¨
           eps = 1e-8  # LLaMA ä½¿ç”¨
```

**è€ƒè™‘å› ç´ **ï¼š

**1. æ•°å€¼ç²¾åº¦**ï¼š
```python
# FP32
æœ€å°æ­£æ•° â‰ˆ 1e-38
1e-6 æ˜¯å®‰å…¨çš„

# FP16
æœ€å°æ­£æ•° â‰ˆ 6e-8
1e-6 æ˜¯å®‰å…¨çš„
1e-8 å¯èƒ½æœ‰é—®é¢˜ï¼ˆæ¥è¿‘æé™ï¼‰
```

**2. å¯¹ç»“æœçš„å½±å“**ï¼š
```python
# eps å¤ªå¤§
rms = sqrt(variance + 1e-3)
# å¦‚æœ variance å¾ˆå°ï¼ˆå¦‚ 1e-5ï¼‰ï¼Œrms ä¸»è¦ç”± eps å†³å®š
# å¯¼è‡´å½’ä¸€åŒ–å¤±æ•ˆ

# eps å¤ªå°
rms = sqrt(variance + 1e-10)
# å¦‚æœ variance ä¸º 0ï¼Œå¯èƒ½æ•°å€¼ä¸ç¨³å®š
```

**3. å®éªŒéªŒè¯**ï¼š
```python
# Qwen3 çš„é€‰æ‹©
eps = 1e-6

åŸå› ï¼š
- FP16/FP32 éƒ½å®‰å…¨
- è¶³å¤Ÿå°ï¼Œä¸å½±å“å½’ä¸€åŒ–æ•ˆæœ
- è¶³å¤Ÿå¤§ï¼Œé¿å…æ•°å€¼é—®é¢˜
```

**å®ç°**ï¼š
```python
class RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps
    
    def forward(self, x):
        variance = x.pow(2).mean(-1, keepdim=True)
        x_norm = x * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * x_norm
```

**æ³¨æ„**ï¼š
```python
# torch.rsqrt = 1/sqrt
# æ¯” 1/torch.sqrt æ›´å¿«ï¼ˆå•ä¸ª CUDA kernelï¼‰
```

---

### Q8.3: ä¸ºä»€ä¹ˆè¦åœ¨ FP16 å’Œ FP32 ä¹‹é—´è½¬æ¢ï¼Ÿ

**å›ç­”è¦ç‚¹**ï¼š

**æ··åˆç²¾åº¦ç­–ç•¥**ï¼š
```python
# æ¨¡å‹ä¸»ä½“ï¼šFP16
model = model.half()  # è½¬ FP16

# ç‰¹å®šæ“ä½œï¼šFP32
x = x.float()  # è½¬ FP32
x = operation(x)
x = x.half()  # è½¬å› FP16
```

**ä¸ºä»€ä¹ˆ FP16**ï¼š

**1. å†…å­˜**ï¼š
```python
FP32: 4 bytes/param
FP16: 2 bytes/param  # -50% å†…å­˜
```

**2. é€Ÿåº¦**ï¼š
```python
# GPU Tensor Coresï¼ˆä¸“é—¨ä¸º FP16 è®¾è®¡ï¼‰
FP16: ~2x é€Ÿåº¦æå‡
```

**3. å¸¦å®½**ï¼š
```python
FP16: ä¼ è¾“æ•°æ®å‡åŠ
```

**ä¸ºä»€ä¹ˆæŸäº›æ“ä½œéœ€è¦ FP32**ï¼š

**1. ç´¯åŠ æ“ä½œ**ï¼š
```python
# Softmax
sum = torch.sum(exp_x)  # ç´¯åŠ å¾ˆå¤šå°æ•°ï¼ŒFP16 ç²¾åº¦ä¸å¤Ÿ

# LayerNorm / RMSNorm
mean = x.sum() / n  # ç´¯åŠ 
variance = ((x - mean) ** 2).sum() / n  # ç´¯åŠ 
```

**2. æ•°å€¼èŒƒå›´å¤§**ï¼š
```python
# Loss è®¡ç®—
loss = -log(prob)  # log å¯èƒ½äº§ç”Ÿå¾ˆå¤§/å¾ˆå°çš„å€¼
```

**3. æ¢¯åº¦**ï¼š
```python
# è®­ç»ƒæ—¶
grad = compute_grad()  # FP16 æ¢¯åº¦å®¹æ˜“ underflow
grad = grad.float()  # è½¬ FP32
param = param - lr * grad  # FP32 æ›´æ–°
```

**å®ç°æ¨¡å¼**ï¼š

**Pattern 1: å±€éƒ¨ FP32**ï¼š
```python
def rmsnorm(x):
    input_dtype = x.dtype
    x = x.float()  # â†’ FP32
    
    # è®¡ç®—ï¼ˆFP32ï¼‰
    variance = x.pow(2).mean(-1, keepdim=True)
    x = x * torch.rsqrt(variance + eps)
    
    return x.to(input_dtype)  # â†’ å›åˆ°åŸ dtype
```

**Pattern 2: Softmax**ï¼š
```python
attn_weights = Q @ K.T  # FP16
attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32)  # FP32
attn_weights = attn_weights.to(Q.dtype)  # â†’ FP16
```

**æ€§èƒ½æƒè¡¡**ï¼š
```python
# å…¨ FP32
é€Ÿåº¦: 1.0x
å†…å­˜: 1.0x
ç²¾åº¦: æœ€é«˜

# å…¨ FP16
é€Ÿåº¦: 2.0x
å†…å­˜: 0.5x
ç²¾åº¦: å¯èƒ½ä¸ç¨³å®š

# æ··åˆç²¾åº¦ï¼ˆæ¨èï¼‰
é€Ÿåº¦: 1.8x
å†…å­˜: 0.5x
ç²¾åº¦: ç¨³å®š
```

---

## 9. æ€»ç»“ï¼šæ ¸å¿ƒè¦ç‚¹

### M1 æœ€é‡è¦çš„ 10 ä¸ªæ¦‚å¿µ

1. **KV Cache**ï¼šé¿å…é‡å¤è®¡ç®—ï¼Œæ˜¯æ¨ç†åŠ é€Ÿçš„æ ¸å¿ƒ
2. **Prefill vs Decode**ï¼šä¸¤é˜¶æ®µæœ‰ä¸åŒçš„æ€§èƒ½ç“¶é¢ˆ
3. **Causal Mask**ï¼šç¡®ä¿ autoregressive ç”Ÿæˆçš„æ­£ç¡®æ€§
4. **RoPE**ï¼šé«˜æ•ˆçš„ç›¸å¯¹ä½ç½®ç¼–ç 
5. **GQA**ï¼šå¹³è¡¡å†…å­˜å’Œæ€§èƒ½çš„ attention å˜ä½“
6. **Temperature/Top-k/Top-p**ï¼šæ§åˆ¶ç”Ÿæˆè´¨é‡å’Œå¤šæ ·æ€§
7. **RMSNorm**ï¼šæ¯” LayerNorm æ›´é«˜æ•ˆçš„å½’ä¸€åŒ–
8. **Fused Operations**ï¼šå‡å°‘å†…å­˜è®¿é—®ï¼Œæå‡æ€§èƒ½
9. **æ··åˆç²¾åº¦**ï¼šFP16 ä¸ºä¸»ï¼ŒFP32 for ç¨³å®šæ€§
10. **åˆ†å±‚æ¶æ„**ï¼šä¾¿äºæ‰©å±•å’Œç»´æŠ¤

### M1 åˆ° M2 çš„æ¼”è¿›

```
M1: å•è¯·æ±‚åŒæ­¥æ¨ç†
  â†“
M2: å¤šè¯·æ±‚ Continuous Batching
  - Schedulerï¼ˆè¯·æ±‚è°ƒåº¦ï¼‰
  - å¼‚æ­¥æ¥å£
  - åŠ¨æ€ batching
  - æ›´é«˜ååé‡
```

---

**æ–‡æ¡£å®Œæˆï¼**

æœ¬æ–‡æ¡£æ¶µç›–äº† M1 å¯èƒ½é‡åˆ°çš„æ‰€æœ‰å…³é”®é¢è¯•é—®é¢˜ã€‚

