# FoloVLLM å¿«é€Ÿå‚è€ƒå¡ç‰‡

## ğŸš€ ä¸€åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

```bash
# å®‰è£…
pip install -e .

# åŸºç¡€æ¨ç†
from folovllm import LLM
llm = LLM(model="Qwen/Qwen2.5-0.6B")
output = llm.generate("ä½ å¥½")[0]
print(output.text)
```

---

## ğŸ“Š Milestone é€ŸæŸ¥è¡¨

| é˜¶æ®µ   | åŠŸèƒ½            | æ ¸å¿ƒæ–‡ä»¶ï¼ˆå¯¹é½ vLLM v1ï¼‰                | å…³é”®æ¦‚å¿µ           | æ€§èƒ½æå‡         |
| ------ | --------------- | --------------------------------------- | ------------------ | ---------------- |
| **M0** | é¡¹ç›®åˆå§‹åŒ–      | `config.py`, `request.py`               | é…ç½®ç®¡ç†, è¯·æ±‚å®šä¹‰ | -                |
| **M1** | åŸºç¡€æ¨ç†        | `engine/llm_engine.py`                  | KV Cache, Sampling | Baseline         |
| **M2** | è¿ç»­æ‰¹å¤„ç†      | `core/sched/scheduler.py`               | Dynamic Batching   | åå 3-5x â†‘      |
| **M3** | Paged KV        | `core/block_pool.py`                    | PagedAttention     | æ˜¾å­˜åˆ©ç”¨ç‡ 100%  |
| **M4** | Flash Attn      | `attention/backends/flash_attn.py`      | IO-aware           | é€Ÿåº¦ 1.5-2x â†‘    |
| **M5** | Chunked Prefill | `core/sched/scheduler.py`               | æ··åˆè°ƒåº¦           | TTFT æ˜¾è‘— â†“      |
| **M6** | å‰ç¼€å¤ç”¨        | `core/kv_cache_manager.py`              | Trie, COW          | ç¼“å­˜å‘½ä¸­ 3-10x â†“ |
| **M7** | GPTQ            | `model_executor/layers/quantization.py` | 4-bit é‡åŒ–         | æ˜¾å­˜ 75% â†“       |

---

## ğŸ”§ æ ¸å¿ƒ API

### LLM åˆå§‹åŒ–

```python
from folovllm import LLM
from folovllm.config import EngineConfig

config = EngineConfig(
    # M3: Paged KV
    enable_paged_kv=True,
    block_size=16,
    
    # M4: Flash Attention
    attention_backend="flash",
    
    # M5: Chunked Prefill
    enable_chunked_prefill=True,
    max_chunk_size=512,
    
    # M6: Prefix Caching
    enable_prefix_caching=True,
    
    # M7: GPTQ
    quantization="gptq"
)

llm = LLM(model="Qwen/Qwen2.5-0.6B", engine_config=config)
```

### Sampling å‚æ•°

```python
from folovllm.sampling_params import SamplingParams

params = SamplingParams(
    temperature=0.7,     # éšæœºæ€§ (0=ç¡®å®š, 1=éšæœº)
    top_p=0.9,          # nucleus sampling
    top_k=50,           # top-k sampling
    max_tokens=100,     # æœ€å¤§ç”Ÿæˆ token æ•°
    repetition_penalty=1.1,  # é‡å¤æƒ©ç½š
)

output = llm.generate(prompt, params)
```

### æ‰¹é‡æ¨ç†

```python
prompts = ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"]
outputs = llm.generate(prompts, params)

for i, output in enumerate(outputs):
    print(f"Prompt {i}: {output.prompt}")
    print(f"Output {i}: {output.text}")
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–é€ŸæŸ¥

### æå‡ååé‡

```python
config = EngineConfig(
    enable_paged_kv=True,      # å…è®¸æ›´å¤§ batch
    max_batch_size=64,         # å¤§ batch
    attention_backend="flash", # åŠ é€Ÿè®¡ç®—
    max_chunk_size=1024,       # å¤§ chunk (prefill åå)
)
```

### é™ä½å»¶è¿Ÿ

```python
config = EngineConfig(
    attention_backend="flash",     # åŠ é€Ÿ
    enable_chunked_prefill=True,   # å‡å°‘é˜»å¡
    max_chunk_size=256,            # å° chunk (TTFT)
    enable_prefix_caching=True,    # ç¼“å­˜åŠ é€Ÿ
    max_batch_size=4,              # å° batch (å‡å°‘ç­‰å¾…)
)
```

### èŠ‚çœæ˜¾å­˜

```python
config = EngineConfig(
    enable_paged_kv=True,    # é›¶ç¢ç‰‡
    block_size=16,           # çµæ´»åˆ†é…
    quantization="gptq",     # 4-bit æƒé‡
    max_model_len=2048,      # é™åˆ¶é•¿åº¦
)
```

---

## ğŸ” å¸¸ç”¨å‘½ä»¤

### å¼€å‘

```bash
# æ ¼å¼åŒ–
make format

# æ£€æŸ¥
make lint

# æµ‹è¯•
make test

# è¦†ç›–ç‡
make coverage
```

### æ€§èƒ½æµ‹è¯•

```bash
# å»¶è¿Ÿæµ‹è¯•
python tests/benchmark/latency_test.py --model Qwen/Qwen2.5-0.6B

# ååé‡æµ‹è¯•
python tests/benchmark/throughput_test.py --batch-size 16

# å¯¹æ¯”ä¸åŒ milestone
python tests/benchmark/compare.py --milestones m1,m2,m3
```

### è¿è¡Œç¤ºä¾‹

```bash
# åŸºç¡€æ¨ç†
python examples/basic_inference.py

# æ‰¹é‡æ¨ç†
python examples/batch_inference.py

# Few-shot
python examples/few_shot.py
```

---

## ğŸ› è°ƒè¯•æŠ€å·§

### è¯¦ç»†æ—¥å¿—

```python
import logging
logging.basicConfig(level=logging.DEBUG)

llm = LLM(model="...", log_level="DEBUG")
```

### æ€§èƒ½åˆ†æ

```python
import torch.profiler as profiler

with profiler.profile(activities=[
    profiler.ProfilerActivity.CPU,
    profiler.ProfilerActivity.CUDA
]) as prof:
    llm.generate(prompt)

print(prof.key_averages().table())
```

### æ˜¾å­˜è¿½è¸ª

```python
import torch

torch.cuda.reset_peak_memory_stats()
llm.generate(prompt)
peak_mem = torch.cuda.max_memory_allocated() / 1e9
print(f"Peak memory: {peak_mem:.2f} GB")
```

---

## ğŸ’¡ å…³é”®æ•°æ®ç»“æ„

### Request

```python
@dataclass
class Request:
    request_id: str
    prompt: str
    sampling_params: SamplingParams
    arrival_time: float
```

### Sequence

```python
@dataclass
class Sequence:
    seq_id: str
    request_id: str
    token_ids: List[int]
    kv_blocks: List[KVBlock]
    status: SequenceStatus  # WAITING/RUNNING/FINISHED
```

### KVBlock

```python
@dataclass
class KVBlock:
    block_id: int          # ç‰©ç† block ID
    ref_count: int         # å¼•ç”¨è®¡æ•°
    block_hash: Optional[int]  # å‰ç¼€ hash (M6)
```

---

## ğŸ¯ æ€§èƒ½æŒ‡æ ‡

### å»¶è¿ŸæŒ‡æ ‡

- **TTFT**: Time to First Token (é¦– token å»¶è¿Ÿ)
- **TPOT**: Time Per Output Token (å¹³å‡æ¯ token æ—¶é—´)
- **E2E**: End-to-End Latency (æ€»å»¶è¿Ÿ)

### ååé‡æŒ‡æ ‡

- **Tokens/s**: æ¯ç§’å¤„ç† token æ•°
- **Requests/s**: æ¯ç§’å®Œæˆè¯·æ±‚æ•°

### èµ„æºæŒ‡æ ‡

- **Memory**: æ˜¾å­˜å ç”¨
- **GPU Util**: GPU åˆ©ç”¨ç‡

---

## ğŸ“š æ–‡æ¡£å¯¼èˆª

### è§„åˆ’æ–‡æ¡£
- [å¼€å‘è®¡åˆ’](development_plan.md) - å®Œæ•´å¼€å‘è·¯çº¿å›¾
- [æŠ€æœ¯è·¯çº¿å›¾](roadmap.md) - æŠ€æœ¯æ¼”è¿›è·¯å¾„
- [é‡Œç¨‹ç¢‘æ£€æŸ¥æ¸…å•](milestone_checklist.md) - å®Œæˆæ ‡å‡†

### å­¦ä¹ æ–‡æ¡£
- [å­¦ä¹ ç¬”è®°](learn/) - æŠ€æœ¯åŸç†æ·±åº¦è®²è§£
- [é¢è¯•å‡†å¤‡](interview_guide.md) - é¢è¯•é—®ç­”æ±‡æ€»
- [æŠ€æœ¯å¯¹æ¯”](technical_comparison.md) - æ€§èƒ½å¯¹æ¯”åˆ†æ

### ä½¿ç”¨æ–‡æ¡£
- [å¿«é€Ÿå¼€å§‹](getting_started.md) - å®‰è£…å’Œä½¿ç”¨æŒ‡å—
- [è´¡çŒ®æŒ‡å—](../CONTRIBUTING.md) - å¼€å‘è§„èŒƒ
- [é¡¹ç›®æ€»ç»“](project_summary.md) - é¡¹ç›®æ¦‚è§ˆ

### å¼€å‘æ–‡æ¡£
- [å¼€å‘æ—¥å¿—](dev/) - å„é˜¶æ®µå®ç°ç»†èŠ‚
- [API æ–‡æ¡£](api/) - è‡ªåŠ¨ç”Ÿæˆçš„ API æ–‡æ¡£

---

## ğŸ”‘ å…³é”®æ¦‚å¿µé€Ÿè®°

### KV Cache
- **ä½œç”¨**: é¿å…é‡å¤è®¡ç®—å†å² token
- **å®ç°**: å­˜å‚¨ Key/Valueï¼Œå¢é‡æ›´æ–°
- **é—®é¢˜**: æ˜¾å­˜å ç”¨å¤§

### PagedAttention
- **æ€æƒ³**: è™šæ‹Ÿå†…å­˜ç®¡ç†
- **ä¼˜åŠ¿**: é›¶ç¢ç‰‡ï¼Œé«˜åˆ©ç”¨ç‡
- **å®ç°**: Block Pool + Block Table

### Continuous Batching
- **åŸç†**: Iteration-level scheduling
- **ä¼˜åŠ¿**: åŠ¨æ€æ‰¹å¤„ç†ï¼Œé«˜åå
- **å®ç°**: åŠ¨æ€æ·»åŠ /ç§»é™¤åºåˆ—

### Flash Attention
- **ä¼˜åŒ–**: IO-aware, Tiling
- **æ•ˆæœ**: å‡å°‘ HBM è®¿é—®ï¼Œ2-4x å¿«
- **å®ç°**: Kernel fusion, Recomputation

### Chunked Prefill
- **ç›®çš„**: å¹³è¡¡ TTFT å’Œåå
- **æ–¹æ³•**: Prefill åˆ†å—ï¼Œä¸ Decode æ··åˆ
- **æƒè¡¡**: Chunk size é€‰æ‹©

### Prefix Caching
- **åº”ç”¨**: Few-shot, å¤šè½®å¯¹è¯
- **å®ç°**: Trie åŒ¹é… + COW
- **æ•ˆæœ**: ç¼“å­˜å‘½ä¸­ 10x å¿«

### GPTQ
- **ç›®æ ‡**: 4-bit é‡åŒ–ï¼Œä¿æŒç²¾åº¦
- **ç®—æ³•**: Hessian-based
- **æ•ˆæœ**: æ˜¾å­˜ 75% â†“, ç²¾åº¦æŸå¤± < 1%

---

## ğŸš¨ å¸¸è§é—®é¢˜å¿«é€Ÿè§£å†³

| é—®é¢˜           | åŸå›             | è§£å†³æ–¹æ¡ˆ                            |
| -------------- | --------------- | ----------------------------------- |
| **CUDA OOM**   | æ˜¾å­˜ä¸è¶³        | â†“batch size / å¯ç”¨ Paged KV / GPTQ  |
| **é€Ÿåº¦æ…¢**     | è®¡ç®—æ•ˆç‡ä½      | å¯ç”¨ Flash Attn / â†‘batch size       |
| **TTFT é«˜**    | Prefill é˜»å¡    | å¯ç”¨ Chunked Prefill / Prefix Cache |
| **è¾“å‡ºè´¨é‡å·®** | å‚æ•°ä¸å½“        | è°ƒæ•´ temperature / top_p            |
| **ç¼“å­˜ä¸ç”Ÿæ•ˆ** | æœªå¯ç”¨/å‰ç¼€ä¸åŒ | æ£€æŸ¥é…ç½® / ç¡®è®¤ token åºåˆ—          |

---

## âœ… å¼€å‘ Checklist

### å¼€å§‹æ–° Milestone
- [ ] é˜…è¯»å¼€å‘è®¡åˆ’å’Œä¸Šä¸€é˜¶æ®µæ—¥å¿—
- [ ] åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
- [ ] è®¾è®¡æ¥å£å’Œæ•°æ®ç»“æ„
- [ ] ç¼–å†™æµ‹è¯•ç”¨ä¾‹

### å®Œæˆ Milestone
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½æµ‹è¯•å®Œæˆ
- [ ] å­¦ä¹ ç¬”è®°ç¼–å†™
- [ ] å¼€å‘æ—¥å¿—è®°å½•
- [ ] README æ›´æ–°
- [ ] æäº¤ PR

---

## ğŸ“ æ¨èå­¦ä¹ è·¯å¾„

1. **åŸºç¡€çŸ¥è¯†** (1-2 å¤©)
   - Transformer æ¶æ„
   - Attention æœºåˆ¶
   - è‡ªå›å½’ç”Ÿæˆ

2. **M0-M1** (3-5 å¤©)
   - é¡¹ç›®åˆå§‹åŒ–
   - åŸºç¡€æ¨ç†æµç¨‹
   - KV Cache åŸç†

3. **M2-M3** (7-10 å¤©)
   - åŠ¨æ€æ‰¹å¤„ç†
   - PagedAttention
   - å†…å­˜ç®¡ç†

4. **M4-M5** (5-7 å¤©)
   - Flash Attention
   - Chunked Prefill
   - æ€§èƒ½ä¼˜åŒ–

5. **M6-M7** (5-7 å¤©)
   - å‰ç¼€å¤ç”¨
   - GPTQ é‡åŒ–
   - ç»¼åˆä¼˜åŒ–

**æ€»è®¡**: ~6 å‘¨

---

## ğŸ“ è·å–å¸®åŠ©

- **æ–‡æ¡£**: æŸ¥çœ‹ `docs/` ç›®å½•
- **ç¤ºä¾‹**: è¿è¡Œ `examples/` ä»£ç 
- **Issues**: GitHub Issues
- **Discussions**: GitHub Discussions

---

**æŒç»­æ›´æ–°ä¸­... ğŸš€**

