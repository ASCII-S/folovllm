# FoloVLLM - è½»é‡çº§ LLM æ¨ç†æ¡†æ¶

![Python](https://img.shields.io/badge/python-3.10+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)
![License](https://img.shields.io/badge/license-Apache%202.0-green.svg)

ä¸€ä¸ªæ¨¡ä»¿ vLLM è®¾è®¡çš„è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ¡†æ¶ï¼Œä¸“æ³¨äºæ•™å­¦å’Œç†è§£ç°ä»£ LLM æ¨ç†ä¼˜åŒ–æŠ€æœ¯ã€‚

## ğŸ¯ é¡¹ç›®ç›®æ ‡

FoloVLLM æ—¨åœ¨é€šè¿‡æ¸è¿›å¼å¼€å‘ï¼Œå®ç°ä¸€ä¸ª**å¯ç†è§£ã€å¯å¤ç°**çš„ LLM æ¨ç†æ¡†æ¶ï¼Œæ¶µç›–ä»¥ä¸‹æ ¸å¿ƒæŠ€æœ¯ï¼š

- âœ… **ç¦»çº¿æ¨ç†** - åŸºç¡€æ¨ç†æµç¨‹
- âœ… **è¿ç»­æ‰¹å¤„ç†** (Continuous Batching) - åŠ¨æ€æ‰¹å¤„ç†è°ƒåº¦  
- âœ… **Paged KV Cache** - PagedAttention å†…å­˜ä¼˜åŒ–
- âœ… **Flash Attention** - é«˜æ•ˆ attention è®¡ç®—
- âœ… **Chunked Prefill** - åˆ†å—é¢„å¡«å……
- âœ… **å‰ç¼€å¤ç”¨** (Prefix Caching) - å…±äº«å‰ç¼€ä¼˜åŒ–
- âœ… **GPTQ é‡åŒ–** - 4-bit é‡åŒ–æ¨ç†

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè®¾ç½®

**ä¸€é”®è®¾ç½®ï¼ˆæ¨èï¼‰:**

```bash
# Linux / macOS
bash setup_env.sh

# Windows
setup_env.bat
```

**æ‰‹åŠ¨è®¾ç½®:**

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate  # Linux/macOS
# æˆ–
venv\Scripts\activate.bat  # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
pip install -e .
```

ğŸ“– è¯¦ç»†è¯´æ˜: [ç¯å¢ƒè®¾ç½®æŒ‡å—](ENVIRONMENT_SETUP.md)

### 2. è¿è¡Œç¤ºä¾‹

```bash
# M0 åŸºç¡€åŠŸèƒ½æ¼”ç¤º
python examples/m0_basic_usage.py

# M1 æ¨ç†ç¤ºä¾‹
python examples/m1_inference.py \
    --model Qwen/Qwen2.5-0.5B \
    --prompt "What is the capital of France?" \
    --max-tokens 50

# M2 æ‰¹é‡æ¨ç†ç¤ºä¾‹ï¼ˆè¿ç»­æ‰¹å¤„ç†ï¼‰
python examples/m2_inference.py \
    --model Qwen/Qwen2.5-0.5B \
    --num-prompts 5 \
    --max-tokens 64 \
    --compare-sequential

# è¿è¡Œæµ‹è¯•
pytest tests/unit/test_m0_*.py -v
pytest tests/unit/test_m1_*.py -v
pytest tests/unit/test_m2_*.py -v
pytest tests/integration/test_m2_e2e.py -v
```

### 3. åŸºç¡€ä½¿ç”¨

```python
from folovllm import LLMEngine, ModelConfig, SamplingParams

# åˆ›å»ºé…ç½®
config = ModelConfig(
    model="Qwen/Qwen3-0.6B",
    dtype="float16",
    trust_remote_code=True
)

# åˆå§‹åŒ–å¼•æ“
engine = LLMEngine(config, device="cuda")

# M1: å•è¯·æ±‚ç”Ÿæˆ
sampling_params = SamplingParams(
    temperature=0.7,
    top_k=50,
    top_p=0.95,
    max_tokens=100
)
output = engine.generate("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±", sampling_params)
print(output.outputs[0].text)

# M2: æ‰¹é‡ç”Ÿæˆï¼ˆè¿ç»­æ‰¹å¤„ç†ï¼‰
from folovllm import SchedulerConfig

scheduler_config = SchedulerConfig(
    max_num_seqs=256,
    max_num_batched_tokens=2048
)
engine = LLMEngine(config, scheduler_config=scheduler_config, device="cuda")

prompts = [
    "What is the capital of France?",
    "Explain quantum computing.",
    "Write a haiku about AI.",
]
outputs = engine.generate_batch(prompts, sampling_params)

for req_id, output in outputs.items():
    print(f"{output.prompt} -> {output.outputs[0].text}")
```

## ğŸ“š å¼€å‘è·¯çº¿

æœ¬é¡¹ç›®é‡‡ç”¨**æ¸è¿›å¼å¼€å‘**ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æ˜¯ä¸Šä¸€é˜¶æ®µçš„è¶…é›†ï¼š

| é˜¶æ®µ   | åŠŸèƒ½            | çŠ¶æ€     | æ–‡æ¡£                                                                                                                                                                   |
| ------ | --------------- | -------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **M0** | é¡¹ç›®åˆå§‹åŒ–      | âœ… å·²å®Œæˆ | [å¼€å‘æ—¥å¿—](docs/dev/milestone_0.md)                                                                                                                                    |
| **M1** | åŸºç¡€ç¦»çº¿æ¨ç†    | âœ… å·²å®Œæˆ | [ğŸ“– æ€»è§ˆ](docs/milestone_1_index.md) Â· [å­¦ä¹ ç¬”è®°](docs/learn/milestone_1.md) Â· [å£è¿°å±•ç¤º](docs/presentation/milestone_1.md) Â· [é¢è¯•æŒ‡å—](docs/interview/milestone_1.md) |
| **M2** | è¿ç»­æ‰¹å¤„ç†      | âœ… å·²å®Œæˆ | [å¼€å‘æ—¥å¿—](docs/dev/milestone_2.md)                                                                                                                                    |
| **M3** | Paged KV Cache  | â³ å¾…å¼€å§‹ | [å­¦ä¹ ç¬”è®°](docs/learn/03_paged_kv_cache.md)                                                                                                                            |
| **M4** | Flash Attention | â³ å¾…å¼€å§‹ | [å­¦ä¹ ç¬”è®°](docs/learn/04_flash_attention.md)                                                                                                                           |
| **M5** | Chunked Prefill | â³ å¾…å¼€å§‹ | [å­¦ä¹ ç¬”è®°](docs/learn/05_chunked_prefill.md)                                                                                                                           |
| **M6** | å‰ç¼€å¤ç”¨        | â³ å¾…å¼€å§‹ | [å­¦ä¹ ç¬”è®°](docs/learn/06_prefix_caching.md)                                                                                                                            |
| **M7** | GPTQ é‡åŒ–       | â³ å¾…å¼€å§‹ | [å­¦ä¹ ç¬”è®°](docs/learn/07_gptq_quantization.md)                                                                                                                         |

ğŸ“– **å®Œæ•´å¼€å‘è®¡åˆ’**: [development_plan.md](docs/development_plan.md)

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

> **è®¾è®¡åŸåˆ™**: é¡¹ç›®ç»“æ„ä¸ [vLLM v1](https://github.com/vllm-project/vllm) æºç å®Œå…¨å¯¹é½ï¼Œä¾¿äºå­¦ä¹ å’Œå‚è€ƒ

```
folovllm/
â”œâ”€â”€ folovllm/                  # æ ¸å¿ƒåŒ…ï¼ˆå¯¹é½ vllm.v1ï¼‰
â”‚   â”œâ”€â”€ request.py            # è¯·æ±‚å®šä¹‰
â”‚   â”œâ”€â”€ outputs.py            # è¾“å‡ºæ ¼å¼
â”‚   â”œâ”€â”€ config.py             # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ core/                 # æ ¸å¿ƒç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ sched/           # è°ƒåº¦å™¨
â”‚   â”‚   â”œâ”€â”€ block_pool.py    # Block Pool
â”‚   â”‚   â””â”€â”€ kv_cache_manager.py  # KV Cache ç®¡ç†
â”‚   â”œâ”€â”€ engine/              # æ¨ç†å¼•æ“
â”‚   â”œâ”€â”€ model_executor/      # æ¨¡å‹æ‰§è¡Œ
â”‚   â”œâ”€â”€ attention/           # Attention å®ç°
â”‚   â”œâ”€â”€ sample/              # é‡‡æ ·
â”‚   â”œâ”€â”€ worker/              # Worker
â”‚   â””â”€â”€ executor/            # æ‰§è¡Œå™¨
â”œâ”€â”€ tests/                   # æµ‹è¯•
â”œâ”€â”€ docs/                    # æ–‡æ¡£
â”‚   â”œâ”€â”€ project_structure.md # ğŸ“‹ ç»“æ„è¯¦è§£
â”‚   â”œâ”€â”€ learn/              # å­¦ä¹ ç¬”è®°
â”‚   â””â”€â”€ dev/                # å¼€å‘æ—¥å¿—
â””â”€â”€ examples/               # ç¤ºä¾‹ä»£ç 
```

ğŸ“– è¯¦ç»†ç»“æ„è¯´æ˜: [project_structure.md](docs/project_structure.md)

## ğŸ’¡ æ ¸å¿ƒæŠ€æœ¯

### 1. Continuous Batching
åŠ¨æ€è°ƒåº¦å¤šä¸ªè¯·æ±‚ï¼Œå®ç°é«˜ååé‡æ¨ç†ã€‚

### 2. PagedAttention
ä½¿ç”¨åˆ†é¡µå†…å­˜ç®¡ç† KV Cacheï¼Œå†…å­˜åˆ©ç”¨ç‡æå‡è‡³æ¥è¿‘ 100%ã€‚

### 3. Flash Attention
ä¼˜åŒ–çš„ attention ç®—æ³•ï¼Œé™ä½ HBM è®¿é—®ï¼Œæå‡è®¡ç®—æ•ˆç‡ã€‚

### 4. Chunked Prefill
å°†é•¿ prefill åˆ†å—å¤„ç†ï¼Œå¹³è¡¡é¦–tokenå»¶è¿Ÿå’Œååé‡ã€‚

### 5. Prefix Caching
è‡ªåŠ¨æ£€æµ‹å’Œå¤ç”¨å…±äº«å‰ç¼€ï¼ŒåŠ é€Ÿ few-shot å’Œå¤šè½®å¯¹è¯ã€‚

### 6. GPTQ Quantization
4-bit æƒé‡é‡åŒ–ï¼Œé™ä½æ˜¾å­˜å ç”¨ï¼Œæå‡æ¨ç†é€Ÿåº¦ã€‚

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### ç›®æ ‡æ€§èƒ½ (Qwen3-0.6B on A100)

| ä¼˜åŒ–é˜¶æ®µ            | ååé‡     | å»¶è¿Ÿ (TTFT) | æ˜¾å­˜å ç”¨ |
| ------------------- | ---------- | ----------- | -------- |
| M1: åŸºç¡€æ¨ç†        | åŸºçº¿       | åŸºçº¿        | åŸºçº¿     |
| M2: è¿ç»­æ‰¹å¤„ç†      | 3-5x â†‘     | -           | -        |
| M3: Paged KV        | -          | -           | 2x â†“     |
| M4: Flash Attn      | 1.5-2x â†‘   | 20-30% â†“    | -        |
| M5: Chunked Prefill | -          | æ˜¾è‘—æ”¹å–„    | -        |
| M6: Prefix Cache    | -          | 3-10x â†“     | -        |
| M7: GPTQ            | 1.2-1.5x â†‘ | -           | 4x â†“     |

## ğŸ§ª æµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest tests/

# å•å…ƒæµ‹è¯•
pytest tests/unit/

# é›†æˆæµ‹è¯•
pytest tests/integration/

# æ€§èƒ½æµ‹è¯•
pytest tests/benchmark/
```

## ğŸ“– å­¦ä¹ èµ„æº

æ¯ä¸ªé˜¶æ®µéƒ½åŒ…å«è¯¦ç»†çš„å­¦ä¹ ç¬”è®°ï¼Œæ¶µç›–ï¼š
- âœ¨ æŠ€æœ¯åŸç†è®²è§£
- ğŸ”§ å®ç°ç»†èŠ‚åˆ†æ
- ğŸ’¼ é¢è¯•å¸¸è§é—®é¢˜
- ğŸ“š å‚è€ƒèµ„æ–™é“¾æ¥

æŸ¥çœ‹ [docs/learn/](docs/learn/) ç›®å½•è·å–å®Œæ•´å†…å®¹ã€‚

## ğŸ”— å‚è€ƒèµ„æ–™

### è®ºæ–‡
- [vLLM: Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)
- [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323)

### ä»£ç 
- [vLLM Official Repository](https://github.com/vllm-project/vllm)
- [Flash Attention](https://github.com/Dao-AILab/flash-attention)

## ğŸ¤ è´¡çŒ®

æœ¬é¡¹ç›®ä¸»è¦ç”¨äºå­¦ä¹ å’Œæ•™å­¦ç›®çš„ã€‚æ¬¢è¿æå‡ºé—®é¢˜å’Œå»ºè®®ï¼

## ğŸ“ License

Apache 2.0 License

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ vLLM å›¢é˜Ÿçš„å¼€æºå·¥ä½œï¼Œä¸ºæœ¬é¡¹ç›®æä¾›äº†å®è´µçš„å‚è€ƒã€‚

---

**Current Status**: ğŸ”„ Milestone 0 - é¡¹ç›®åˆå§‹åŒ–ä¸­

æŸ¥çœ‹ [å¼€å‘è®¡åˆ’](docs/development_plan.md) äº†è§£è¯¦ç»†è¿›åº¦ã€‚
